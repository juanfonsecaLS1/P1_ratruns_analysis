---
editor: 
  markdown: 
    wrap: 72
---

# Results

```{r,message=FALSE}
#| label: libraries
#| message: false

options(repos = c(CRAN = "https://cloud.r-project.org"))
if (!require("remotes")) install.packages("remotes")
pkgs = c(
    "sf",
    "tidyverse",
    "tmap",
    "apng",
    "gganimate",
    "kableExtra",
    "lme4",
    "equatiomatic",
    "marginaleffects"
)

remotes::install_cran(pkgs)
sapply(pkgs, require, character.only = TRUE)
```

## Loading data

### Offence reports (tickets)

```{r}
urban_perimeter <- st_read("raw_data/perimetrourbano.gpkg") |>
  st_transform(3116)
```

```{r}
off_sf_all <- (st_read("sf_network/manualtickets_clean_sf.gpkg") |>
                 st_transform(3116))[urban_perimeter,] |> filter(year == 2019,day_type != "friday")
# off_sf_all <- (st_read("sf_network/wwd_clean_sf.gpkg") |>
#                  st_transform(3116))[urban_perimeter,] |> filter(year == 2019,day_type != "friday")
```

### Network

```{r}
sf_net_exp <- st_read("sf_network/full_sf_network.gpkg") |>
  st_transform(3116)
sf_net <- sf_net_exp[urban_perimeter,]
```

### Centrality results

```{r}
cent_results <- read_csv("sf_network/hourly_cent_results.csv",
                       lazy = F)
```

## Assigning reports to the network

We need to assign the reports to the network. As we are interested in
the residential roads, any bi-directional road is represented in the
`sf` object as two `LINESTRING` objects. Since we do not have
information to know which specific direction the reports correspond to,
we will need to simplify the spatial object. Our target variable is the
betweenness centrality, so we are going to keep the two centrality
values for each bi-directional element.

First, we find the pairs of links with the `st_contains` function.

```{r}
simplifed_net_indexes <- lapply(st_contains(sf_net,sf_net),
            \(x){
              x[order(x)]
}) |> unique()
```

Each pair is then assigned a unique id.

```{r}
simp_groups <- do.call(bind_rows,
        lapply(seq_along(simplifed_net_indexes),
               \(i){
                 tibble(id = simplifed_net_indexes[[i]],
                        pair_id = i)
               })) |> 
  arrange(id)


sf_net$pair_id <- simp_groups$pair_id
```

We will create a subset of the `residential` and `unclassified` roads,
and another with all other roads

```{r}
subset_net <- sf_net |> 
  filter(roadclass %in% c(
    "residential",
    "unclassified"
    )) 
```

```{r}
major_net <- sf_net |> 
  filter(!roadclass %in% c(
    "residential",
    "unclassified"
    )) 
```

Using the `pair_id` we extract the minimum, maximum and average change
in centrality for each pair.

```{r}
summary_pairs <- cent_results |> 
  right_join(sf_net |>
               st_drop_geometry() |>
               select(edge_id,way_id,pair_id),
            by = "edge_id") |> 
  # # Filtering only the links that were inverted during the network creation and standard links
  filter(!any(str_detect(way_id,"r$"))|str_detect(way_id,"r$"),.by = pair_id) |>
  summarise(across(diff:logreldiff.ff,
                   list(min=min, max=max, avg = mean)),
            .by = c(pair_id,day_type,hour))
```

```{r}
summary_pairs_dist.jct <- 
  sf_net |>
  st_drop_geometry() |>
  summarise(across(dist.jct,\(x) mean(x) |> round()),
            .by = c(pair_id))
```

A simplified version of the `sf` object is produced extracting the first
element of each pair, we will discard columns with the centrality
metrics from this object to avoid confusion

```{r}
# simpl_network_sf <- subset_net[vapply(simplifed_net_indexes,\(x) x[1],numeric(1)),] |> select(lanes:component,pair_id)

simpl_network_sf <- sf_net |>
  # # Filtering only the links that were inverted during the network creation and standard links
  filter(!any(str_detect(way_id,"r$"))|str_detect(way_id,"r$"),
         .by = pair_id) |>
  slice_head(n = 1,by = pair_id) |>
  select(lanes:component,pair_id)

simpl_subset_sf <- simpl_network_sf |> 
  filter(roadclass %in% c(
    "residential",
    "unclassified"
    ))

```

We are interested in the reports on residential and unclassified
streets. For this, we will create two buffers from the subsets created
before. It is uncertain how the coordinates of each report are recorded,
there might be some error associated with the use of GPS devices, and
also, some uncertainty in the way the officers do it.

```{r}
anti_buffer <- major_net |>
  st_union() |> 
  st_buffer(10,endCapStyle = "FLAT")
```

```{r}
subset_buffer <- subset_net |> 
  st_union()  |> 
  st_buffer(20,endCapStyle = "FLAT")
```

As some reports might be associated to the major network, we will first
filter reports within 10 meters from a major road, so these do not get
wrongly assigned to a minor road.

```{r}
off_sf <- off_sf_all[anti_buffer,,op = st_disjoint]
```

The offences that are assumed to happen in the minor offences are
assumed to be within 20 meters.

```{r}
minor_offences <- off_sf[subset_buffer,,op = st_intersects]
```

```{r}
# tmap_mode("view")
tm_shape(major_net)+
  tm_lines("gray60",col_alpha = 0.6)+
  tm_shape(subset_buffer)+
  tm_fill("blue",fill_alpha = 0.6)+
  tm_shape(off_sf_all|>
                 filter(abs(hour - 9)<=1,
                        year == 2019,
                        day_type == "weekday"))+
  tm_dots("yellow")+
  tm_shape(minor_offences)+
  tm_dots("red")+
  tm_layout(bg.color = "grey10")
```

#### Summary

Total length of road network

```{r}
simpl_network_sf |> mutate(distance = st_length(geom,)) |> pull(distance) |> sum()
```

```{r}
sf_net |> st_drop_geometry() |>
  filter(!str_detect(pattern = "r",way_id)) |> 
  mutate(roadclass = str_remove(roadclass,"_link")) |> 
  summarise(d_weighted = sum(d_weighted)/1e3,.by=c(roadclass)) |> 
  mutate(d_weighted = round(d_weighted),
         roadclass = factor(roadclass,
                            levels = c("trunk","primary","secondary","tertiary","residential","unclassified"),
                            ordered = T)) |> 
  arrange(roadclass) |> 
  mutate(portion = round(d_weighted/sum(d_weighted)*100,1)) |> 
  kableExtra::kable()
```

### Finding the closest element of the network

```{r}
minor_offences$near_index <- st_nearest_feature(minor_offences,simpl_subset_sf)
minor_offences$pair_id <- simpl_subset_sf$pair_id[minor_offences$near_index]
```

## Exploring BC changes in the network with WWD allowed

```{r}
#| eval: false

map_logdiff <- lapply(
  0:23,
  \(h){
    simpl_network_sf |>
      left_join(summary_pairs,by = "pair_id") |> 
      # filter(hour!=3) |>
      filter(day_type == "weekday", hour == h) |> 
      mutate(logdiff_max = if_else(logdiff_max == 0,1,logdiff_max)) |> 
      ggplot(aes(col =  logdiff_max, linewidth = abs(logdiff_max)))+
      geom_sf()+
      scale_color_steps2(mid = "gray80",high = "dodgerblue2",low = "firebrick3",
                         breaks = c(-12,-8,-4,0,4,8,12),
                         limits=c(-8,8)
      )+
      scale_linewidth_continuous(limits = c(0.00,12), range = c(0.01,20),trans = "exp")+
      theme_void()+
      labs(title = "BC Changes",
           subtitle = paste0('Hour: ',h),
           col = "log "
      )+
      guides(linewidth = "none")+
      theme(legend.position = "inside",
            legend.position.inside = c(0.1,0.8)
            )
    
  }
)
              
```

```{r}
#| eval: false
#| include: false

lapply(0:23,\(i){
  ggsave(plot = map_logdiff[[i+1]],
         filename = paste0("sf_network/",sprintf("%02d",i),"_map_centh.png"),
         dpi = 660,
         units = "cm",width = 7,
         height = 12)
})


png_files= list.files(path = "sf_network/",pattern = "_map_centh.png",full.names = T)


apng(png_files, output_file = "sf_network/anim_map_logdiff.png",num_plays = 0,delay_num = 1,delay_den = 2)

```

![](sf_network/anim_map_logdiff.png)

## Modelling

```{r}
time_slots <- summary_pairs |> select(day_type,hour) |> unique()

offences_bool <- plyr::join_all(
  lapply(1:nrow(time_slots),
       \(j,
         h_threshold = 1){
         summary_pairs |>
           select(pair_id) |> 
           unique() |> 
           left_join(minor_offences |>
           st_drop_geometry() |> 
           filter(day_type == time_slots$day_type[j],
                  abs(hour - time_slots$hour[j])<=h_threshold) |> 
           count(pair_id),
           by = join_by(pair_id)) |> 
           rename_with(.fn =\(x) {paste(time_slots$day_type[j],time_slots$hour[j],sep = "_")},.cols = "n")
       }),by = "pair_id") |> 
  mutate(across(-pair_id,\(x){!is.na(x)})) |>
  pivot_longer(-pair_id,values_to = "offence_bool") |>
  separate_wider_delim(name, delim = "_",names = c("day_type","hour")) |> 
  mutate(across(hour,as.numeric))
```

```{r}
offences_bool |>  
  filter(offence_bool) |> 
  count(day_type,hour) |> 
  ggplot(aes(hour,n,col = day_type,group = day_type))+
  geom_line()+
  theme_minimal()
```

```{r}
model_data <- (summary_pairs |>
  inner_join(simpl_subset_sf |>
               st_drop_geometry() |>
               select(pair_id,way_id,oneway),
            by = "pair_id")) |> 
  left_join(summary_pairs_dist.jct,by = join_by(pair_id)) |> 
  left_join(offences_bool,by = join_by(pair_id,hour,day_type))
```

#### Fitting global logisting regressions

```{r}
glm_models_0 <- model_data |> 
  nest(data = c(pair_id,
                diff_min:offence_bool)) |> 
  mutate(
    model_rel = map(data,
                \(.x) {
                 glm(offence_bool ~ logdiff_max,
                     data = .x,
                     family = binomial(link = "logit"))
                # },
                # model_abs = map(data,
                # \(.x) {
                #  glm(p ~ logdiff_max,
                #      data = .x,
                #      family = binomial(link = "logit"))
                }
                )
                )


mod_pred_0 <- glm_models_0 |> 
  mutate(predicted = map2(model_rel, data ,\(.x,.y) {
    tibble(logdiff_max = .y$logdiff_max,
           offence_bool = predict(.x,
                                  newdata = .y,
                                  type = "response"))
    })) |> 
  select(-data,-model_rel) |> 
  unnest(cols = predicted) |> 
  unite("id",day_type:hour,remove = F) |> unique()
```

A visual of the fitted lines

```{r}
mod_pred_0 |> 
  ggplot(aes(x = logdiff_max,
             y = offence_bool,
             group = id,
             col = hour,linetype = day_type))+
  geom_line(alpha = 0.1)+
  scale_y_continuous(limits = c(0,1))+
  theme_minimal()+
  scale_color_viridis_c(option = "plasma")
```

### Expanding the reports to adjacent links

```{r}
#| eval: false

grid_fill_p <- expand_grid(day_type = c("weekday","weekend"),
            hour = unique(minor_offences$hour)) |>
  arrange(day_type,hour)


exp_probs <-
  do.call(bind_rows, lapply(
    1:nrow(grid_fill_p),
    # 4:5,
    \(i) {
      ## Extracting the pair ids that have WWD reports at the day type-hour
      t_pair_ids <- minor_offences |>
        st_drop_geometry() |>
        filter(hour == grid_fill_p$hour[i], day_type == grid_fill_p$day_type[i]) |>
        pull(pair_id)
      
      
      if (length(t_pair_ids) == 0) {
        t_full_exp <- data.frame(
          from_id = NA_character_,
          to_id = NA_character_,
          p = NA_real_,
          hour = grid_fill_p$hour[i],
          day_type = grid_fill_p$day_type[i]
        )
      } else {
        subset_net_offence <- subset_net |>
          mutate(offence_bool = pair_id %in% t_pair_ids)
        
        
        
        
        fill_probs <- function(edges_df, direction = c("1", "-1")) {
          direction = match.arg(direction)
          
          if (direction == "1") {
            do.call(bind_rows, lapply(1:nrow(edges_df), \(j) {
              subset_net_offence |>
                st_drop_geometry() |>
                select(from_id, to_id) |>
                filter(to_id == edges_df$from_id[j],
                       from_id != edges_df$to_id[j]) |>
                mutate(p = edges_df$p[j] / n())
            }))
          } else {
            do.call(bind_rows, lapply(1:nrow(edges_df), \(j) {
              subset_net_offence |>
                st_drop_geometry() |>
                select(from_id, to_id) |>
                filter(from_id == edges_df$to_id[j],
                       to_id != edges_df$from_id[j]) |>
                mutate(p = edges_df$p[j] / n())
            }))
            
          }
        }
        
        
        expand_reports <- function(df, max_degree = 3) {
          check0 <- df |>
            st_drop_geometry() |>
            select(from_id, to_id) |>
            mutate(p = 1)
          
          check <- list()
          check[[1]] <- fill_probs(check0)
          for (i in 2:max_degree) {
            if (nrow(check[[i - 1]]) < 1)
              break
            check[[i]] <- fill_probs(check[[i - 1]])
          }
          
          checkr <- list()
          checkr[[1]] <- fill_probs(check0, direction = "-1")
          for (i in 2:max_degree) {
            if (nrow(checkr[[i - 1]]) < 1)
              break
            checkr[[i]] <- fill_probs(checkr[[i - 1]], direction = "-1")
          }
          
          ckeck_df <- bind_rows(check0,
                                do.call(bind_rows, check),
                                do.call(bind_rows, checkr)) |>
            summarise(across(p, max), .by = c(from_id, to_id))
          return(ckeck_df)
        }
        
        t_full_exp <- subset_net_offence |>
          filter(offence_bool) |>
          expand_reports() |>
          mutate(hour = grid_fill_p$hour[i], day_type = grid_fill_p$day_type[i])
      }
      
      return(t_full_exp)
    }
  ))

```

```{r}
#| eval: false

clean_probs <- exp_probs |>
  drop_na(from_id, to_id) |>
  summarise(p = max(p),
            .by = c(day_type, hour, from_id, to_id)) |> 
  tibble()
```

```{r}
#| eval: false
#| include: false

save(clean_probs,file = "sf_network/clean_probs_alloff.Rdata")
```

```{r}
#| include: false
load(file = "sf_network/clean_probs_alloff.Rdata")
```

```{r}
clean_probs_ids <- subset_net |>
  st_drop_geometry() |>
  select(from_id, to_id,pair_id) |> 
  inner_join(clean_probs,
            by = join_by(from_id, to_id)) |> 
  summarise(across(p,\(x) max(x,na.rm = T)),
            .by = c(day_type,hour,pair_id))
```

A visual check

```{r}
simpl_network_sf |> 
  left_join(clean_probs_ids |> 
              filter(day_type=="weekday",
                     hour == 18),
            by = "pair_id") |> 
  tm_shape()+
  tm_lines("p")
```

```{r}
model_data_p <- model_data |> 
  left_join(clean_probs_ids,
  by = join_by(pair_id,hour,day_type)) |> 
  mutate(p = if_else(is.na(p),0,p))
```

### Analysis by sectors

We will use classification of Local Planning Units as analysis units.
For that purpose, we load the `gkpg` file which has been extracted from
the Reference map (available
[here](https://www.ideca.gov.co/recursos/mapas/mapa-de-referencia-para-bogota-dc)).

```{r}
#| label: get-LPU-boundaries

dir.create("raw_data",showWarnings = F)

if(!file.exists(file.path("raw_data", "UPL_Bogota.zip"))) {
  u <-
    "https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/UPL_Bogota.zip"
  download.file(u, file.path("raw_data", basename(u)), mode = "wb")
  unzip(zipfile = file.path("raw_data", basename(u)), exdir = "raw_data")
}

lpu_boundaries <- st_read("raw_data/MR_VR0924_UPL.gpkg")
```

LPU have been classified according to official sources (see this). North
and Northwest sectors have been merged. Only urban sectors are
considered.

```{r}
lpu_sector <- read_csv("raw_data/UPL_sectors.csv",
                       col_types = cols(
                         CODIGO_UPL = col_character(),
                         Sector = col_character()
                         ))

sector_boundaries <- lpu_boundaries |> 
  left_join(lpu_sector,
            by = "CODIGO_UPL") |> 
  mutate(Sector = str_remove(Sector,"(?<=North)west")) |> 
  group_by(Sector) |> 
  summarise(geom = st_union(geom)) |> 
  filter(Sector != "Rural") |> 
  st_transform(st_crs(simpl_network_sf))
```

This shows

```{r}
tm_shape(sector_boundaries) +
  tm_fill("Sector",
          fill.scale = tm_scale_categorical(values = "brewer.set3"),
          fill_alpha = 0.7) +
  tm_shape(major_net) +
  tm_lines("gray60",col_alpha = .6)
```

Spatial Join

```{r}
sector_boundaries$n_reports <- st_intersects(sector_boundaries,minor_offences) |> vapply(length,numeric(1))
sector_boundaries
```

```{r}
grid_off <- st_intersects(sector_boundaries,simpl_network_sf)
```

```{r}
minor_offences$sector <- st_intersects(minor_offences,sector_boundaries) |> vapply(\(x) (x),numeric(1))

simpl_subset_sf$sector <- 
  simpl_subset_sf |>
  st_centroid() |>
  st_intersects(sector_boundaries) |>
  vapply(\(x) {
    if (length(x) == 0) {
      NA
    } else{
      x
    }
  }, numeric(1)) 

pair_2_sector <- simpl_subset_sf |> 
  st_drop_geometry() |> 
  select(pair_id,sector)
```

```{r}
summary_sector_off <- minor_offences |> 
  st_drop_geometry() |> 
  summarise(n = n(),.by = c(day_type,hour,sector)) |> 
  arrange(day_type,hour)
```

```{r}
ggplot(summary_sector_off,
       aes(x = hour, y = n,col = factor(sector)))+
  geom_line()+
  facet_grid(day_type~.)+
  scale_color_brewer(type = "qual",
                     palette = "Set3")+
  theme_minimal()
  
```

```{r}
model_data_p_sec <- model_data_p |> 
  left_join(pair_2_sector, by = join_by(pair_id)) |> 
  drop_na(sector)
```

### Creating pseudo-absences

Under the assumption that the transport police have the same number of
officers allocated along the day. We can assume that the sample size
(the number of monitored roads) is the same.

In this case we will assume that 10% of the roads are constantly
monitored. Also, it is assumed that the allocation of officers is
proportional to the size of the network.

```{r}
sample_size <-  0.1
```

Calculating the actual sample size for each sector.

```{r}
sample_size_sector <- model_data_p_sec |>
  count(day_type,hour,sector,
        name = "n_links") |> 
  select(sector,n_links) |> 
  unique() |> 
  mutate(sample_size = round(sample_size*n_links,-1),.keep = "none",.by = sector)
sample_size_sector |> kable()
```

Based on these sample sizes, we will randomly sample different roads
within the sector as pseudo-absences. Pseudo-absences are used in
ecology when working with presence-only data and no actual absences are
recorded, for example, recording presence of exotic species.

```{r}
set.seed(1234)
model_data_sampled <- model_data_p_sec |>  
  nest(.by = c(day_type,hour,sector)) |> 
  left_join(sample_size_sector,by="sector") |>
  mutate(
    n_true = map_dbl(data,\(.x) {
      .x |> filter(offence_bool) |> nrow()
      }),
    obs = map(data,\(.x) {
      .x |> filter(offence_bool) 
      }),
  pseudoabs = map2(.x = data,
                       .y = sample_size-n_true,
                       .f = \(.x,.y) {
                         .x |>
                           filter(p==0) |>
                           slice_sample(n = .y)
                         }
                       ),
  model_data = map2(.x = obs,
                    .y = pseudoabs,
                    .f = \(.x,.y){
                      bind_rows(.x,.y)
                    })
         ) |> 
  select(day_type:sector,n_true,sample_size,model_data) |> 
  unnest(model_data)
```

```{r}
model_data_sampled_mlm <- model_data_sampled |>
  filter(day_type == "weekday",between(hour,5,20)) |> 
  mutate(logdiff_max_gmc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour)) |> 
  mutate(logdiff_max_smc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour,sector))
```

```{r}
# saveRDS(model_data_sampled_mlm,"model_data.RDS")
# model_data_sampled_mlm <- readRDS("model_data.RDS") |> data.frame() |> filter()
```

### Fitting a multilevel model

```{r}
m0a <- glmer(offence_bool ~ (1|sector),
            data = model_data_sampled_mlm,family = "binomial")
summary(m0a)

m0b <- glmer(offence_bool ~ (1|hour),
            data = model_data_sampled_mlm,family = "binomial")
summary(m0b)
```

#### Intraclass Correlation Coefficient (ICC)

The chances of having a offence report is explained by between-sector
differences

```{r}
icca <- m0a@theta[1]^2/ (m0a@theta[1]^2 + (pi^2/3))
icca
```

The chances of having a offence report is explained by between-hour
differences

```{r}
iccb <- m0b@theta[1]^2/ (m0b@theta[1]^2 + (pi^2/3))
iccb
```

#### Constrained intermediate models

```{r}
CIM <- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1|hour),
            data = model_data_sampled_mlm,family = "binomial")

summary(CIM)
```

```{r}
AIMa <- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc|sector)+(1|hour),
            data = model_data_sampled_mlm,family = "binomial")

summary(AIMa)
```

```{r}
AIMb <- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1 + logdiff_max_gmc||hour),
            data = model_data_sampled_mlm,family = "binomial")

summary(AIMb)
```

```{r}
AIMc <- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc||sector)+(1 + logdiff_max_gmc||hour),
            data = model_data_sampled_mlm,family = "binomial")

summary(AIMc)
```

```{r}
anova(CIM,AIMa,AIMb,AIMc)
```

```{r}
extract_eq(AIMa)
```



$$
\begin{aligned}
  \operatorname{offence\_bool}_{i}  &\sim \operatorname{Binomial}(n = 1, \operatorname{prob}_{\operatorname{offence\_bool} = 1} = \widehat{P}) \\
    \log\left[\frac{\hat{P}}{1 - \hat{P}} \right] &=\alpha_{j[i],k[i]} + \beta_{1}(\operatorname{diff\_max\_gmc}) \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for hour j = 1,} \dots \text{,J} \\
    \alpha_{k}  &\sim N \left(\mu_{\alpha_{k}}, \sigma^2_{\alpha_{k}} \right)
    \text{, for sector k = 1,} \dots \text{,K}
\end{aligned}

$$
CIM


```{r}
plot_slopes(CIM,
            variables = "logdiff_max_gmc",
            by = c("hour","sector"))+
  scale_color_brewer(type = "qual",
                     palette = "Set3")+
  scale_fill_brewer(type = "qual",
                      palette = "Set3")+
  theme_minimal()+
  scale_y_continuous(limits = c(0,0.01))+
  labs(col = "Sector",
       fill = "Sector")
```

AIMa

```{r}
plot_slopes(AIMa,
            variables = "logdiff_max_gmc",
            by = c("hour","sector"))+
  scale_color_brewer(type = "qual",
                     palette = "Set3")+
  scale_fill_brewer(type = "qual",
                      palette = "Set3")+
  theme_minimal()+
  scale_y_continuous(limits = c(0,0.01))+
  labs(col = "Sector",
       fill = "Sector")
```




## Daily potential

```{r}
n_d10_logdiff_max <- simpl_network_sf |>
  left_join(summary_pairs,by = "pair_id") |>  
  mutate(d10_bool = logdiff_max >= quantile(logdiff_max, 0.90,na.rm = T),
         .by=c(hour,day_type,roadclass)) |> 
  st_drop_geometry() |> 
  summarise(n_d10 = sum(d10_bool,na.rm = T),.by = c(pair_id,day_type)) |> 
  pivot_wider(names_from = day_type,values_from = n_d10)
```

```{r}
map_ratruns_wk <- simpl_network_sf |>
  left_join(n_d10_logdiff_max, by = "pair_id") |>
  mutate(across(weekday:weekend, \(x) {
    if_else(roadclass == "residential", x, NA)
    })) |>
  mutate(across(weekday:weekend, list(
    ld = \(x) {
      if_else(roadclass != "residential", 1, x)
      }))) |>
  ggplot(aes(col =  weekday,
             linewidth = weekday_ld)) +
  geom_sf() +
  scale_colour_viridis_b(na.value = "gray30",
                         option = "plasma",
                         direction = -1) +
  scale_linewidth_continuous(
    limits = c(0, 24),
    range = c(0.05, 0.5),
    transform = "exp"
  ) +
  theme_void() +
  guides(linewidth = "none") +
  labs(col = "Rat-run potential") +
  theme(legend.position = "inside",
        legend.position.inside = c(0.2, 0.7))

map_ratruns_wk
```

```{r}
simpl_network_sf |>
  left_join(n_d10_logdiff_max, by = "pair_id") |>
  mutate(across(weekday:weekend, \(x) {
    if_else(roadclass == "residential", x, NA)
    })) |>
  mutate(across(weekday:weekend, list(
    ld = \(x) {
      if_else(roadclass != "residential", 1, x)
      })))
```
