[
  {
    "objectID": "D2_congestion_tests.html",
    "href": "D2_congestion_tests.html",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\npackageVersion (\"dodgr\")\n\n[1] '0.4.1.44'\n\n\n\n\n\nsf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019_full &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 3) |&gt; \n  st_transform(4326)\n# zb_view(bog_zone)\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,]\n\n\n\n\n\ndodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nmain_component &lt;- graph_bogota |&gt; data.frame() |&gt; count(component) |&gt; slice_max(n) |&gt; pull(component)\nclear_dodgr_cache()\n\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\nFixing the weighted time column\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncongested_contracted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\ncongested_contracted$time_weighted[congested_contracted$highway == \"road_60\"] &lt;- (3.6*congested_contracted$d_weighted[congested_contracted$highway == \"road_60\"])/30\n\nCalculating the centrality\n\ncongested_centrality &lt;- congested_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nExporting the graph to sf object\n\nsf_net0 &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nsf_net &lt;- sf_net0 |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-1,1))\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis section includes the code for joining the speed data network with OSM network\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[bog_zone |&gt; st_transform(3116) ,]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\nA quick visualisation of the network that could be part of the correspondence\n\ntm_shape(sf_bog_major[speed_buffer,])+\n  tm_lines()+\n  tm_shape(speed_buffer)+\n  tm_polygons(\"blue\",alpha = 0.3)\n\n\n\n\n\n\n\n\nusing the spatial operation st_intersects we select the links that can be related to the buffer.\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nA quick check on the distribution of overlaps\n\noverlap_buffer |&gt; \n  ggplot(aes(n))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nThe following code will produce plots for all links with one-to-many correspondence. However, we will calculate the average of the speed of the observed speeds\n\n# for (j in 1:nrow(overlap_buffer)){\n#   mmap &lt;- tm_shape(speed_buffer[speed_buffer$TID %in% TID_to_edge_id$TID[TID_to_edge_id$edge_id==overlap_buffer$edge_id[j]],])+\n#     tm_polygons(\"TID\",alpha = 0.3)+\n#     tm_shape(sf_bog_major[sf_bog_major$edge_id == overlap_buffer$edge_id[j],])+\n#     tm_lines(\"yellow\")\n#   print(mmap)\n#   }\n\nTo check if there is any link in the speed dataset that has not been linked to any object of the road network.\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 0\n\n\n\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nWe identify the hour that showed the lowest speeds in average\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) \n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday     18           0.434\n2  2019 weekday     17           0.453\n3  2019 weekday     12           0.461\n4  2019 weekday      9           0.463\n5  2019 weekday     11           0.463\n\n\nThe morning peak is selected as the number of WWD reports is higher. To compare, the speeds for the same hour in a weekend will be extracted\n\nmin_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekday\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nwkend_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekend\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nspeed_tbl_wkend &lt;- speed_data |&gt;\n  semi_join(wkend_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nwkend_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl_wkend,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\n\nbog_contr_wkend &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_wkend$obs_speed &lt;- tibble(edge_id = bog_contr_wkend$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_wkend$time_weighted[!is.na(bog_contr_wkend$obs_speed)] &lt;- (3.6*bog_contr_wkend$d_weighted[!is.na(bog_contr_wkend$obs_speed)])/bog_contr_wkend$obs_speed[!is.na(bog_contr_wkend$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nThe following code produce some quick visualisation of the differences\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-2,2))\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nlength(colSums(dist_matrix)[is.na(colSums(dist_matrix,na.rm = T))])\n\n[1] 0\n\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\n\nsf_net_jct &lt;- sf_net0 |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))\n\n\n\n\n\n\n\nWe assumed an initial speed of 10 km/h for the links that represent the wrong direction. But that choice is arbitrary. The following section will produce the results for multiple speeds, to compare the changes in centrality based on the assumed fre-flow speed of wrong-way links. First, we will test the changes on the free-flow network\n\ntest_centralities &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contracted\n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 *\n                                                                                                 bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities |&gt;\n  pivot_longer(cols = any_of(names(test_centralities)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contracted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\ntest_centralities_cong &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contr_adjusted \n                               \n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               \n                               \n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                               \n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities_cong |&gt;\n  pivot_longer(cols = any_of(names(test_centralities_cong)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contr_adjusted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities_cong) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\nwwlinks &lt;- bind_cols(\n  tibble(highway = bog_contr_adjusted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities_cong) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"ff_cent\"),\n  tibble(highway = bog_contracted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"cong_cent\")\n)\n\nA simple linear model\n\nm1 &lt;- lm(ff_cent~cong_cent+0, data = wwlinks)\nsummary(m1)\n\n\nCall:\nlm(formula = ff_cent ~ cong_cent + 0, data = wwlinks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4685421    -6166        0        6 10459696 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \ncong_cent   1.6457     0.0129   127.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 334200 on 84412 degrees of freedom\nMultiple R-squared:  0.1616,    Adjusted R-squared:  0.1616 \nF-statistic: 1.627e+04 on 1 and 84412 DF,  p-value: &lt; 2.2e-16\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent)/(0.5*(cong_cent+ff_cent)),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n# t_thresholds = round(30*1.2^seq(0,30,2),-1) |&gt; unique()\nt_thresholds = seq(1,25,3)*60\n\ngrid_test &lt;- expand_grid(v = seq(0, 30, 3),\n                         th = t_thresholds)\n\n\ntest_centralities_threshold &lt;- do.call(bind_cols,\n                                       lapply(1:nrow(grid_test),\n                                              # lapply(1:2,\n                                              \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_adjusted\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntest_centralities_threshold_wkend &lt;- do.call(bind_cols,\n                                             lapply(1:nrow(grid_test),\n                                                    # lapply(1:2,\n                                                    \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_wkend\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntidy_test &lt;- test_centralities_threshold |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\ntidy_test_wkend &lt;- test_centralities_threshold_wkend |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\nExploration of distributions for the entire network\n\ntidy_test |&gt;\n  arrange(dist.th) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\nFor one-way residential links\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = logreldiff.ff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\n\nsample_ids &lt;- bog_contr_adjusted |&gt;\n  data.frame() |&gt;\n  filter(highway ==\"road_10\") |&gt;\n  pull(edge_id) |&gt;\n  sample(15)\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\",edge_id %in% sample_ids) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = dist.th,\n             y = logdiff,\n             group = factor(edge_id),col = factor(edge_id)))+\n  geom_line(alpha = 0.3)+\n  theme_minimal()+\n  facet_wrap(wwd.speed~.)+\n  scale_x_continuous(breaks = t_thresholds,labels = round(t_thresholds/60,1))+\n  theme(panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\n\nwrite_csv(tidy_test, file = \"sf_network/cent_tests.csv\")\nwrite_csv(tidy_test_wkend, file = \"sf_network/cent_tests_wkend.csv\")\n\nst_write(sf_net_jct, \"sf_network/small_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting layer `small_sf_network' to data source \n  `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting 41440 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "D2_congestion_tests.html#loading-network",
    "href": "D2_congestion_tests.html#loading-network",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "sf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019_full &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 3) |&gt; \n  st_transform(4326)\n# zb_view(bog_zone)\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,]"
  },
  {
    "objectID": "D2_congestion_tests.html#baseline-graph-building",
    "href": "D2_congestion_tests.html#baseline-graph-building",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "dodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nmain_component &lt;- graph_bogota |&gt; data.frame() |&gt; count(component) |&gt; slice_max(n) |&gt; pull(component)\nclear_dodgr_cache()\n\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\nFixing the weighted time column\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncongested_contracted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\ncongested_contracted$time_weighted[congested_contracted$highway == \"road_60\"] &lt;- (3.6*congested_contracted$d_weighted[congested_contracted$highway == \"road_60\"])/30\n\nCalculating the centrality\n\ncongested_centrality &lt;- congested_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nExporting the graph to sf object\n\nsf_net0 &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nsf_net &lt;- sf_net0 |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-1,1))\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")"
  },
  {
    "objectID": "D2_congestion_tests.html#using-the-actual-speeds-for-adjusting-graph",
    "href": "D2_congestion_tests.html#using-the-actual-speeds-for-adjusting-graph",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "This section includes the code for joining the speed data network with OSM network\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[bog_zone |&gt; st_transform(3116) ,]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\nA quick visualisation of the network that could be part of the correspondence\n\ntm_shape(sf_bog_major[speed_buffer,])+\n  tm_lines()+\n  tm_shape(speed_buffer)+\n  tm_polygons(\"blue\",alpha = 0.3)\n\n\n\n\n\n\n\n\nusing the spatial operation st_intersects we select the links that can be related to the buffer.\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nA quick check on the distribution of overlaps\n\noverlap_buffer |&gt; \n  ggplot(aes(n))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nThe following code will produce plots for all links with one-to-many correspondence. However, we will calculate the average of the speed of the observed speeds\n\n# for (j in 1:nrow(overlap_buffer)){\n#   mmap &lt;- tm_shape(speed_buffer[speed_buffer$TID %in% TID_to_edge_id$TID[TID_to_edge_id$edge_id==overlap_buffer$edge_id[j]],])+\n#     tm_polygons(\"TID\",alpha = 0.3)+\n#     tm_shape(sf_bog_major[sf_bog_major$edge_id == overlap_buffer$edge_id[j],])+\n#     tm_lines(\"yellow\")\n#   print(mmap)\n#   }\n\nTo check if there is any link in the speed dataset that has not been linked to any object of the road network.\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 0\n\n\n\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nWe identify the hour that showed the lowest speeds in average\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) \n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday     18           0.434\n2  2019 weekday     17           0.453\n3  2019 weekday     12           0.461\n4  2019 weekday      9           0.463\n5  2019 weekday     11           0.463\n\n\nThe morning peak is selected as the number of WWD reports is higher. To compare, the speeds for the same hour in a weekend will be extracted\n\nmin_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekday\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nwkend_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekend\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nspeed_tbl_wkend &lt;- speed_data |&gt;\n  semi_join(wkend_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nwkend_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl_wkend,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\n\nbog_contr_wkend &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_wkend$obs_speed &lt;- tibble(edge_id = bog_contr_wkend$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_wkend$time_weighted[!is.na(bog_contr_wkend$obs_speed)] &lt;- (3.6*bog_contr_wkend$d_weighted[!is.na(bog_contr_wkend$obs_speed)])/bog_contr_wkend$obs_speed[!is.na(bog_contr_wkend$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nThe following code produce some quick visualisation of the differences\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-2,2))\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nlength(colSums(dist_matrix)[is.na(colSums(dist_matrix,na.rm = T))])\n\n[1] 0\n\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\n\nsf_net_jct &lt;- sf_net0 |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))"
  },
  {
    "objectID": "D2_congestion_tests.html#tests-for-wrong-direction-speed",
    "href": "D2_congestion_tests.html#tests-for-wrong-direction-speed",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "We assumed an initial speed of 10 km/h for the links that represent the wrong direction. But that choice is arbitrary. The following section will produce the results for multiple speeds, to compare the changes in centrality based on the assumed fre-flow speed of wrong-way links. First, we will test the changes on the free-flow network\n\ntest_centralities &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contracted\n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 *\n                                                                                                 bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities |&gt;\n  pivot_longer(cols = any_of(names(test_centralities)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contracted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\ntest_centralities_cong &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contr_adjusted \n                               \n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               \n                               \n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                               \n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities_cong |&gt;\n  pivot_longer(cols = any_of(names(test_centralities_cong)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contr_adjusted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities_cong) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\nwwlinks &lt;- bind_cols(\n  tibble(highway = bog_contr_adjusted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities_cong) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"ff_cent\"),\n  tibble(highway = bog_contracted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"cong_cent\")\n)\n\nA simple linear model\n\nm1 &lt;- lm(ff_cent~cong_cent+0, data = wwlinks)\nsummary(m1)\n\n\nCall:\nlm(formula = ff_cent ~ cong_cent + 0, data = wwlinks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4685421    -6166        0        6 10459696 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \ncong_cent   1.6457     0.0129   127.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 334200 on 84412 degrees of freedom\nMultiple R-squared:  0.1616,    Adjusted R-squared:  0.1616 \nF-statistic: 1.627e+04 on 1 and 84412 DF,  p-value: &lt; 2.2e-16\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent)/(0.5*(cong_cent+ff_cent)),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")"
  },
  {
    "objectID": "D2_congestion_tests.html#tests-for-distance-threshold",
    "href": "D2_congestion_tests.html#tests-for-distance-threshold",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "# t_thresholds = round(30*1.2^seq(0,30,2),-1) |&gt; unique()\nt_thresholds = seq(1,25,3)*60\n\ngrid_test &lt;- expand_grid(v = seq(0, 30, 3),\n                         th = t_thresholds)\n\n\ntest_centralities_threshold &lt;- do.call(bind_cols,\n                                       lapply(1:nrow(grid_test),\n                                              # lapply(1:2,\n                                              \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_adjusted\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntest_centralities_threshold_wkend &lt;- do.call(bind_cols,\n                                             lapply(1:nrow(grid_test),\n                                                    # lapply(1:2,\n                                                    \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_wkend\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntidy_test &lt;- test_centralities_threshold |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\ntidy_test_wkend &lt;- test_centralities_threshold_wkend |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\nExploration of distributions for the entire network\n\ntidy_test |&gt;\n  arrange(dist.th) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\nFor one-way residential links\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = logreldiff.ff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\n\n\n\n\n\n\n\n\nsample_ids &lt;- bog_contr_adjusted |&gt;\n  data.frame() |&gt;\n  filter(highway ==\"road_10\") |&gt;\n  pull(edge_id) |&gt;\n  sample(15)\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\",edge_id %in% sample_ids) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = dist.th,\n             y = logdiff,\n             group = factor(edge_id),col = factor(edge_id)))+\n  geom_line(alpha = 0.3)+\n  theme_minimal()+\n  facet_wrap(wwd.speed~.)+\n  scale_x_continuous(breaks = t_thresholds,labels = round(t_thresholds/60,1))+\n  theme(panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\n\nwrite_csv(tidy_test, file = \"sf_network/cent_tests.csv\")\nwrite_csv(tidy_test_wkend, file = \"sf_network/cent_tests_wkend.csv\")\n\nst_write(sf_net_jct, \"sf_network/small_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting layer `small_sf_network' to data source \n  `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting 41440 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "D3_WWD_joining.html",
    "href": "D3_WWD_joining.html",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\",\n    \"ggExtra\",\n    \"effectsize\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap     ggExtra  effectsize \n       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n\n\n\n\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\noff_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\n\nReading layer `wwd_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/wwd_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6869 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.20585 ymin: 4.476179 xmax: -74.02131 ymax: 4.819386\nGeodetic CRS:  WGS 84\n\n#off_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\ncent_tests_wkend &lt;- read_csv(\"sf_network/cent_tests_wkend.csv\",\n                             lazy = F)\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\nsummary_pairs_wkend &lt;- cent_tests_wkend |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\noff_sf_wkend &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekend\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\nminor_offences_wkend &lt;- off_sf_wkend[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\nminor_offences_wkend$near_index &lt;- st_nearest_feature(minor_offences_wkend,simpl_network_sf)\nminor_offences_wkend$pair_id &lt;- simpl_network_sf$pair_id[minor_offences_wkend$near_index]\n\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nmodel_data_wkend &lt;- (summary_pairs_wkend |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences_wkend$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\n## Congested\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n## Weekend\nmodel_data_wkend |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\ntest1_wkend &lt;- model_data_wkend |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_wkend &lt;- model_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nmod_pred_0_wkend |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs &lt;- glm_models_0 |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\nmod_0_coefs_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\n\ncontrol_rates &lt;- mod_0_coefs_wkend |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\noddsratio_to_riskratio(glm_models_0$model_rel[[1]])\n\nParameter   | Risk Ratio |       95% CI\n---------------------------------------\n(Intercept) |   1.35e-03 |             \nlogdiff max |       0.13 | [0.03, 0.42]\n\nRR_summary &lt;-mod_0_coefs |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary |&gt; \n  ggplot(aes(x = RR, y = dist.th, col =wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nmodel_reports &lt;- model_data |&gt;\n  filter(offence_bool)\n\n\nrand_absences_data &lt;- bind_rows(\n  model_reports,\n  model_data |&gt;\n    filter(!offence_bool) |&gt;\n    sample_n(size = nrow(model_reports)*2, replace = F)\n)\n\n\n# Same links are considered\nrand_absences_data_wkend &lt;- model_data_wkend |&gt; \n  semi_join(rand_absences_data,by = join_by(pair_id,wwd.speed,dist.th))\n\n\nglm_models_0_rand &lt;- rand_absences_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand &lt;- glm_models_0_rand |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_rand_wkend &lt;- rand_absences_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand_wkend &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nmod_pred_0_rand |&gt;\n  filter(wwd.speed == 12) |&gt;\n  ggplot(aes(\n    x = logdiff_max,\n    y = offence_bool,\n    group = id,\n    col = dist.th\n  )) +\n  geom_line(alpha = 0.3) +\n  geom_line(\n    data = mod_pred_0_rand_wkend |&gt;\n      filter(wwd.speed == 12),\n    linetype = \"dashed\",\n    alpha = 0.6\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal() +\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs_rand &lt;- glm_models_0_rand |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\ncontrol_rates_rand &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\")) |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\nRR_summary_rand &lt;- mod_0_coefs_rand |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates_rand,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary_rand |&gt; \n  filter(between(wwd.speed,3,22),dist.th&gt;500) |&gt; \n  ggplot(aes(y = RR, x = e.value, col = wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point(alpha = 0.6)+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"RR\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  },
  {
    "objectID": "D3_WWD_joining.html#loading-data",
    "href": "D3_WWD_joining.html#loading-data",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "bog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\noff_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\n\nReading layer `wwd_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/wwd_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6869 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.20585 ymin: 4.476179 xmax: -74.02131 ymax: 4.819386\nGeodetic CRS:  WGS 84\n\n#off_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\ncent_tests_wkend &lt;- read_csv(\"sf_network/cent_tests_wkend.csv\",\n                             lazy = F)\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\nsummary_pairs_wkend &lt;- cent_tests_wkend |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\noff_sf_wkend &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekend\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\nminor_offences_wkend &lt;- off_sf_wkend[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\nminor_offences_wkend$near_index &lt;- st_nearest_feature(minor_offences_wkend,simpl_network_sf)\nminor_offences_wkend$pair_id &lt;- simpl_network_sf$pair_id[minor_offences_wkend$near_index]\n\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nmodel_data_wkend &lt;- (summary_pairs_wkend |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences_wkend$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\n## Congested\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n## Weekend\nmodel_data_wkend |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\ntest1_wkend &lt;- model_data_wkend |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_wkend &lt;- model_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nmod_pred_0_wkend |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs &lt;- glm_models_0 |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\nmod_0_coefs_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\n\ncontrol_rates &lt;- mod_0_coefs_wkend |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\noddsratio_to_riskratio(glm_models_0$model_rel[[1]])\n\nParameter   | Risk Ratio |       95% CI\n---------------------------------------\n(Intercept) |   1.35e-03 |             \nlogdiff max |       0.13 | [0.03, 0.42]\n\nRR_summary &lt;-mod_0_coefs |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary |&gt; \n  ggplot(aes(x = RR, y = dist.th, col =wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nmodel_reports &lt;- model_data |&gt;\n  filter(offence_bool)\n\n\nrand_absences_data &lt;- bind_rows(\n  model_reports,\n  model_data |&gt;\n    filter(!offence_bool) |&gt;\n    sample_n(size = nrow(model_reports)*2, replace = F)\n)\n\n\n# Same links are considered\nrand_absences_data_wkend &lt;- model_data_wkend |&gt; \n  semi_join(rand_absences_data,by = join_by(pair_id,wwd.speed,dist.th))\n\n\nglm_models_0_rand &lt;- rand_absences_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand &lt;- glm_models_0_rand |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_rand_wkend &lt;- rand_absences_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand_wkend &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nmod_pred_0_rand |&gt;\n  filter(wwd.speed == 12) |&gt;\n  ggplot(aes(\n    x = logdiff_max,\n    y = offence_bool,\n    group = id,\n    col = dist.th\n  )) +\n  geom_line(alpha = 0.3) +\n  geom_line(\n    data = mod_pred_0_rand_wkend |&gt;\n      filter(wwd.speed == 12),\n    linetype = \"dashed\",\n    alpha = 0.6\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal() +\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs_rand &lt;- glm_models_0_rand |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\ncontrol_rates_rand &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\")) |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\nRR_summary_rand &lt;- mod_0_coefs_rand |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates_rand,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary_rand |&gt; \n  filter(between(wwd.speed,3,22),dist.th&gt;500) |&gt; \n  ggplot(aes(y = RR, x = e.value, col = wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point(alpha = 0.6)+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"RR\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  },
  {
    "objectID": "1B_speed_data.html",
    "href": "1B_speed_data.html",
    "title": "Speed Data Processing",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"data.table\",\n    \"paletteer\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n        sf  tidyverse data.table  paletteer \n      TRUE       TRUE       TRUE       TRUE \n\nsetDTthreads(0)"
  },
  {
    "objectID": "1B_speed_data.html#preliminary-cleaning",
    "href": "1B_speed_data.html#preliminary-cleaning",
    "title": "Speed Data Processing",
    "section": "Preliminary cleaning",
    "text": "Preliminary cleaning\n\nall_data[,\n           INICIO := mdy_hms(INICIO,\n                             tz = \"America/Bogota\")\n         ][,\n           `:=`(year = year(INICIO))\n           ]\n\nNot all links have data for all months. So the first step is to identify the links with enough data per year.\n\nlinks_summary &lt;- unique(\n  all_data[,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"TID\",\"year\")]\n\n\nlinks_summary |&gt; \n  ggplot(aes(count))+\n  geom_histogram(binwidth = 1)+\n  scale_y_log10()+\n  scale_x_continuous(breaks = 0:12)+\n  facet_grid(year~.)\n\nLinks with data in 5 or more months per year will be used for this analysis.\n\nlinks_data = links_summary[,count := 1*(count&gt;=5)\n                           ][,.(tot_count = sum(count)),\n                             by = \"TID\"][\n                               tot_count == 2,\n                               ] |&gt; pull(TID)\n\nWe extract the data only for these links with the following code and clean the memory:\n\nclean_data &lt;- all_data[TID %in% links_data]\nrm(all_data,links_data,links_summary,lst_files)\ngc()"
  },
  {
    "objectID": "1B_speed_data.html#classifiying-the-data",
    "href": "1B_speed_data.html#classifiying-the-data",
    "title": "Speed Data Processing",
    "section": "Classifiying the data",
    "text": "Classifiying the data\nFirst, we will identify the days that were bank holidays in Colombia\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nclean_data[date(INICIO) %in% bank_holidays,\n           DIA_SEMANA := \"Festivo\"\n           ]\n\nclean_data[,day_type := fcase(\n             DIA_SEMANA == \"Domingo\",\"weekend\",\n             DIA_SEMANA == \"Sabado\",\"weekend\",\n             DIA_SEMANA == \"Festivo\",\"weekend\",\n             DIA_SEMANA == \"Viernes\",\"friday\",\n             default = \"weekday\")\n             ]\n\nThe following code extracts the 94th percentile of the speeds in each road link.\n\nmax_speeds &lt;- clean_data[,\n                             .(p94_speed = quantile(VEL_PROMEDIO,\n                                                       0.94,\n                                                       na.rm = T)),\n                             by =  c(\"TID\",\"year\")]\n\nNow, we calculate the median hourly speed for each road link by day type and road link.\n\nsummary_speeds &lt;- clean_data[, as.list(summary(VEL_PROMEDIO)),\n  by = .(TID, HORA, day_type, year)]\n\nsetnames(summary_speeds,\n         old = c(\"HORA\",\"Min.\",\"1st Qu.\",\"Median\",\"Mean\",\"3rd Qu.\",\"Max.\"),\n         new = c(\"hour\",\"d_min_speed\",\"d_q1_speed\",\"d_median_speed\",\"d_mean_speed\",\"d_q3_speed\",\"d_max_speed\"))\n\nNow we normalise the values by dividing by the 95th-percentile speed\n\nnorm_summary_spd &lt;- merge(summary_speeds,\n                          max_speeds,\n                          by = c(\"TID\",\"year\"))[,\n                                                d_norm_speed := d_median_speed/p94_speed\n                                                ]\n\n\nfwrite(norm_summary_spd,file = \"sf_network/summary_speeds.csv\")"
  },
  {
    "objectID": "1B_speed_data.html#some-visualisations-of-speed-distribution",
    "href": "1B_speed_data.html#some-visualisations-of-speed-distribution",
    "title": "Speed Data Processing",
    "section": "Some visualisations of speed distribution:",
    "text": "Some visualisations of speed distribution:\n\nHourly distributions\n\nNormalised speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  geom_jitter(alpha = 0.1, size = 0.05,col = \"gray60\")+\n  geom_boxplot(aes(group = hour), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1,\n               alpha = 0.6,\n               show.legend = F)+\n  facet_grid(day_type~year)+\n  theme_minimal()+\n  labs(x = \"Hour\",\n       y = bquote(\"Normalised speed *\"),\n       caption = expression('*'~Observed~'/'~94^{'th'}~percentile))+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  theme(axis.text.x = element_text(angle = 90))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))\n\n\n\n\n\n\n\n\n\nnorm_summary_spd |&gt; \n  mutate(day_type = str_to_title(day_type)) |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  # geom_jitter(alpha = 0.05, size = 0.05,col = \"gray80\",shape =1)+\n  # geom_boxplot(aes(group = hour), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  stat_summary(aes(fill = day_type),\n               fun.data = median_hilow, fun.args = list(conf.int = 1), \n                 geom = 'ribbon', alpha = 0.15, colour = NA)+\n  stat_summary(aes(fill = day_type),\n               fun.data = median_hilow, fun.args = list(conf.int = 0.5), \n                 geom = 'ribbon', alpha = 0.4, colour = NA)+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1,\n               alpha = 1,\n               show.legend = F)+\n  facet_grid(day_type~.)+\n  theme_minimal()+\n  labs(x = NULL,\n       y = bquote(\"Normalised speed *\"),\n       caption = expression('*'~Observed~'/'~94^{'th'}~percentile))+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  theme(axis.text.x = element_text(angle = 90),legend.position = \"none\",\n        panel.grid.minor.x = element_blank())+\n  scale_colour_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))+\n  scale_fill_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))+\n  scale_y_continuous(limits = c(0,1.5))\n\n\n\n\n\n\n\n\n\n\nAbsolute speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_median_speed))+\n  geom_jitter(alpha = 0.3, size = 0.3,col = \"gray60\")+\n  geom_boxplot(aes(group = hour), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1,\n               alpha = 0.6,\n               show.legend = F)+\n  facet_grid(day_type~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Observed speed\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  theme(axis.text.x = element_text(angle = 90))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))\n\n\n\n\n\n\n\n\n\n\n\nDaily profile by Link\n\nNormalised speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  # geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  # geom_jitter(alpha = 0.3, size = 0.3,col = \"gray60\")+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type,\n                   group = TID),\n               linewidth = 0.01,\n               alpha = 0.1,\n               # show.legend = F\n               )+\n  facet_grid(day_type~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Speed ratio (observed/94th percentile)\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nAbsolute speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_median_speed))+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type,\n                   group = TID),\n               linewidth = 0.01,\n               alpha = 0.1,\n               # show.legend = F\n               )+\n  facet_grid(day_type~year)+\n  geom_hline(yintercept = 60,col = \"#EE4C97\",linetype = \"dashed\",alpha = 0.6,linewidth = 1)+\n  annotate(geom = \"text\",x = 23,y = 63,label = \"Speed Limit \",vjust = 0,hjust = 1,face = \"italic\")+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Observed speed\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,100),breaks = seq(0,100,20))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nMedian overall speed profile\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1.5,\n               alpha = 0.6,\n               # show.legend = F\n               )+\n  facet_grid(.~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Speed ratio (observed/94th percentile)\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\")"
  },
  {
    "objectID": "1B_speed_data.html#spatial-data",
    "href": "1B_speed_data.html#spatial-data",
    "title": "Speed Data Processing",
    "section": "Spatial data",
    "text": "Spatial data\nUp to this point, all the data processing has not involved the spatial component. On the open data platform, it is possible to download the gpkg files for each month. With the following code, we will identify which file(s) is(are) needed to have all data.\nIdeally, we need a file that contains all 770 TID.\n\nunique(\n  clean_data[,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"year\",\"MES\")][count == max(count)]\n\n    year      MES count\n   &lt;int&gt;   &lt;char&gt; &lt;int&gt;\n1:  2019 February   767\n\n\nAs the file of February 2019 does not have all the links, we identify alternative files.\n\ntid_feb2019 &lt;- unique(clean_data[year == 2019 & MES == \"February\",\"TID\"])[,1]\ntid_missing &lt;- unique(clean_data[!(TID %in% tid_feb2019$TID),\"TID\"])\n\nunique(\n  clean_data[TID %in% tid_missing$TID,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"year\",\"MES\")][count == max(count)]\n\n     year       MES count\n    &lt;int&gt;    &lt;char&gt; &lt;int&gt;\n 1:  2020     April     3\n 2:  2019    August     3\n 3:  2020    August     3\n 4:  2019  December     3\n 5:  2020  December     3\n 6:  2020   January     3\n 7:  2020  February     3\n 8:  2019      July     3\n 9:  2020      July     3\n10:  2020      June     3\n11:  2020     March     3\n12:  2020       May     3\n13:  2019  November     3\n14:  2020  November     3\n15:  2019   October     3\n16:  2020   October     3\n17:  2019 September     3\n18:  2020 September     3\n\n\nThe files for February 2019 and October 2020 have been downloaded manually from the same source. As we are only interested in the geometries, we filter out all the other data.\n\nsf_feb2019&lt;- st_read(\n   \"raw_data/bogota/speed_data/Velocidades_Bitcarrier_Febrero_2019_1361955177739874723.gpkg\",\n   query=\"select TID,SHAPE from 'Velocidades_Bitcarrier_Febrero_2019'\"\n   ) |&gt;\n  filter(TID %in% tid_feb2019$TID) |&gt; select(TID) |&gt; slice_head(n = 1,by = TID)\n\nReading query `select TID,SHAPE from 'Velocidades_Bitcarrier_Febrero_2019''\nfrom data source `/home/juan/P1_ratruns_analysis/raw_data/bogota/speed_data/Velocidades_Bitcarrier_Febrero_2019_1361955177739874723.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1930309 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_oct2020 &lt;- st_read(\n  \"raw_data/bogota/speed_data/Velocidades_Bitcarrier_Octubre_2020_-1518838627102027019.gpkg\",\n  query=\"select TID,SHAPE from 'Velocidades_Bitcarrier_Octubre_2020'\") |&gt;\n  filter(TID %in% tid_missing$TID) |&gt; slice_head(n = 1,by = TID)\n\nReading query `select TID,SHAPE from 'Velocidades_Bitcarrier_Octubre_2020''\nfrom data source `/home/juan/P1_ratruns_analysis/raw_data/bogota/speed_data/Velocidades_Bitcarrier_Octubre_2020_-1518838627102027019.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2325212 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.20333 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nsf_speed &lt;- bind_rows(sf_feb2019,sf_oct2020)\nrm(sf_feb2019,sf_oct2020)\ngc()\n\n            used   (Mb) gc trigger    (Mb)   max used    (Mb)\nNcells   1656047   88.5   41198428  2200.3   51498035  2750.3\nVcells 837985423 6393.4 1428780161 10900.8 1427212474 10888.8\n\n\nThe elements in the spatial objects contain MULTILINESTRINGS which cover long sections of the main corridors. Before saving the results, we will cast all features as LINESTRING to ease the process of working out a correspondence with the OSM network.\n\nsf_speed_cast &lt;- sf_speed |&gt; st_cast(\"LINESTRING\")\n\nSaving the network\n\nst_write(sf_speed_cast,dsn = \"sf_network/sf_speed_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/sf_speed_network.gpkg' using driver `GPKG'\nWriting layer `sf_speed_network' to data source \n  `sf_network/sf_speed_network.gpkg' using driver `GPKG'\nWriting 1267 features with 1 fields and geometry type Line String.\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\")\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nA quick visualisation of the maximum speeds in 2019:\n\nsf_speed_cast[urban_perimeter,] |&gt; \n  left_join(max_speeds |&gt; filter(year==2019), by = \"TID\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = p94_speed),linewidth = 0.3,alpha = 0.7)+\n  scale_color_gradientn(colours = paletteer_c(\"grDevices::Plasma\", 30))+\n  theme_void()+\n  facet_grid(.~year)"
  },
  {
    "objectID": "D1_graph_centrality_tests.html",
    "href": "D1_graph_centrality_tests.html",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\n\n\n\n\nsf_bogota_2019_full &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                          layer = \"network_2019\")\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",n_circles = 2) |&gt;\n  st_transform(st_crs(sf_bogota_2019_full))\n# zb_view(bog_zone)\n\nThe following the code will clip the network; also, since we will be using wrong-way-driving, we have to allow both directions inone-way links to capture the changes of centrality.\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,] |&gt; \n  mutate(oneway = if_else(highway %in% c(\"residential\"),\"no\",oneway))\n\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\n\n\n\nIn order to include the actual speed along each corridor, I will specify 60 different classes to be applied to each road link. By default, speeds are assigned by road link by road type, e.g. any trunk road in Bogotá has a 60 km/h speed.\n\n# custom_wp &lt;- dodgr::weighting_profiles$weighting_profiles |&gt; filter(name==\"motorcar\")\ncustom_penalties &lt;- dodgr::weighting_profiles$penalties |&gt; filter(name==\"motorcar\")\ncustom_surfaces &lt;- dodgr::weighting_profiles$surface_speeds |&gt; filter(name==\"motorcar\")\n\ncustom_wp &lt;- data.frame(name = \"motorcar\",\n                        way = paste0(\"road_\",1:60),\n                        value = 1,\n                        max_speed = as.numeric(1:60))\n\ncustom_wp_list &lt;- list(weighting_profiles = custom_wp,\n                       surface_speeds = custom_surfaces,\n                       pealties = custom_penalties)\n\nwpj &lt;- jsonlite::toJSON (custom_wp_list, pretty = TRUE)\nwrite_lines(wpj,file = \"custom_wp_speeds.json\")\n\nAssigning a speed category based on the speed limit (baseline scenario)\n\nsf_bogota_2019_cwp &lt;- sf_bogota_2019 |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                                               TRUE ~ \"road_30\")) |&gt; \n  select(-highway)\n\nThe following code builds the graph using the new column instead of highway.\n\ngraph_bogota_custom &lt;- weight_streetnet(sf_bogota_2019_cwp,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\n\n\nWe calculate the centralities with both weighting profiles\n\ngraph_baseline_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\ngraph_cwp_centrality &lt;- graph_bogota_custom |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\nTo validate that the results are identical, we can run the following code\n\nidentical(graph_baseline_centrality$centrality,graph_cwp_centrality$centrality)\n\n[1] TRUE\n\n\n\n\n\n\nThere are four core alternatives for calculating the centrality using dodgr.\n\nbog_contracted[,\n    c(\"d_weighted\", \"d\", \"time_weighted\", \"time\")] |&gt;\n  pairs()\n\n\n\n\n\n\n\n\nAn initial check on the four alternatives reveal some inconsistencies in the time_weighted column.\nThere are 49 links in the contracted graph with differences in the weighted distance and the distance. The location of these edges will be inspected after calculating the centrality in Section 1.3.3.\nWhen multiple paths are available between two nodes in the contracted version of the graph, dodgr takes only the shortest weighted distance, which creates the difference. However, the resulting weighted time is not corrected and in some cases a 0 is returned.\nGiven this, we will recalculate the weighted time based on the weighted distance.\n\nbog_contracted$time_weighted[bog_contracted$time_weighted==0] &lt;- (3.6*bog_contracted$d_weighted[bog_contracted$time_weighted==0])/case_when(\n      bog_contracted$highway[bog_contracted$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\n\n\n\ncent_all &lt;- lapply(c(\"d_weighted\", \"d\", \"time_weighted\", \"time\"), \\(x) {\n  \n    df &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = x)\n  \n  tibble(edge_id = df$edge_id, cent = df$centrality) |&gt;\n    rename_with(.cols = \"cent\", .fn = \\(.y) {\n      paste(.y, x, sep = \"_\")\n    })\n  }) |&gt;\n  plyr::join_all(by = \"edge_id\",type = \"full\")\n\n\n\n\n\nigraph_bog &lt;- bog_contracted |&gt; \n  dodgr_to_igraph()\n\n\ncent_all_ig &lt;- cent_all |&gt; \n  left_join(\n    tibble(edge_id = bog_contracted$edge_id,\n       cent_ig_d = igraph::edge_betweenness(graph = igraph_bog,\n                                            weights = bog_contracted$d),\n       cent_ig_d_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                     weights = bog_contracted$d_weighted),\n       cent_ig_time = igraph::edge_betweenness(graph = igraph_bog,weights = bog_contracted$time),\n       cent_ig_time_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                        weights = bog_contracted$time_weighted)\n       ),\n    by = \"edge_id\")\n\n\nsummary(cent_all_ig[,2:9])\n\n cent_d_weighted       cent_d        cent_time_weighted   cent_time      \n Min.   :      0   Min.   :      0   Min.   :      0    Min.   :      0  \n 1st Qu.:   4526   1st Qu.:   4526   1st Qu.:   4479    1st Qu.:   4479  \n Median :  15900   Median :  15670   Median :  13560    Median :  13465  \n Mean   : 108354   Mean   : 108428   Mean   : 107514    Mean   : 107668  \n 3rd Qu.:  75756   3rd Qu.:  74517   3rd Qu.:  56990    3rd Qu.:  56265  \n Max.   :2606261   Max.   :2608137   Max.   :2679102    Max.   :2684680  \n   cent_ig_d       cent_ig_d_weighted  cent_ig_time     cent_ig_time_weighted\n Min.   :      0   Min.   :      0    Min.   :      0   Min.   :      0      \n 1st Qu.:   4526   1st Qu.:   4526    1st Qu.:   4479   1st Qu.:   4479      \n Median :  15670   Median :  15900    Median :  13465   Median :  13560      \n Mean   : 108428   Mean   : 108354    Mean   : 107668   Mean   : 107514      \n 3rd Qu.:  74517   3rd Qu.:  75756    3rd Qu.:  56265   3rd Qu.:  56990      \n Max.   :2608137   Max.   :2606261    Max.   :2684680   Max.   :2679102      \n\n\nCheck if igraph results are identical to the ones obtained with dodgr\n\nwith(cent_all_ig,identical(cent_ig_d_weighted,cent_d_weighted))\n\n[1] TRUE\n\nwith(cent_all_ig,identical(cent_ig_time_weighted,cent_time_weighted))\n\n[1] TRUE\n\n\n\npairs(cent_all_ig[,2:9])\n\n\n\n\n\n\n\n\nJoining the results to the sf object\n\nsf_net &lt;- graph_bogota |&gt;\n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\n\n\n\nsf_net |&gt;  \n  select(cent_d_weighted:cent_time) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = log(cent),linewidth = cent+1))+\n  facet_wrap(type~.)+\n  scale_color_viridis_c(\n    # direction = -1\n    )+\n  scale_linewidth_continuous(range = c(0.05,0.5),\n                             transform = scales::transform_boxcox(p = 2))+\n  theme_void()+\n  guides(linewidth = \"none\",)\n\n\n\n\n\n\n\n\nVisualising differences\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot(aes(cent))+\n  geom_histogram()+\n  facet_wrap(type~.)+\n  scale_x_continuous(limits = c(-1,1))\n\n\n\n\n\n\n\n\nInspecting significant changes\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  mutate(cent = case_when(abs(cent)&lt;0.1~0,\n                          cent &gt; 0.6 ~ 1,\n                          -cent &gt; 0.6 ~ -1,\n                          is.na(cent)~NA)) |&gt; \n  ggplot()+\n  geom_sf(aes(col = cent))+\n  facet_wrap(type~.)+\n  scale_color_distiller(palette = \"Spectral\", direction = 1)+\n  # scale_linewidth_continuous(range = c(0.05,0.5),\n  #                            transform = scales::transform_boxcox(p = 2))+\n  theme_void()\n\n\n\n\n\n\n\n\nIdentifying the links with differences &gt; 0 in d and weighted d\n\nsf_net |&gt; \n  mutate(diff_check = (d-d_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nSame for time\n\nsf_net |&gt; \n  mutate(diff_check = (time-time_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nA course assumption is that people decide the route they will use based on the ETA, also differences in speed limits can only captured if time is used. So weighted time will be used.\n\n\n\n\nlibrary(sfnetworks)\n\n\nsf_net$time_weighted[sf_net$time_weighted==0] &lt;- (3.6*sf_net$d_weighted[sf_net$time_weighted==0])/case_when(\n      sf_net$highway[sf_net$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\nbog_sfnet &lt;- sf_net |&gt; \n  as_sfnetwork(directed = TRUE)\n\n\nbog_sfnet &lt;- bog_sfnet |&gt;\n  activate(\"edges\") |&gt; \n  mutate(cent_sfn_time_weighted = tidygraph::centrality_edge_betweenness(\n    weights = time_weighted,\n    directed = T))\n\n\nbog_sfnet |&gt; \n  st_as_sf(\"edges\") |&gt; \nggplot()+\n  geom_sf(aes(col = cent_time_weighted))+\n  theme_void()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  select(cent_time_weighted,cent_sfn_time_weighted) |&gt; \n  pairs()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  with(identical(cent_time_weighted,cent_sfn_time_weighted))\n\n[1] TRUE\n\n\n\np1 = st_geometry(bog_sfnet, \"nodes\")[2401]\nst_crs(p1) = st_crs(bog_sfnet)\np2 = st_geometry(bog_sfnet, \"nodes\")[1407]\np3 = st_geometry(bog_sfnet, \"nodes\")[1509]\np4 = st_geometry(bog_sfnet, \"nodes\")[1840]\nst_crs(p3) = st_crs(bog_sfnet)\n\npaths = st_network_paths(bog_sfnet, from = p1, to = c(p2,p3,p4), weights = \"time_weighted\")\n\n\nplot_path = function(node_path) {\n  bog_sfnet %&gt;%\n    activate(\"nodes\") %&gt;%\n    slice(node_path) %&gt;%\n    plot(cex = 1.5, lwd = 1.5, add = TRUE)\n}\n\ncolors = sf.colors(4, categorical = TRUE)\n\nplot(bog_sfnet, col = \"grey\")\npaths %&gt;%\n  pull(node_paths) %&gt;%\n  walk(plot_path)\nplot(c(p1, p2, p3,p4), col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE)"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#loading-network",
    "href": "D1_graph_centrality_tests.html#loading-network",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "sf_bogota_2019_full &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                          layer = \"network_2019\")\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",n_circles = 2) |&gt;\n  st_transform(st_crs(sf_bogota_2019_full))\n# zb_view(bog_zone)\n\nThe following the code will clip the network; also, since we will be using wrong-way-driving, we have to allow both directions inone-way links to capture the changes of centrality.\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,] |&gt; \n  mutate(oneway = if_else(highway %in% c(\"residential\"),\"no\",oneway))\n\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#assignment-of-the-weighting-profile",
    "href": "D1_graph_centrality_tests.html#assignment-of-the-weighting-profile",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "In order to include the actual speed along each corridor, I will specify 60 different classes to be applied to each road link. By default, speeds are assigned by road link by road type, e.g. any trunk road in Bogotá has a 60 km/h speed.\n\n# custom_wp &lt;- dodgr::weighting_profiles$weighting_profiles |&gt; filter(name==\"motorcar\")\ncustom_penalties &lt;- dodgr::weighting_profiles$penalties |&gt; filter(name==\"motorcar\")\ncustom_surfaces &lt;- dodgr::weighting_profiles$surface_speeds |&gt; filter(name==\"motorcar\")\n\ncustom_wp &lt;- data.frame(name = \"motorcar\",\n                        way = paste0(\"road_\",1:60),\n                        value = 1,\n                        max_speed = as.numeric(1:60))\n\ncustom_wp_list &lt;- list(weighting_profiles = custom_wp,\n                       surface_speeds = custom_surfaces,\n                       pealties = custom_penalties)\n\nwpj &lt;- jsonlite::toJSON (custom_wp_list, pretty = TRUE)\nwrite_lines(wpj,file = \"custom_wp_speeds.json\")\n\nAssigning a speed category based on the speed limit (baseline scenario)\n\nsf_bogota_2019_cwp &lt;- sf_bogota_2019 |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                                               TRUE ~ \"road_30\")) |&gt; \n  select(-highway)\n\nThe following code builds the graph using the new column instead of highway.\n\ngraph_bogota_custom &lt;- weight_streetnet(sf_bogota_2019_cwp,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\n\n\nWe calculate the centralities with both weighting profiles\n\ngraph_baseline_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\ngraph_cwp_centrality &lt;- graph_bogota_custom |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\nTo validate that the results are identical, we can run the following code\n\nidentical(graph_baseline_centrality$centrality,graph_cwp_centrality$centrality)\n\n[1] TRUE"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#centrality-calculations",
    "href": "D1_graph_centrality_tests.html#centrality-calculations",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "There are four core alternatives for calculating the centrality using dodgr.\n\nbog_contracted[,\n    c(\"d_weighted\", \"d\", \"time_weighted\", \"time\")] |&gt;\n  pairs()\n\n\n\n\n\n\n\n\nAn initial check on the four alternatives reveal some inconsistencies in the time_weighted column.\nThere are 49 links in the contracted graph with differences in the weighted distance and the distance. The location of these edges will be inspected after calculating the centrality in Section 1.3.3.\nWhen multiple paths are available between two nodes in the contracted version of the graph, dodgr takes only the shortest weighted distance, which creates the difference. However, the resulting weighted time is not corrected and in some cases a 0 is returned.\nGiven this, we will recalculate the weighted time based on the weighted distance.\n\nbog_contracted$time_weighted[bog_contracted$time_weighted==0] &lt;- (3.6*bog_contracted$d_weighted[bog_contracted$time_weighted==0])/case_when(\n      bog_contracted$highway[bog_contracted$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\n\n\n\ncent_all &lt;- lapply(c(\"d_weighted\", \"d\", \"time_weighted\", \"time\"), \\(x) {\n  \n    df &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = x)\n  \n  tibble(edge_id = df$edge_id, cent = df$centrality) |&gt;\n    rename_with(.cols = \"cent\", .fn = \\(.y) {\n      paste(.y, x, sep = \"_\")\n    })\n  }) |&gt;\n  plyr::join_all(by = \"edge_id\",type = \"full\")\n\n\n\n\n\nigraph_bog &lt;- bog_contracted |&gt; \n  dodgr_to_igraph()\n\n\ncent_all_ig &lt;- cent_all |&gt; \n  left_join(\n    tibble(edge_id = bog_contracted$edge_id,\n       cent_ig_d = igraph::edge_betweenness(graph = igraph_bog,\n                                            weights = bog_contracted$d),\n       cent_ig_d_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                     weights = bog_contracted$d_weighted),\n       cent_ig_time = igraph::edge_betweenness(graph = igraph_bog,weights = bog_contracted$time),\n       cent_ig_time_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                        weights = bog_contracted$time_weighted)\n       ),\n    by = \"edge_id\")\n\n\nsummary(cent_all_ig[,2:9])\n\n cent_d_weighted       cent_d        cent_time_weighted   cent_time      \n Min.   :      0   Min.   :      0   Min.   :      0    Min.   :      0  \n 1st Qu.:   4526   1st Qu.:   4526   1st Qu.:   4479    1st Qu.:   4479  \n Median :  15900   Median :  15670   Median :  13560    Median :  13465  \n Mean   : 108354   Mean   : 108428   Mean   : 107514    Mean   : 107668  \n 3rd Qu.:  75756   3rd Qu.:  74517   3rd Qu.:  56990    3rd Qu.:  56265  \n Max.   :2606261   Max.   :2608137   Max.   :2679102    Max.   :2684680  \n   cent_ig_d       cent_ig_d_weighted  cent_ig_time     cent_ig_time_weighted\n Min.   :      0   Min.   :      0    Min.   :      0   Min.   :      0      \n 1st Qu.:   4526   1st Qu.:   4526    1st Qu.:   4479   1st Qu.:   4479      \n Median :  15670   Median :  15900    Median :  13465   Median :  13560      \n Mean   : 108428   Mean   : 108354    Mean   : 107668   Mean   : 107514      \n 3rd Qu.:  74517   3rd Qu.:  75756    3rd Qu.:  56265   3rd Qu.:  56990      \n Max.   :2608137   Max.   :2606261    Max.   :2684680   Max.   :2679102      \n\n\nCheck if igraph results are identical to the ones obtained with dodgr\n\nwith(cent_all_ig,identical(cent_ig_d_weighted,cent_d_weighted))\n\n[1] TRUE\n\nwith(cent_all_ig,identical(cent_ig_time_weighted,cent_time_weighted))\n\n[1] TRUE\n\n\n\npairs(cent_all_ig[,2:9])\n\n\n\n\n\n\n\n\nJoining the results to the sf object\n\nsf_net &lt;- graph_bogota |&gt;\n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\n\n\n\nsf_net |&gt;  \n  select(cent_d_weighted:cent_time) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = log(cent),linewidth = cent+1))+\n  facet_wrap(type~.)+\n  scale_color_viridis_c(\n    # direction = -1\n    )+\n  scale_linewidth_continuous(range = c(0.05,0.5),\n                             transform = scales::transform_boxcox(p = 2))+\n  theme_void()+\n  guides(linewidth = \"none\",)\n\n\n\n\n\n\n\n\nVisualising differences\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot(aes(cent))+\n  geom_histogram()+\n  facet_wrap(type~.)+\n  scale_x_continuous(limits = c(-1,1))\n\n\n\n\n\n\n\n\nInspecting significant changes\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  mutate(cent = case_when(abs(cent)&lt;0.1~0,\n                          cent &gt; 0.6 ~ 1,\n                          -cent &gt; 0.6 ~ -1,\n                          is.na(cent)~NA)) |&gt; \n  ggplot()+\n  geom_sf(aes(col = cent))+\n  facet_wrap(type~.)+\n  scale_color_distiller(palette = \"Spectral\", direction = 1)+\n  # scale_linewidth_continuous(range = c(0.05,0.5),\n  #                            transform = scales::transform_boxcox(p = 2))+\n  theme_void()\n\n\n\n\n\n\n\n\nIdentifying the links with differences &gt; 0 in d and weighted d\n\nsf_net |&gt; \n  mutate(diff_check = (d-d_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nSame for time\n\nsf_net |&gt; \n  mutate(diff_check = (time-time_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nA course assumption is that people decide the route they will use based on the ETA, also differences in speed limits can only captured if time is used. So weighted time will be used.\n\n\n\n\nlibrary(sfnetworks)\n\n\nsf_net$time_weighted[sf_net$time_weighted==0] &lt;- (3.6*sf_net$d_weighted[sf_net$time_weighted==0])/case_when(\n      sf_net$highway[sf_net$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\nbog_sfnet &lt;- sf_net |&gt; \n  as_sfnetwork(directed = TRUE)\n\n\nbog_sfnet &lt;- bog_sfnet |&gt;\n  activate(\"edges\") |&gt; \n  mutate(cent_sfn_time_weighted = tidygraph::centrality_edge_betweenness(\n    weights = time_weighted,\n    directed = T))\n\n\nbog_sfnet |&gt; \n  st_as_sf(\"edges\") |&gt; \nggplot()+\n  geom_sf(aes(col = cent_time_weighted))+\n  theme_void()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  select(cent_time_weighted,cent_sfn_time_weighted) |&gt; \n  pairs()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  with(identical(cent_time_weighted,cent_sfn_time_weighted))\n\n[1] TRUE\n\n\n\np1 = st_geometry(bog_sfnet, \"nodes\")[2401]\nst_crs(p1) = st_crs(bog_sfnet)\np2 = st_geometry(bog_sfnet, \"nodes\")[1407]\np3 = st_geometry(bog_sfnet, \"nodes\")[1509]\np4 = st_geometry(bog_sfnet, \"nodes\")[1840]\nst_crs(p3) = st_crs(bog_sfnet)\n\npaths = st_network_paths(bog_sfnet, from = p1, to = c(p2,p3,p4), weights = \"time_weighted\")\n\n\nplot_path = function(node_path) {\n  bog_sfnet %&gt;%\n    activate(\"nodes\") %&gt;%\n    slice(node_path) %&gt;%\n    plot(cex = 1.5, lwd = 1.5, add = TRUE)\n}\n\ncolors = sf.colors(4, categorical = TRUE)\n\nplot(bog_sfnet, col = \"grey\")\npaths %&gt;%\n  pull(node_paths) %&gt;%\n  walk(plot_path)\nplot(c(p1, p2, p3,p4), col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE)"
  },
  {
    "objectID": "2A_Congested_graph_std_network.html",
    "href": "2A_Congested_graph_std_network.html",
    "title": "Congested Network",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\npackageVersion (\"dodgr\")\n\n[1] '0.4.1.44'\n\n\n\n\n\nsf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_raw\n\n\n\n\n\ndodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\ndodgr::clear_dodgr_cache()\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\ndodgr::clear_dodgr_cache()\n\n\n\n\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;\n  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nExploring the extension of the network that has speed data\n\nsf_bog_major |&gt; \n  mutate(speedbool = edge_id %in% TID_to_edge_id$edge_id) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(total_length = sum(d_weighted),.by=c(roadclass,speedbool)) |&gt; \n  mutate(portion = total_length/sum(total_length),.by=c(roadclass)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nspeedbool\ntotal_length\nportion\n\n\n\n\ntertiary\nFALSE\n2749930.82\n0.9876405\n\n\ntrunk\nFALSE\n494693.68\n0.7938867\n\n\nsecondary\nFALSE\n1142514.98\n0.8899419\n\n\nprimary\nFALSE\n561319.33\n0.6085016\n\n\ntrunk\nTRUE\n128435.13\n0.2061133\n\n\nprimary\nTRUE\n361142.19\n0.3914984\n\n\nsecondary\nTRUE\n141293.53\n0.1100581\n\n\ntertiary\nTRUE\n34413.13\n0.0123595\n\n\n\n\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\n\nspeed_key &lt;- speed_data |&gt;\n  select(year, hour, day_type) |&gt;\n  filter(day_type != \"friday\",year == 2019) |&gt;\n  unique()\n\n\nhourly_centrality &lt;- \n  do.call(bind_cols,\n          lapply(\n            1:nrow(speed_key),\n            \\(i) {\n              \n              speed_tbl &lt;- speed_data |&gt;\n                semi_join(speed_key[i,],\n                          by = join_by(year, day_type, hour)) |&gt;\n                select(TID, d_mean_speed)\n              \n              \n              obs_speeds_edges &lt;- TID_to_edge_id |&gt;\n                left_join(speed_tbl, by = \"TID\") |&gt;\n                summarise(obs_speed = mean(d_mean_speed), .by = edge_id)\n              \n              bog_contr_adjusted &lt;- bog_contracted\n              \n              dodgr::clear_dodgr_cache()\n              \n              bog_contr_adjusted$obs_speed &lt;-\n                tibble(edge_id = bog_contr_adjusted$edge_id) |&gt;\n                left_join(obs_speeds_edges, by = \"edge_id\") |&gt;\n                pull(obs_speed)\n              \n              dodgr::clear_dodgr_cache()\n              \n              bog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;-\n                (3.6 * bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)]) /\n                bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n              \n              try({\n                hourly_centrality &lt;-\n                  bog_contr_adjusted |&gt;\n                  dodgr_centrality(column = \"time_weighted\",\n                                   dist_threshold = 1200)\n                \n                t &lt;-\n                  tibble(cent = hourly_centrality$centrality)\n                \n                names(t) &lt;-\n                  paste(names(t),\n                        speed_key$year[i],\n                        speed_key$hour[i],\n                        speed_key$day_type[i],\n                        sep = \"_\")\n                \n                return(t)\n              })\n              \n            }))\n\n\nhourly_centrality$edge_id &lt;- bog_contracted$edge_id\n\nsave(hourly_centrality,\n     file = \"sf_network/hourly_centrality_stdnet.rdata\")\n\nPosted speed limit is not the actual travel speed for all links, even during non-congested hours. So we are going to use the hour of the day with the highest average speed as baseline.\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour))\n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(-mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday      3           0.963\n2  2019 weekday      2           0.950\n3  2019 weekday      1           0.925\n4  2019 weekday      4           0.918\n5  2019 weekday      0           0.873\n\n\n\nbl_id &lt;- summary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt; \n  slice_max(mean_norm_speed) |&gt; \n  mutate(id=paste(\"cent\",year,hour,day_type,sep = \"_\")) |&gt; \n  pull(id)\n\nAdding a column with the centrality results of the baseline hour\n\nhourly_centrality$cent_2019_bl &lt;- hourly_centrality |&gt; pull(bl_id)\n\nAdding the component id to the dataset\n\nhourly_centrality$component &lt;- bog_contracted$component\n\nPreparing a tidy data frame with all the centrality results. Relative difference in centrality are adjusted by \\(n - 1\\), where \\(n\\) is the number of nodes in the component. This is done because BC is the number of shortest paths from/to anytother nodes in the network, however, in our case, we are also interested in the paths from/to that particular link.\n\ntidy_hourly_centrality &lt;- hourly_centrality |&gt; \n  pivot_longer(cols = cent_2019_0_weekday:cent_2019_23_weekend,\n               names_to = \"scenario\",\n               values_to = \"cent\",\n               names_prefix = \"cent_2019_\") |&gt; \n  separate_wider_delim(scenario,\n                       delim = \"_\",\n                       names = c(\"hour\",\"day_type\")) |&gt; \n  left_join(n_nodes, by = join_by(component)) |&gt; \n  rename(cong = cent,\n         ff = cent_2019_bl) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nrm(dist_matrix)\n\n\nsf_net_jct &lt;- sf_net |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))\n\n\n\n\n\nwrite_csv(tidy_hourly_centrality, file = \"sf_network/hourly_cent_results_stdnet.csv\")\nst_write(sf_net_jct, \"sf_network/full_sf_network_stdnet.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network_stdnet.gpkg' using driver `GPKG'\nWriting layer `full_sf_network_stdnet' to data source \n  `sf_network/full_sf_network_stdnet.gpkg' using driver `GPKG'\nWriting 232394 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "2A_Congested_graph_std_network.html#loading-network",
    "href": "2A_Congested_graph_std_network.html#loading-network",
    "title": "Congested Network",
    "section": "",
    "text": "sf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_raw"
  },
  {
    "objectID": "2A_Congested_graph_std_network.html#graph-building",
    "href": "2A_Congested_graph_std_network.html#graph-building",
    "title": "Congested Network",
    "section": "",
    "text": "dodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\ndodgr::clear_dodgr_cache()\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\ndodgr::clear_dodgr_cache()"
  },
  {
    "objectID": "2A_Congested_graph_std_network.html#using-observed-speed-to-adjust-the-weighting-of-the-graph",
    "href": "2A_Congested_graph_std_network.html#using-observed-speed-to-adjust-the-weighting-of-the-graph",
    "title": "Congested Network",
    "section": "",
    "text": "sf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;\n  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nExploring the extension of the network that has speed data\n\nsf_bog_major |&gt; \n  mutate(speedbool = edge_id %in% TID_to_edge_id$edge_id) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(total_length = sum(d_weighted),.by=c(roadclass,speedbool)) |&gt; \n  mutate(portion = total_length/sum(total_length),.by=c(roadclass)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nspeedbool\ntotal_length\nportion\n\n\n\n\ntertiary\nFALSE\n2749930.82\n0.9876405\n\n\ntrunk\nFALSE\n494693.68\n0.7938867\n\n\nsecondary\nFALSE\n1142514.98\n0.8899419\n\n\nprimary\nFALSE\n561319.33\n0.6085016\n\n\ntrunk\nTRUE\n128435.13\n0.2061133\n\n\nprimary\nTRUE\n361142.19\n0.3914984\n\n\nsecondary\nTRUE\n141293.53\n0.1100581\n\n\ntertiary\nTRUE\n34413.13\n0.0123595\n\n\n\n\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\n\nspeed_key &lt;- speed_data |&gt;\n  select(year, hour, day_type) |&gt;\n  filter(day_type != \"friday\",year == 2019) |&gt;\n  unique()\n\n\nhourly_centrality &lt;- \n  do.call(bind_cols,\n          lapply(\n            1:nrow(speed_key),\n            \\(i) {\n              \n              speed_tbl &lt;- speed_data |&gt;\n                semi_join(speed_key[i,],\n                          by = join_by(year, day_type, hour)) |&gt;\n                select(TID, d_mean_speed)\n              \n              \n              obs_speeds_edges &lt;- TID_to_edge_id |&gt;\n                left_join(speed_tbl, by = \"TID\") |&gt;\n                summarise(obs_speed = mean(d_mean_speed), .by = edge_id)\n              \n              bog_contr_adjusted &lt;- bog_contracted\n              \n              dodgr::clear_dodgr_cache()\n              \n              bog_contr_adjusted$obs_speed &lt;-\n                tibble(edge_id = bog_contr_adjusted$edge_id) |&gt;\n                left_join(obs_speeds_edges, by = \"edge_id\") |&gt;\n                pull(obs_speed)\n              \n              dodgr::clear_dodgr_cache()\n              \n              bog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;-\n                (3.6 * bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)]) /\n                bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n              \n              try({\n                hourly_centrality &lt;-\n                  bog_contr_adjusted |&gt;\n                  dodgr_centrality(column = \"time_weighted\",\n                                   dist_threshold = 1200)\n                \n                t &lt;-\n                  tibble(cent = hourly_centrality$centrality)\n                \n                names(t) &lt;-\n                  paste(names(t),\n                        speed_key$year[i],\n                        speed_key$hour[i],\n                        speed_key$day_type[i],\n                        sep = \"_\")\n                \n                return(t)\n              })\n              \n            }))\n\n\nhourly_centrality$edge_id &lt;- bog_contracted$edge_id\n\nsave(hourly_centrality,\n     file = \"sf_network/hourly_centrality_stdnet.rdata\")\n\nPosted speed limit is not the actual travel speed for all links, even during non-congested hours. So we are going to use the hour of the day with the highest average speed as baseline.\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour))\n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(-mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday      3           0.963\n2  2019 weekday      2           0.950\n3  2019 weekday      1           0.925\n4  2019 weekday      4           0.918\n5  2019 weekday      0           0.873\n\n\n\nbl_id &lt;- summary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt; \n  slice_max(mean_norm_speed) |&gt; \n  mutate(id=paste(\"cent\",year,hour,day_type,sep = \"_\")) |&gt; \n  pull(id)\n\nAdding a column with the centrality results of the baseline hour\n\nhourly_centrality$cent_2019_bl &lt;- hourly_centrality |&gt; pull(bl_id)\n\nAdding the component id to the dataset\n\nhourly_centrality$component &lt;- bog_contracted$component\n\nPreparing a tidy data frame with all the centrality results. Relative difference in centrality are adjusted by \\(n - 1\\), where \\(n\\) is the number of nodes in the component. This is done because BC is the number of shortest paths from/to anytother nodes in the network, however, in our case, we are also interested in the paths from/to that particular link.\n\ntidy_hourly_centrality &lt;- hourly_centrality |&gt; \n  pivot_longer(cols = cent_2019_0_weekday:cent_2019_23_weekend,\n               names_to = \"scenario\",\n               values_to = \"cent\",\n               names_prefix = \"cent_2019_\") |&gt; \n  separate_wider_delim(scenario,\n                       delim = \"_\",\n                       names = c(\"hour\",\"day_type\")) |&gt; \n  left_join(n_nodes, by = join_by(component)) |&gt; \n  rename(cong = cent,\n         ff = cent_2019_bl) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)"
  },
  {
    "objectID": "2A_Congested_graph_std_network.html#extracting-distance-to-the-major-network",
    "href": "2A_Congested_graph_std_network.html#extracting-distance-to-the-major-network",
    "title": "Congested Network",
    "section": "",
    "text": "The distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nrm(dist_matrix)\n\n\nsf_net_jct &lt;- sf_net |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))"
  },
  {
    "objectID": "2A_Congested_graph_std_network.html#saving-results",
    "href": "2A_Congested_graph_std_network.html#saving-results",
    "title": "Congested Network",
    "section": "",
    "text": "write_csv(tidy_hourly_centrality, file = \"sf_network/hourly_cent_results_stdnet.csv\")\nst_write(sf_net_jct, \"sf_network/full_sf_network_stdnet.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network_stdnet.gpkg' using driver `GPKG'\nWriting layer `full_sf_network_stdnet' to data source \n  `sf_network/full_sf_network_stdnet.gpkg' using driver `GPKG'\nWriting 232394 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "1A_network.html",
    "href": "1A_network.html",
    "title": "Extracting OSM networks",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"osmextract\",\n    \"rvest\",\n    \"paletteer\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n        sf  tidyverse osmextract      rvest  paletteer \n      TRUE       TRUE       TRUE       TRUE       TRUE \n\n\nFor this study, we will use OpenStreetMap to obtain the road network data. First, we will download a spatial data file with the urban perimeter of Bogotá from Datos Abiertos de Bogotá (Bogotá’s Open Data platform).\n\ndir.create(\"raw_data\",showWarnings = F)\n\nif(!file.exists(file.path(\"raw_data\", \"perimetrourbano.gpkg\"))) {\n  u &lt;- \"https://datosabiertos.bogota.gov.co/dataset/12a704ee-e5bb-4c5d-bad6-a5069d12f90a/resource/bfc61e3c-fa58-4fe7-9581-7ead66c494cb/download/perimetrourbano.gpkg\"\n  download.file(u, file.path(\"raw_data\", basename(u)), mode = \"wb\")\n}\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\")\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use the osmextract package to get OSM data in R. Please note that we will be using previous versions of the OSM data, as the network might have changed in recent years. For example, some sections of the roads are currently closed (2024) due to the works for the first metro line in Bogotá. We will use OSM networks for end of 2019 (January 1st, 2020) and 2020 (January 1st, 2021).\nFirst, we produce a boundary box for an area covering the urban perimeter and the neighbouring municipalities. This is done by producing a buffer of 20 km around Bogotá.\n\nbbox_bogota &lt;- urban_perimeter |&gt;\n  st_buffer(dist = 20e3) |&gt; \n  st_bbox() |&gt;\n  st_as_sfc() |&gt;\n  st_transform(crs = 4326)\n\nWe obtain the url of the pbf file in Geofabrik which contains the area we are interested in.\n\nbog_match &lt;- oe_match(bbox_bogota,provider = \"geofabrik\")\n\nThe following code obtains the names of the files with the older versions of the OSM data and downloads the 2019 and 2020 pbf files.\n\nu &lt;- dirname(bog_match$url)\nf &lt;- basename(bog_match$url)\n\nid_files &lt;- gsub(\"latest\\\\.osm\\\\.pbf\",replacement = \"\",f)\n\nfiles_table &lt;- (rvest::read_html(u) |&gt; html_table())[[1]]\n\navailable_versions &lt;- files_table$Name[grep(paste0(id_files,\"\\\\d{6}\\\\.osm\\\\.pbf$\"),\n                                            files_table$Name)]\n\n\nnet_match_19 &lt;- grep(\"200101\",available_versions)\nnet_match_20 &lt;- grep(\"210101\",available_versions)\n\nnet_options &lt;- osmextract:::load_options_driving(NA_character_)\n\nnet_options$extra_tags &lt;- c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\",net_options$extra_tags)\n\nnet_old_200101 &lt;- do.call(oe_read,\n                          c(file_path = paste0(u,\"/\",available_versions[net_match_19]),\n                            net_options[2:4]\n                     )\n                   )\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n0...10...20...30...40...50...60...70...80...90...100 - done.\nReading layer `lines' from data source \n  `/tmp/Rtmp9UmnYJ/geofabrik_colombia-200101.gpkg' using driver `GPKG'\nSimple feature collection with 461488 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -81.73539 ymin: -4.259884 xmax: -66.83441 ymax: 13.38586\nGeodetic CRS:  WGS 84\n\nnet_old_210101 &lt;- do.call(oe_read,\n                          c(file_path = paste0(u,\"/\",available_versions[net_match_20]),\n                            net_options[2:4]\n                     )\n                   )\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n0...10...20...30...40...50...60...70...80...90...100 - done.\nReading layer `lines' from data source \n  `/tmp/Rtmp9UmnYJ/geofabrik_colombia-210101.gpkg' using driver `GPKG'\nSimple feature collection with 650882 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -81.73539 ymin: -4.259656 xmax: -66.83441 ymax: 13.38586\nGeodetic CRS:  WGS 84\n\n\nOnce we have downloaded the data. We can clean and clip the network by using the boundary box we produced and by filtering only relevant road links (see the road_types vector).\n\nroad_types &lt;- c(\"tertiary\"       ,\n  \"residential\"    ,\n  \"primary_link\"   ,\n  \"primary\"      ,\n  \"secondary\"      ,\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  # \"service\"      ,\n  \"secondary_link\" ,\n  \"unclassified\"   ,\n  \"tertiary_link\"  \n  # \"living_street\"\n  # \"track\"          ,\n  # \"busway\"         ,\n  # \"raceway\"\n)\n\nosm_bogota_200101 &lt;- net_old_200101[bbox_bogota,] |&gt;\n  filter(highway %in% road_types) |&gt; st_transform(st_crs(urban_perimeter))\n\nosm_bogota_210101 &lt;- net_old_210101[bbox_bogota,] |&gt;\n  filter(highway %in% road_types) |&gt; st_transform(st_crs(urban_perimeter))\n\nrm(net_old_200101,net_old_210101)\n\n\n\n\nnet_2019_map &lt;- osm_bogota_200101[urban_perimeter,] |&gt; \n  mutate(highway = str_remove(highway, \"_link\") |&gt; str_to_sentence()) |&gt;\n  mutate(highway = factor(highway,\n                          levels = c(\"Trunk\",\n                                     \"Primary\",\n                                     \"Secondary\",\n                                     \"Tertiary\",\n                                     \"Residential\",\n                                     \"Unclassified\"),\n                          ordered = T)) |&gt;\n  ggplot(\n     aes(linewidth = highway)\n    )+\n  geom_sf(aes(col = highway))+\n  # scale_color_viridis_c(direction = -1)+\n  scale_linewidth_manual(values = c(1.7,1.5,1.4,1.2,0.7,0.7)/5)+\n  # scale_linewidth_continuous(range = c(0.05,0.3),transform = scales::transform_boxcox(p = 2))+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))+\n  theme_void()+\n  guides(linewidth = \"none\",)+\n  labs(col = \"Road Class\")+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.0,0.8),\n        text = element_text(family = \"Roboto Condensed\"),\n        legend.key.width = unit(3, \"mm\"))\n\nnet_2019_map  \n\n\n\n\n\n\n\n\n\n\n\nFinally, we save the sf objects as GeoPackages.\n\ndir.create(\"sf_network\",showWarnings = F)\nst_write(osm_bogota_200101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2019\",\n         delete_layer = T)\n\nDeleting layer `network_2019' using driver `GPKG'\nWriting layer `network_2019' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 54535 features with 17 fields and geometry type Line String.\n\nst_write(osm_bogota_210101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2020\",\n         delete_layer = T)\n\nDeleting layer `network_2020' using driver `GPKG'\nWriting layer `network_2020' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 62175 features with 17 fields and geometry type Line String."
  },
  {
    "objectID": "1A_network.html#a-quick-visualisation-of-the-network",
    "href": "1A_network.html#a-quick-visualisation-of-the-network",
    "title": "Extracting OSM networks",
    "section": "",
    "text": "net_2019_map &lt;- osm_bogota_200101[urban_perimeter,] |&gt; \n  mutate(highway = str_remove(highway, \"_link\") |&gt; str_to_sentence()) |&gt;\n  mutate(highway = factor(highway,\n                          levels = c(\"Trunk\",\n                                     \"Primary\",\n                                     \"Secondary\",\n                                     \"Tertiary\",\n                                     \"Residential\",\n                                     \"Unclassified\"),\n                          ordered = T)) |&gt;\n  ggplot(\n     aes(linewidth = highway)\n    )+\n  geom_sf(aes(col = highway))+\n  # scale_color_viridis_c(direction = -1)+\n  scale_linewidth_manual(values = c(1.7,1.5,1.4,1.2,0.7,0.7)/5)+\n  # scale_linewidth_continuous(range = c(0.05,0.3),transform = scales::transform_boxcox(p = 2))+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))+\n  theme_void()+\n  guides(linewidth = \"none\",)+\n  labs(col = \"Road Class\")+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.0,0.8),\n        text = element_text(family = \"Roboto Condensed\"),\n        legend.key.width = unit(3, \"mm\"))\n\nnet_2019_map"
  },
  {
    "objectID": "1A_network.html#save-network-in-sf-format",
    "href": "1A_network.html#save-network-in-sf-format",
    "title": "Extracting OSM networks",
    "section": "",
    "text": "Finally, we save the sf objects as GeoPackages.\n\ndir.create(\"sf_network\",showWarnings = F)\nst_write(osm_bogota_200101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2019\",\n         delete_layer = T)\n\nDeleting layer `network_2019' using driver `GPKG'\nWriting layer `network_2019' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 54535 features with 17 fields and geometry type Line String.\n\nst_write(osm_bogota_210101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2020\",\n         delete_layer = T)\n\nDeleting layer `network_2020' using driver `GPKG'\nWriting layer `network_2020' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 62175 features with 17 fields and geometry type Line String."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rat-runs",
    "section": "",
    "text": "Rat-runs in Bogota\n\nNetwork data\n\nOSM\nIDECA malla vial integral\n\n\n\nInfractions data\n\n\nAssumptions:\n\nRecurring wrong-way infraction reports in residential streets is a result of rat-running\n\n\n\nHypothesis:"
  },
  {
    "objectID": "2B_Analysis_std_network.html",
    "href": "2B_Analysis_std_network.html",
    "title": "Results",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"tmap\",\n    \"ggridges\",\n    \"paletteer\"\n)\n\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n       sf tidyverse      tmap  ggridges paletteer \n     TRUE      TRUE      TRUE      TRUE      TRUE \n\n\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt; st_transform(3116)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\n\n\nsf_net_exp &lt;- st_read(\"sf_network/full_sf_network_stdnet.gpkg\") |&gt; st_transform(3116)\n\nReading layer `full_sf_network_stdnet' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/full_sf_network_stdnet.gpkg' \n  using driver `GPKG'\nSimple feature collection with 232394 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  WGS 84\n\nsf_net &lt;- sf_net_exp[urban_perimeter,]\n\n\n\n\n\ncent_results &lt;- read_csv(\"sf_network/hourly_cent_results_stdnet.csv\",\n                       lazy = F)\n\n\n\n\n\nAs we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects.\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(sf_net,sf_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\n# subset_net$pair_id &lt;- simp_groups$pair_id\nsf_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_results |&gt; \n  right_join(sf_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,day_type,hour))\n\n\nsummary_pairs_dist.jct &lt;- \n  sf_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- sf_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\n\n\nTotal length of road network\n\nsimpl_network_sf |&gt;\n  mutate(distance = st_length(geom, )) |&gt;\n  pull(distance) |&gt;\n  sum()\n\n6590563 [m]\n\n\n\nsimpl_network_sf |&gt;\n  mutate(distance = st_length(geom) |&gt;\n           as.numeric()) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(roadclass = str_remove(roadclass,\"_link\")) |&gt; \n  summarise(d_weighted = sum(distance)/1e3,\n            .by=c(roadclass)) |&gt; \n  mutate(d_weighted = round(d_weighted),\n         roadclass = factor(roadclass,\n                            levels = c(\"trunk\",\n                                       \"primary\",\n                                       \"secondary\",\n                                       \"tertiary\",\n                                       \"residential\",\n                                       \"unclassified\"), \n                            ordered = T)) |&gt; \n  arrange(roadclass) |&gt; \n  mutate(portion = round(d_weighted/sum(d_weighted)*100,1),\n         roadclass = str_to_sentence(roadclass)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nd_weighted\nportion\n\n\n\n\nTrunk\n218\n3.3\n\n\nPrimary\n616\n9.3\n\n\nSecondary\n460\n7.0\n\n\nTertiary\n920\n14.0\n\n\nResidential\n4345\n65.9\n\n\nUnclassified\n32\n0.5\n\n\n\n\n\nExploring the changes in BC during the evening peak with a the empirical cumulative distribution function by road class.\n\n## Unweighted version\n\n# ecdf_18_wk &lt;- simpl_network_sf |&gt;\n#   mutate(roadclass = str_remove(roadclass, \"_link\"),\n#          w = st_length(geom) |&gt; as.numeric()) |&gt;\n#   mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n#   mutate(roadclass = factor(roadclass,\n#                             levels = c(\"Trunk\",\n#                                        \"Primary\",\n#                                        \"Secondary\",\n#                                        \"Tertiary\",\n#                                        \"Residential\",\n#                                        \"Unclassified\"), \n#                             ordered = T)) |&gt;\n#   st_drop_geometry() |&gt;\n#   # filter(roadclass %in% c(\"residential\")) |&gt;\n#   left_join(summary_pairs, by = \"pair_id\") |&gt;\n#   filter(hour == 18, day_type == \"weekday\") |&gt;\n#   ggplot(aes(x =  reldiff_max,\n#              col = roadclass)) +\n#   stat_ecdf(pad = F)+\n#   labs(x = \"Mean relative change in BC\",\n#        y = \"Cumulative distribution\",\n#        col = \"Road class\")+\n#   scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n#   theme_minimal()+\n#   theme(legend.position = \"top\")+\n#   scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk &lt;- simpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\"),\n         w = st_length(geom) |&gt; as.numeric()) |&gt;\n  mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n  mutate(roadclass = factor(roadclass,\n                            levels = c(\"Trunk\",\n                                       \"Primary\",\n                                       \"Secondary\",\n                                       \"Tertiary\",\n                                       \"Residential\",\n                                       \"Unclassified\"), \n                            ordered = T)) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  mutate(bin = cut(reldiff_max,\n                   breaks = seq(\n                     min(reldiff_max,na.rm = T)-0.01,\n                     max(reldiff_max,na.rm = T),\n                     0.005),ordered_result = T)) |&gt; \n  arrange(roadclass,bin) |&gt;\n  summarise(w = sum(w),.by = c(roadclass,bin)) |&gt; \n  drop_na(bin) |&gt; \n  mutate(w_perc = w/sum(w),.by = roadclass) |&gt; \n  mutate(w_cs = cumsum(w_perc),.by = roadclass) |&gt; \n  mutate(bin_n = str_extract(bin,\",.*]\") |&gt; \n           str_remove_all(\"(,|\\\\])\") |&gt; \n           as.numeric()) |&gt; \n  ggplot(aes(x = bin_n,\n             y = w_cs,\n             col = roadclass,group = roadclass)) +\n  geom_line()+\n  labs(x = \"Mean relative change in BC\",\n       y = \"Cumulative distribution\",\n       col = \"Road class\")+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n  theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk\n\n\n\n\n\n\n\n\necdf_18_wk_absdiff\n\necdf_18_wk_logdiff &lt;- simpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\"),\n         w = st_length(geom) |&gt; as.numeric()) |&gt;\n  mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n  mutate(roadclass = factor(roadclass,\n                            levels = c(\"Trunk\",\n                                       \"Primary\",\n                                       \"Secondary\",\n                                       \"Tertiary\",\n                                       \"Residential\",\n                                       \"Unclassified\"), \n                            ordered = T)) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  mutate(bin = cut(logdiff_max,\n                   breaks = seq(\n                     min(logdiff_max,na.rm = T)-0.1,\n                     max(logdiff_max,na.rm = T),\n                     0.1),ordered_result = T)) |&gt; \n  arrange(roadclass,bin) |&gt;\n  summarise(w = sum(w),.by = c(roadclass,bin)) |&gt; \n  drop_na(bin) |&gt; \n  mutate(w_perc = w/sum(w),.by = roadclass) |&gt; \n  mutate(w_cs = cumsum(w_perc),.by = roadclass) |&gt; \n  mutate(bin_n = str_extract(bin,\",.*]\") |&gt; \n           str_remove_all(\"(,|\\\\])\") |&gt; \n           as.numeric()) |&gt; \n  ggplot(aes(x = bin_n,\n             y = w_cs,\n             col = roadclass,group = roadclass)) +\n  geom_line()+\n  labs(x = \"log BC change\",\n       y = \"Cumulative distribution\",\n       col = \"Road class\")+\n  # scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n  theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk_logdiff\n\n\n\n\n\n\n\n\nExploration of distributions for all hours\n\nsimpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\")) |&gt;\n  mutate(roadclass = factor(\n    roadclass,\n    levels = c(\n      \"trunk\",\n      \"primary\",\n      \"secondary\",\n      \"tertiary\",\n      \"residential\",\n      \"unclassified\"\n    ),\n    ordered = T\n  )) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour != 3) |&gt;\n  ggplot(aes(x =  logdiff_max, y = factor(hour) |&gt; fct_rev())) +\n  geom_density_ridges(stat = \"binline\", bins = 100) +\n  facet_grid(day_type ~ roadclass)\n\n\n\n\n\n\n\n\nRelative change of BC across the network for the evening peak\n\nmap_reldiff &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt; \n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  ggplot(aes(col =  reldiff_max, linewidth = abs(reldiff_max))) +\n  geom_sf() +\n  scale_color_steps2(\n    mid = \"gray80\",\n    high = \"dodgerblue2\",\n    low = \"firebrick3\",\n    breaks = c(-2, -1, 0, 1, 2),\n    labels = scales::label_percent()\n  ) +\n  scale_linewidth_continuous(\n    transform = \"exp\",\n    limits = c(0.01, 3),\n    range = c(0.01, 1.5)\n  ) +\n  # scale_color_steps(palette = \"Spectral\",direction = -1)+\n  theme_void()+\n  labs(col = \"BC Relative change\")+\n  guides(linewidth = \"none\")+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.3,0.8))\n\nmap_reldiff\n\n\n\n\n\n\n\n\nwith log difference\n\nmap_logdiff &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt;\n  ggplot(aes(col =  logdiff_max, linewidth = abs(logdiff_max))) +\n  geom_sf() +\n  scale_color_steps2(\n    mid = \"gray80\",\n    high = \"dodgerblue2\",\n    low = \"firebrick3\",\n    breaks = c(-12, -8, -4, 0, 4, 8, 12),\n    # labels = scales::label_percent()\n  ) +\n  scale_linewidth_continuous(\n    limits = c(0.02, 12),\n    range = c(0.01, 20),\n    transform = \"exp\"\n  ) +\n  # scale_color_steps(palette = \"Spectral\",direction = -1)+\n  theme_void() +\n  labs(col = \"log BC difference\") +\n  guides(linewidth = \"none\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.3, 0.8))\n\nmap_logdiff\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n# tmap_mode(\"plot\")\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt;\n  tm_shape() +\n  tm_lines(\"reldiff_max\",)\n\n\n\n\n\n\n\n\n\nlast_dec &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(logdiff_max,\n                   list(d10 = \\(x) quantile(x, 0.90, na.rm = T))), .by =\n              c(hour, day_type, roadclass)) |&gt;\n  filter(roadclass == \"residential\")\n\n\nn_d10_logdiff_max &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt;  \n  mutate(d10_bool = logdiff_max &gt;= quantile(logdiff_max, 0.90,na.rm = T),\n         .by=c(hour,day_type,roadclass)) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n_d10 = sum(d10_bool,na.rm = T),.by = c(pair_id,day_type)) |&gt; \n  pivot_wider(names_from = day_type,values_from = n_d10)\n\n\nmap_ratruns_wk &lt;- simpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      }))) |&gt;\n  ggplot(aes(col =  weekday,\n             linewidth = weekday_ld)) +\n  geom_sf() +\n  scale_colour_viridis_b(na.value = \"gray30\",\n                         option = \"plasma\",\n                         direction = -1) +\n  scale_linewidth_continuous(\n    limits = c(0, 24),\n    range = c(0.05, 0.5),\n    transform = \"exp\"\n  ) +\n  theme_void() +\n  guides(linewidth = \"none\") +\n  labs(col = \"Rat-run potential\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.7))\n\nmap_ratruns_wk\n\n\n\n\n\n\n\n\nAn analysis of the\n\nsimpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n  })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) if_else(roadclass != \"residential\", 1, x)\n  ))) |&gt;\n  mutate(w = st_length(geom) |&gt; as.numeric()) |&gt;\n  st_drop_geometry() |&gt;\n  filter(roadclass == \"residential\") |&gt;\n  arrange(weekday) |&gt;\n  mutate(weekday = factor(weekday, ordered = T)) |&gt;\n  summarise(w = sum(w), .by = weekday) |&gt;\n  mutate(perc = w / sum(w)) |&gt;\n  mutate(cumsum(perc))\n\n   weekday          w        perc cumsum(perc)\n1        0   61375.52 0.014126573   0.01412657\n2        1 3181507.18 0.732275531   0.74640210\n3        2  258725.91 0.059549969   0.80595207\n4        3  131096.34 0.030173950   0.83612602\n5        4   79128.20 0.018212640   0.85433866\n6        5   60069.30 0.013825924   0.86816459\n7        6   42903.06 0.009874837   0.87803942\n8        7   33830.19 0.007786567   0.88582599\n9        8   24268.66 0.005585826   0.89141182\n10       9   19628.07 0.004517720   0.89592954\n11      10   16847.32 0.003877684   0.89980722\n12      11   18406.02 0.004236443   0.90404366\n13      12   15610.65 0.003593043   0.90763671\n14      13   17103.82 0.003936721   0.91157343\n15      14   16607.17 0.003822410   0.91539584\n16      15   20091.42 0.004624366   0.92002020\n17      16   20249.74 0.004660807   0.92468101\n18      17   30791.12 0.007087076   0.93176809\n19      18   31110.28 0.007160537   0.93892862\n20      19   32102.02 0.007388801   0.94631743\n21      20   45423.79 0.010455022   0.95677245\n22      21   40725.05 0.009373532   0.96614598\n23      22   50878.36 0.011710480   0.97785646\n24      23   57593.13 0.013255994   0.99111245\n25      24   38613.60 0.008887547   1.00000000"
  },
  {
    "objectID": "2B_Analysis_std_network.html#loading-data",
    "href": "2B_Analysis_std_network.html#loading-data",
    "title": "Results",
    "section": "",
    "text": "urban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt; st_transform(3116)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\n\n\nsf_net_exp &lt;- st_read(\"sf_network/full_sf_network_stdnet.gpkg\") |&gt; st_transform(3116)\n\nReading layer `full_sf_network_stdnet' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/full_sf_network_stdnet.gpkg' \n  using driver `GPKG'\nSimple feature collection with 232394 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  WGS 84\n\nsf_net &lt;- sf_net_exp[urban_perimeter,]\n\n\n\n\n\ncent_results &lt;- read_csv(\"sf_network/hourly_cent_results_stdnet.csv\",\n                       lazy = F)"
  },
  {
    "objectID": "2B_Analysis_std_network.html#simplified-network",
    "href": "2B_Analysis_std_network.html#simplified-network",
    "title": "Results",
    "section": "",
    "text": "As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects.\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(sf_net,sf_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\n# subset_net$pair_id &lt;- simp_groups$pair_id\nsf_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_results |&gt; \n  right_join(sf_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,day_type,hour))\n\n\nsummary_pairs_dist.jct &lt;- \n  sf_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- sf_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\n\n\nTotal length of road network\n\nsimpl_network_sf |&gt;\n  mutate(distance = st_length(geom, )) |&gt;\n  pull(distance) |&gt;\n  sum()\n\n6590563 [m]\n\n\n\nsimpl_network_sf |&gt;\n  mutate(distance = st_length(geom) |&gt;\n           as.numeric()) |&gt;\n  st_drop_geometry() |&gt;\n  mutate(roadclass = str_remove(roadclass,\"_link\")) |&gt; \n  summarise(d_weighted = sum(distance)/1e3,\n            .by=c(roadclass)) |&gt; \n  mutate(d_weighted = round(d_weighted),\n         roadclass = factor(roadclass,\n                            levels = c(\"trunk\",\n                                       \"primary\",\n                                       \"secondary\",\n                                       \"tertiary\",\n                                       \"residential\",\n                                       \"unclassified\"), \n                            ordered = T)) |&gt; \n  arrange(roadclass) |&gt; \n  mutate(portion = round(d_weighted/sum(d_weighted)*100,1),\n         roadclass = str_to_sentence(roadclass)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nd_weighted\nportion\n\n\n\n\nTrunk\n218\n3.3\n\n\nPrimary\n616\n9.3\n\n\nSecondary\n460\n7.0\n\n\nTertiary\n920\n14.0\n\n\nResidential\n4345\n65.9\n\n\nUnclassified\n32\n0.5\n\n\n\n\n\nExploring the changes in BC during the evening peak with a the empirical cumulative distribution function by road class.\n\n## Unweighted version\n\n# ecdf_18_wk &lt;- simpl_network_sf |&gt;\n#   mutate(roadclass = str_remove(roadclass, \"_link\"),\n#          w = st_length(geom) |&gt; as.numeric()) |&gt;\n#   mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n#   mutate(roadclass = factor(roadclass,\n#                             levels = c(\"Trunk\",\n#                                        \"Primary\",\n#                                        \"Secondary\",\n#                                        \"Tertiary\",\n#                                        \"Residential\",\n#                                        \"Unclassified\"), \n#                             ordered = T)) |&gt;\n#   st_drop_geometry() |&gt;\n#   # filter(roadclass %in% c(\"residential\")) |&gt;\n#   left_join(summary_pairs, by = \"pair_id\") |&gt;\n#   filter(hour == 18, day_type == \"weekday\") |&gt;\n#   ggplot(aes(x =  reldiff_max,\n#              col = roadclass)) +\n#   stat_ecdf(pad = F)+\n#   labs(x = \"Mean relative change in BC\",\n#        y = \"Cumulative distribution\",\n#        col = \"Road class\")+\n#   scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n#   theme_minimal()+\n#   theme(legend.position = \"top\")+\n#   scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk &lt;- simpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\"),\n         w = st_length(geom) |&gt; as.numeric()) |&gt;\n  mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n  mutate(roadclass = factor(roadclass,\n                            levels = c(\"Trunk\",\n                                       \"Primary\",\n                                       \"Secondary\",\n                                       \"Tertiary\",\n                                       \"Residential\",\n                                       \"Unclassified\"), \n                            ordered = T)) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  mutate(bin = cut(reldiff_max,\n                   breaks = seq(\n                     min(reldiff_max,na.rm = T)-0.01,\n                     max(reldiff_max,na.rm = T),\n                     0.005),ordered_result = T)) |&gt; \n  arrange(roadclass,bin) |&gt;\n  summarise(w = sum(w),.by = c(roadclass,bin)) |&gt; \n  drop_na(bin) |&gt; \n  mutate(w_perc = w/sum(w),.by = roadclass) |&gt; \n  mutate(w_cs = cumsum(w_perc),.by = roadclass) |&gt; \n  mutate(bin_n = str_extract(bin,\",.*]\") |&gt; \n           str_remove_all(\"(,|\\\\])\") |&gt; \n           as.numeric()) |&gt; \n  ggplot(aes(x = bin_n,\n             y = w_cs,\n             col = roadclass,group = roadclass)) +\n  geom_line()+\n  labs(x = \"Mean relative change in BC\",\n       y = \"Cumulative distribution\",\n       col = \"Road class\")+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n  theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk\n\n\n\n\n\n\n\n\necdf_18_wk_absdiff\n\necdf_18_wk_logdiff &lt;- simpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\"),\n         w = st_length(geom) |&gt; as.numeric()) |&gt;\n  mutate(roadclass = str_to_sentence(roadclass)) |&gt; \n  mutate(roadclass = factor(roadclass,\n                            levels = c(\"Trunk\",\n                                       \"Primary\",\n                                       \"Secondary\",\n                                       \"Tertiary\",\n                                       \"Residential\",\n                                       \"Unclassified\"), \n                            ordered = T)) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  mutate(bin = cut(logdiff_max,\n                   breaks = seq(\n                     min(logdiff_max,na.rm = T)-0.1,\n                     max(logdiff_max,na.rm = T),\n                     0.1),ordered_result = T)) |&gt; \n  arrange(roadclass,bin) |&gt;\n  summarise(w = sum(w),.by = c(roadclass,bin)) |&gt; \n  drop_na(bin) |&gt; \n  mutate(w_perc = w/sum(w),.by = roadclass) |&gt; \n  mutate(w_cs = cumsum(w_perc),.by = roadclass) |&gt; \n  mutate(bin_n = str_extract(bin,\",.*]\") |&gt; \n           str_remove_all(\"(,|\\\\])\") |&gt; \n           as.numeric()) |&gt; \n  ggplot(aes(x = bin_n,\n             y = w_cs,\n             col = roadclass,group = roadclass)) +\n  geom_line()+\n  labs(x = \"log BC change\",\n       y = \"Cumulative distribution\",\n       col = \"Road class\")+\n  # scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n  theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_color_manual(values =  paletteer_d(\"ggsci::lanonc_lancet\",n = 7))\n\n\necdf_18_wk_logdiff\n\n\n\n\n\n\n\n\nExploration of distributions for all hours\n\nsimpl_network_sf |&gt;\n  mutate(roadclass = str_remove(roadclass, \"_link\")) |&gt;\n  mutate(roadclass = factor(\n    roadclass,\n    levels = c(\n      \"trunk\",\n      \"primary\",\n      \"secondary\",\n      \"tertiary\",\n      \"residential\",\n      \"unclassified\"\n    ),\n    ordered = T\n  )) |&gt;\n  st_drop_geometry() |&gt;\n  # filter(roadclass %in% c(\"residential\")) |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour != 3) |&gt;\n  ggplot(aes(x =  logdiff_max, y = factor(hour) |&gt; fct_rev())) +\n  geom_density_ridges(stat = \"binline\", bins = 100) +\n  facet_grid(day_type ~ roadclass)\n\n\n\n\n\n\n\n\nRelative change of BC across the network for the evening peak\n\nmap_reldiff &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt; \n  filter(hour == 18, day_type == \"weekday\") |&gt; \n  ggplot(aes(col =  reldiff_max, linewidth = abs(reldiff_max))) +\n  geom_sf() +\n  scale_color_steps2(\n    mid = \"gray80\",\n    high = \"dodgerblue2\",\n    low = \"firebrick3\",\n    breaks = c(-2, -1, 0, 1, 2),\n    labels = scales::label_percent()\n  ) +\n  scale_linewidth_continuous(\n    transform = \"exp\",\n    limits = c(0.01, 3),\n    range = c(0.01, 1.5)\n  ) +\n  # scale_color_steps(palette = \"Spectral\",direction = -1)+\n  theme_void()+\n  labs(col = \"BC Relative change\")+\n  guides(linewidth = \"none\")+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.3,0.8))\n\nmap_reldiff\n\n\n\n\n\n\n\n\nwith log difference\n\nmap_logdiff &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt;\n  ggplot(aes(col =  logdiff_max, linewidth = abs(logdiff_max))) +\n  geom_sf() +\n  scale_color_steps2(\n    mid = \"gray80\",\n    high = \"dodgerblue2\",\n    low = \"firebrick3\",\n    breaks = c(-12, -8, -4, 0, 4, 8, 12),\n    # labels = scales::label_percent()\n  ) +\n  scale_linewidth_continuous(\n    limits = c(0.02, 12),\n    range = c(0.01, 20),\n    transform = \"exp\"\n  ) +\n  # scale_color_steps(palette = \"Spectral\",direction = -1)+\n  theme_void() +\n  labs(col = \"log BC difference\") +\n  guides(linewidth = \"none\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.3, 0.8))\n\nmap_logdiff\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n# tmap_mode(\"plot\")\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  filter(hour == 18, day_type == \"weekday\") |&gt;\n  tm_shape() +\n  tm_lines(\"reldiff_max\",)\n\n\n\n\n\n\n\n\n\nlast_dec &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs, by = \"pair_id\") |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(logdiff_max,\n                   list(d10 = \\(x) quantile(x, 0.90, na.rm = T))), .by =\n              c(hour, day_type, roadclass)) |&gt;\n  filter(roadclass == \"residential\")\n\n\nn_d10_logdiff_max &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt;  \n  mutate(d10_bool = logdiff_max &gt;= quantile(logdiff_max, 0.90,na.rm = T),\n         .by=c(hour,day_type,roadclass)) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n_d10 = sum(d10_bool,na.rm = T),.by = c(pair_id,day_type)) |&gt; \n  pivot_wider(names_from = day_type,values_from = n_d10)\n\n\nmap_ratruns_wk &lt;- simpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      }))) |&gt;\n  ggplot(aes(col =  weekday,\n             linewidth = weekday_ld)) +\n  geom_sf() +\n  scale_colour_viridis_b(na.value = \"gray30\",\n                         option = \"plasma\",\n                         direction = -1) +\n  scale_linewidth_continuous(\n    limits = c(0, 24),\n    range = c(0.05, 0.5),\n    transform = \"exp\"\n  ) +\n  theme_void() +\n  guides(linewidth = \"none\") +\n  labs(col = \"Rat-run potential\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.7))\n\nmap_ratruns_wk\n\n\n\n\n\n\n\n\nAn analysis of the\n\nsimpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n  })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) if_else(roadclass != \"residential\", 1, x)\n  ))) |&gt;\n  mutate(w = st_length(geom) |&gt; as.numeric()) |&gt;\n  st_drop_geometry() |&gt;\n  filter(roadclass == \"residential\") |&gt;\n  arrange(weekday) |&gt;\n  mutate(weekday = factor(weekday, ordered = T)) |&gt;\n  summarise(w = sum(w), .by = weekday) |&gt;\n  mutate(perc = w / sum(w)) |&gt;\n  mutate(cumsum(perc))\n\n   weekday          w        perc cumsum(perc)\n1        0   61375.52 0.014126573   0.01412657\n2        1 3181507.18 0.732275531   0.74640210\n3        2  258725.91 0.059549969   0.80595207\n4        3  131096.34 0.030173950   0.83612602\n5        4   79128.20 0.018212640   0.85433866\n6        5   60069.30 0.013825924   0.86816459\n7        6   42903.06 0.009874837   0.87803942\n8        7   33830.19 0.007786567   0.88582599\n9        8   24268.66 0.005585826   0.89141182\n10       9   19628.07 0.004517720   0.89592954\n11      10   16847.32 0.003877684   0.89980722\n12      11   18406.02 0.004236443   0.90404366\n13      12   15610.65 0.003593043   0.90763671\n14      13   17103.82 0.003936721   0.91157343\n15      14   16607.17 0.003822410   0.91539584\n16      15   20091.42 0.004624366   0.92002020\n17      16   20249.74 0.004660807   0.92468101\n18      17   30791.12 0.007087076   0.93176809\n19      18   31110.28 0.007160537   0.93892862\n20      19   32102.02 0.007388801   0.94631743\n21      20   45423.79 0.010455022   0.95677245\n22      21   40725.05 0.009373532   0.96614598\n23      22   50878.36 0.011710480   0.97785646\n24      23   57593.13 0.013255994   0.99111245\n25      24   38613.60 0.008887547   1.00000000"
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html",
    "href": "3A_Congested_graph_WWD_allowed.html",
    "title": "Congested Network",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\npackageVersion (\"dodgr\")\n\n[1] '0.4.1.44'\n\n\n\n\n\nsf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019 &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\n\n\n\n\ndodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\ndodgr::clear_dodgr_cache()\n\n# bog_centrality_bl &lt;- bog_contracted |&gt;\n#         dodgr_centrality(column = \"time_weighted\",\n#                          dist_threshold = 1200)\n\n\n\n\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\n\nspeed_key &lt;- speed_data |&gt;\n  select(year, hour, day_type) |&gt;\n  filter(day_type != \"friday\",year == 2019) |&gt;\n  unique()\n\n\nhourly_centrality &lt;- do.call(bind_cols,lapply(\n  1:nrow(speed_key),\n  \n  \\(i) {\n    speed_tbl &lt;- speed_data |&gt;\n      semi_join(speed_key[i, ],\n                by = join_by(year, day_type, hour)) |&gt;\n      select(TID, d_mean_speed)\n    \n    \n    obs_speeds_edges &lt;- TID_to_edge_id |&gt;\n      left_join(speed_tbl, by = \"TID\") |&gt;\n      summarise(obs_speed = mean(d_mean_speed), .by = edge_id)\n    \n    bog_contr_adjusted &lt;- bog_contracted\n    \n    dodgr::clear_dodgr_cache()\n    \n    bog_contr_adjusted$obs_speed &lt;-\n      tibble(edge_id = bog_contr_adjusted$edge_id) |&gt;\n      left_join(obs_speeds_edges, by = \"edge_id\") |&gt;\n      pull(obs_speed)\n    \n    dodgr::clear_dodgr_cache()\n    \n    bog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;-\n      (3.6 * bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)]) /\n      bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n    \n    try({\n      hourly_centrality &lt;-\n        bog_contr_adjusted |&gt;\n        dodgr_centrality(column = \"time_weighted\",\n                         dist_threshold = 1200)\n      \n      t &lt;-\n        tibble(cent = hourly_centrality$centrality)\n      \n      names(t) &lt;-\n        paste(names(t),\n              speed_key$year[i],\n              speed_key$hour[i],\n              speed_key$day_type[i],\n              sep = \"_\")\n      \n      return(t)\n    })\n    \n  }))\n\n\nhourly_centrality$edge_id &lt;- bog_contracted$edge_id\n\nsave(hourly_centrality,\n     file = \"sf_network/hourly_centrality.rdata\")\n\nPosted speed limit is not the actual travel speed for all links, even during non-congested hours. So we are going to use the hour of the day with the highest average speed as baseline.\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour))\n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(-mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday      3           0.963\n2  2019 weekday      2           0.950\n3  2019 weekday      1           0.925\n4  2019 weekday      4           0.918\n5  2019 weekday      0           0.873\n\n\n\nbl_id &lt;- summary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt; \n  slice_max(mean_norm_speed) |&gt; \n  mutate(id=paste(\"cent\",year,hour,day_type,sep = \"_\")) |&gt; \n  pull(id)\n\nAdding a column with the centrality results of the baseline hour\n\nhourly_centrality$cent_2019_bl &lt;- hourly_centrality |&gt; pull(bl_id)\n\nAdding the component id to the dataset\n\nhourly_centrality$component &lt;- bog_contracted$component\n\n\ntidy_hourly_centrality &lt;- hourly_centrality |&gt; \n  pivot_longer(cols = cent_2019_0_weekday:cent_2019_23_weekend,\n               names_to = \"scenario\",\n               values_to = \"cent\",\n               names_prefix = \"cent_2019_\") |&gt; \n  separate_wider_delim(scenario,\n                       delim = \"_\",\n                       names = c(\"hour\",\"day_type\")) |&gt; \n  left_join(n_nodes, by = join_by(component)) |&gt; \n  rename(cong = cent,\n         ff = cent_2019_bl) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  # mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 ,0,x))) |&gt;\n  select(-n,-component)\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nrm(dist_matrix)\n\n\nsf_net_jct &lt;- sf_net |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))\n\n\n\n\n\nwrite_csv(tidy_hourly_centrality, file = \"sf_network/hourly_cent_results.csv\")\nst_write(sf_net_jct, \"sf_network/full_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network.gpkg' using driver `GPKG'\nWriting layer `full_sf_network' to data source \n  `sf_network/full_sf_network.gpkg' using driver `GPKG'\nWriting 242294 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html#loading-network",
    "href": "3A_Congested_graph_WWD_allowed.html#loading-network",
    "title": "Congested Network",
    "section": "",
    "text": "sf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019 &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)"
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html#baseline-graph-building",
    "href": "3A_Congested_graph_WWD_allowed.html#baseline-graph-building",
    "title": "Congested Network",
    "section": "",
    "text": "dodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\ndodgr::clear_dodgr_cache()\n\n# bog_centrality_bl &lt;- bog_contracted |&gt;\n#         dodgr_centrality(column = \"time_weighted\",\n#                          dist_threshold = 1200)"
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html#using-observed-speed-to-adjust-the-weighting-of-the-graph",
    "href": "3A_Congested_graph_WWD_allowed.html#using-observed-speed-to-adjust-the-weighting-of-the-graph",
    "title": "Congested Network",
    "section": "",
    "text": "sf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\n\nspeed_key &lt;- speed_data |&gt;\n  select(year, hour, day_type) |&gt;\n  filter(day_type != \"friday\",year == 2019) |&gt;\n  unique()\n\n\nhourly_centrality &lt;- do.call(bind_cols,lapply(\n  1:nrow(speed_key),\n  \n  \\(i) {\n    speed_tbl &lt;- speed_data |&gt;\n      semi_join(speed_key[i, ],\n                by = join_by(year, day_type, hour)) |&gt;\n      select(TID, d_mean_speed)\n    \n    \n    obs_speeds_edges &lt;- TID_to_edge_id |&gt;\n      left_join(speed_tbl, by = \"TID\") |&gt;\n      summarise(obs_speed = mean(d_mean_speed), .by = edge_id)\n    \n    bog_contr_adjusted &lt;- bog_contracted\n    \n    dodgr::clear_dodgr_cache()\n    \n    bog_contr_adjusted$obs_speed &lt;-\n      tibble(edge_id = bog_contr_adjusted$edge_id) |&gt;\n      left_join(obs_speeds_edges, by = \"edge_id\") |&gt;\n      pull(obs_speed)\n    \n    dodgr::clear_dodgr_cache()\n    \n    bog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;-\n      (3.6 * bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)]) /\n      bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n    \n    try({\n      hourly_centrality &lt;-\n        bog_contr_adjusted |&gt;\n        dodgr_centrality(column = \"time_weighted\",\n                         dist_threshold = 1200)\n      \n      t &lt;-\n        tibble(cent = hourly_centrality$centrality)\n      \n      names(t) &lt;-\n        paste(names(t),\n              speed_key$year[i],\n              speed_key$hour[i],\n              speed_key$day_type[i],\n              sep = \"_\")\n      \n      return(t)\n    })\n    \n  }))\n\n\nhourly_centrality$edge_id &lt;- bog_contracted$edge_id\n\nsave(hourly_centrality,\n     file = \"sf_network/hourly_centrality.rdata\")\n\nPosted speed limit is not the actual travel speed for all links, even during non-congested hours. So we are going to use the hour of the day with the highest average speed as baseline.\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour))\n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(-mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday      3           0.963\n2  2019 weekday      2           0.950\n3  2019 weekday      1           0.925\n4  2019 weekday      4           0.918\n5  2019 weekday      0           0.873\n\n\n\nbl_id &lt;- summary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt; \n  slice_max(mean_norm_speed) |&gt; \n  mutate(id=paste(\"cent\",year,hour,day_type,sep = \"_\")) |&gt; \n  pull(id)\n\nAdding a column with the centrality results of the baseline hour\n\nhourly_centrality$cent_2019_bl &lt;- hourly_centrality |&gt; pull(bl_id)\n\nAdding the component id to the dataset\n\nhourly_centrality$component &lt;- bog_contracted$component\n\n\ntidy_hourly_centrality &lt;- hourly_centrality |&gt; \n  pivot_longer(cols = cent_2019_0_weekday:cent_2019_23_weekend,\n               names_to = \"scenario\",\n               values_to = \"cent\",\n               names_prefix = \"cent_2019_\") |&gt; \n  separate_wider_delim(scenario,\n                       delim = \"_\",\n                       names = c(\"hour\",\"day_type\")) |&gt; \n  left_join(n_nodes, by = join_by(component)) |&gt; \n  rename(cong = cent,\n         ff = cent_2019_bl) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  # mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 ,0,x))) |&gt;\n  select(-n,-component)"
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html#extracting-distance-to-the-major-network",
    "href": "3A_Congested_graph_WWD_allowed.html#extracting-distance-to-the-major-network",
    "title": "Congested Network",
    "section": "",
    "text": "The distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nrm(dist_matrix)\n\n\nsf_net_jct &lt;- sf_net |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))"
  },
  {
    "objectID": "3A_Congested_graph_WWD_allowed.html#saving-results",
    "href": "3A_Congested_graph_WWD_allowed.html#saving-results",
    "title": "Congested Network",
    "section": "",
    "text": "write_csv(tidy_hourly_centrality, file = \"sf_network/hourly_cent_results.csv\")\nst_write(sf_net_jct, \"sf_network/full_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network.gpkg' using driver `GPKG'\nWriting layer `full_sf_network' to data source \n  `sf_network/full_sf_network.gpkg' using driver `GPKG'\nWriting 242294 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "1C_infractions.html",
    "href": "1C_infractions.html",
    "title": "Ticket Data",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"tmap\",\n    \"webshot2\",\n    \"gganimate\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n       sf tidyverse      tmap  webshot2 gganimate \n     TRUE      TRUE      TRUE      TRUE      TRUE \n\ntmap_mode(\"plot\")\n\n\n\n\nif(!dir.exists(\"raw_data/bogota\")) {\n  if (!file.exists(\"raw_data/bogota_data.zip\")) {\n    options(timeout = 180)\n    download.file(\n      \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/bogota_data.zip\",\n      destfile = \"raw_data/bogota_data.zip\",\n      mode = \"wb\"\n    )\n    unzip(\"raw_data/bogota_data.zip\", exdir = \"raw_data\")\n  }\n}\n\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt;\n  st_transform(4326)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\n\n\n\n\nreports2019 &lt;- read_csv(\"raw_data/bogota/Comparendos_2019_Bogota_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          OBJECTID = col_double(),\n                          NUM_COMPARENDO = col_character(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURRENCIA = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETECCION = col_character(),\n                          CLASE_VEHICULO = col_character(),\n                          TIPO_SERVICIO = col_character(), \n                          INFRACCION = col_character(),\n                          DES_INFRACCION = col_character(),\n                          LOCALIDAD = col_character(),\n                          MUNICIPIO = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double(),\n                          GlobalID = col_character()\n                        ))\n\nreports2020 &lt;- read_csv(\"raw_data/bogota/Comparendos_DEI_2020_Bogot%C3%A1_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          FID = col_double(),\n                          OBJECTID = col_double(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURR = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETE = col_character(),\n                          CLASE_VEHI = col_character(),\n                          TIPO_SERVI = col_character(),\n                          INFRACCION = col_character(),\n                          DES_INFRAC = col_character(),\n                          MUNICIPIO = col_character(),\n                          PAIS = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double()))\n\nA list of the codes with codes of offences related to vehicles circulating.\n\nselected_codes &lt;- read.csv(\"list_infractions.csv\")$INFRACCION\n\n\n\n\n\nreports2019 |&gt; \n  count(INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.90)) |&gt; \n  ggplot(aes(y = fct_reorder(INFRACCION,n),\n             x = n))+\n  geom_col()+\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\ncount_infractions &lt;- reports2019 |&gt; \n  count(INFRACCION,DES_INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.70)) |&gt; \n  arrange(-n)\n\n\n\n\nCreating an sf object with the reports for driving in the wrong-way\n\nwrong_way_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nwrong_way_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\n\nmanual_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nmanual_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\n\n\n\nWe will select only relevant columns and also classify the records by type of day. For that, we will use the bank holiday list we used for the speed data processing.\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nwwd_2019_clean &lt;- wrong_way_2019_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\n\nwwd_2020_clean &lt;- wrong_way_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nAll manual tickets\n\nmanual_2019_clean &lt;-  manual_2019_sf|&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\n\nmanual_2020_clean &lt;- manual_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nAnd consolidating the two datasets\n\nall_wwd_reports &lt;- bind_rows(wwd_2019_clean,wwd_2020_clean)[urban_perimeter,] |&gt; \n  mutate(veh_class = str_to_lower(veh_class)) |&gt; drop_na()\n\nall_manual_reports &lt;- bind_rows(manual_2019_clean,manual_2020_clean)[urban_perimeter,] |&gt; \n  mutate(veh_class = str_to_lower(veh_class)) |&gt; drop_na()\n\n\n\n\n\n\nA daily profile\n\nggplot(all_wwd_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()+\n  labs(title = \"WWD reports\")+\n  facet_grid(.~day_type)\n\n\n\n\n\n\n\nggplot(all_manual_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()+\n  labs(title = \"All manual reports\")+\n  facet_grid(.~day_type)\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\nall_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nA check of the reports per day\n\nexp_dates_count &lt;- all_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_wwd_reports$date_time)),\n                      max(date(all_wwd_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nexp_dates_count_all &lt;- all_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_manual_reports$date_time)),\n                      max(date(all_manual_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nA quick check of the timeline reveals gaps in the reports of 2019. We do not know the causes.\n\nexp_dates_count |&gt; \n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  ggplot(aes(x = date,y = n))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nexp_dates_count |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)+\n  labs(title = \"Daily WWD reports\")\n\n\n\n\n\n\n\nexp_dates_count_all |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)+\n  labs(title = \"Daily manual reports (all)\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(all_wwd_reports,aes(col = factor(year)))+\n  geom_sf(alpha = 0.2,size = 0.5)+\n  theme_void()   \n\n\n\n\n\n\n\n\nGenerating a grid covering the urban perimeter\n\ngrid_bog &lt;- urban_perimeter |&gt; \n  st_transform(3116) |&gt; \n  st_make_grid(cellsize = 1e3) |&gt; \n  st_as_sf() \n\n\ngrid_bog &lt;- grid_bog[urban_perimeter |&gt; \n                       st_transform(3116) |&gt;\n                       st_union() |&gt; \n                       st_convex_hull(),] |&gt; \n  st_transform(st_crs(all_manual_reports)) |&gt; \n  rowid_to_column(\"cell_id\")\n\nAssigning the reports to the grid cells\n\nall_manual_grid &lt;- all_manual_reports\n\nall_manual_grid$grid_cell &lt;- st_intersects(all_manual_reports,\n                                              grid_bog) |&gt; \n  unlist()\n\n\nexp_hourly_grid &lt;- expand_grid(grid_cell = all_manual_grid$grid_cell |&gt; unique(),\n                               hour=all_manual_grid$hour |&gt; unique(),\n                               date = seq(min(date(all_manual_grid$date_time)),\n                                          max(date(all_manual_grid$date_time)),\n                                          by = \"1 day\"))\n\nhourly_grid &lt;- all_manual_grid |&gt; \n  st_drop_geometry() |&gt; \n  count(grid_cell,date = date(timestamp),hour) |&gt;\n  right_join(exp_hourly_grid,\n             by = join_by(date, hour,grid_cell)) |&gt;\n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  mutate(year = year(date),\n         day_type = case_when(date %in% bank_holidays~\"weekend\",\n                              wday(date, week_start = 1)&lt;=4~\"weekday\",\n                              wday(date, week_start = 1) == 5~\"friday\",\n                              T~\"weekend\")) |&gt; \n  summarise(across(n,\n                   list(mean = mean,\n                        median = median)),\n            .by = c(grid_cell,hour,year,day_type)\n            ) \n\n\np &lt;- grid_bog |&gt;\n  right_join(hourly_grid |&gt;\n               expand(grid_cell,hour,year,day_type),\n             by = c(\"cell_id\" = \"grid_cell\")) |&gt;\n  left_join(hourly_grid |&gt; filter(n_mean&gt;0),\n            by = c(\"cell_id\" = \"grid_cell\",\"hour\",\"year\",\"day_type\")) |&gt;\n            # by = c(\"cell_id\" = \"grid_cell\")) |&gt;\n  filter(day_type == \"weekday\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = n_mean,\n              group = cell_id),\n          alpha = 0.6) +\n  scale_fill_viridis_b(na.value = \"gray80\") +\n  theme_void() +\n  facet_grid(. ~ year) +\n  labs(title = \"Mean hourly reports\",\n    subtitle = 'Hour: {closest_state}') +\n  transition_states(hour, transition_length = 2, state_length = 1) +\n  enter_appear()\n\nanim_save(animation = p,filename = \"sf_network/map_animated.gif\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us check if there is any patter related to the day of the week\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nThere seems to be a pattern that might be related to the sampling i.e. how the enforcement officers are assigned along the week.\nLet’s compare with other offences reported manually by officers\n\nall_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nWe can also inspect the median number of tickets per weekday for all offences that are reported by traffic management officers in 2019 and 2020\n\nall_manual_reports |&gt; \n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nIs it a problem with the data, as some months are not reported? Let’s see if the pattern is similar across months\n\nall_manual_reports |&gt; \n  st_drop_geometry() |&gt;  \n  mutate(month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month,year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month,year)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")+\n  theme_minimal()+\n  facet_grid(.~year)+\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nVisually, there seems to be a consistent pattern of a low number of reports during Friday and Saturday, followed by a surge on Sundays.\n\n\n\nLet’s assign all reports to the nearest link to explore where the enforcement offices tend to catch the offenders\n\nnet_bog_2019 &lt;-\n  st_read(file.path(\"sf_network\", \"bogota_osm_network.gpkg\")) |&gt;\n  st_transform(3116)\n\nMultiple layers are present in data source /home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg, reading layer `network_2019'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nmajor_network &lt;- net_bog_2019 |&gt;\n  filter(!(highway %in% c(\"residential\",\"unclassified\")))\n\n\nmanual_reports_2019 &lt;- reports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\",INFRACCION %in% selected_codes) |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION,\n         LONGITUD,LATITUD) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326) |&gt; \n  st_transform(3116)\n\n\nmanual_reports_2019$near_index &lt;- st_nearest_feature(manual_reports_2019,net_bog_2019)\nmanual_reports_2019$near_major_index &lt;- st_nearest_feature(manual_reports_2019,major_network)\n\n\nmanual_reports_2019$highway &lt;- net_bog_2019$highway[manual_reports_2019$near_index]\nmanual_reports_2019$oneway &lt;- net_bog_2019$oneway[manual_reports_2019$near_index]\n\n\nmanual_reports_2019$dist_to_major &lt;- st_distance(\n  manual_reports_2019,\n  major_network[manual_reports_2019$near_major_index,],\n  by_element = T) |&gt; as.numeric()\n\nType of road where the offence was reported\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt; \n  ggplot(aes(x = fct_reorder(highway,n,.desc = F),y = n))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\n\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt;\n  mutate(highway = factor(highway,\n                          levels =\n                            c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                          ordered = T)) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt;\n  arrange(highway) |&gt; \n  mutate(perc = n/sum(n)) |&gt;\n  # mutate(prev = cumsum(perc)-perc) |&gt; \n  # select(-n) |&gt; \n  # pivot_longer(-highway) |&gt; \n  ggplot(aes(x = 1,y = perc,fill = highway))+\n  geom_col(position = \"stack\")+\n  # scale_alpha_manual(values = c(1,0.2))+\n  coord_flip()+\n  theme_minimal()+\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nMedian daily reports in residential roads\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  filter(highway == \"residential\") |&gt;\n  count(date = date(timestamp),offence) |&gt; \n  summarise(n = median(n),.by = c(offence)) |&gt;\n  slice_max(n,n=15) |&gt; \n  ggplot(aes(x = fct_reorder(offence,n), y = n))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\n\nEuclidean distance to major network vs all reports\n\nggplot()+\n  geom_histogram(data = manual_reports_2019,\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nggplot()+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.2)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"),offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\n\nnet_points &lt;- net_bog_2019 |&gt;\n  st_cast(\"POINT\") |&gt;\n  slice_head(by = osm_id)\nnet_points &lt;- net_bog_2019 |&gt; st_centroid()\n\nnet_points$near_id &lt;- st_nearest_feature(net_points,major_network)\nnet_points$dist_to_major &lt;- st_distance(net_points,\n                                        major_network[net_points$near_id,],\n                                        by_element = T) |&gt;\n  as.numeric()\n\n\nggplot()+\n  geom_density(data = net_points|&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_density(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt(breaks = c(0,0.1,0.25,.5,1,2.5,5,7.5,10)*1e3)\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntm_shape(major_network)+\n  tm_lines()+\n  tm_shape(manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"))+\n  tm_dots(\"dist_to_major\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt;  \n  count(veh_class,year) |&gt; \n  arrange(-n) |&gt; \n  mutate(n = n/sum(n),.by=year) |&gt; \n  ggplot(aes(x = fct_reorder(veh_class,n,.desc = F),  y = n, fill = factor(year)))+\n  geom_col(position = \"dodge\",)+\n  coord_flip()+\n  scale_y_continuous(labels = scales::label_percent())+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"dodgerblue3\",\"firebrick3\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nst_write(all_wwd_reports,\n         dsn = \"sf_network/wwd_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting layer `wwd_clean_sf' to data source \n  `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting 6869 features with 8 fields and geometry type Point.\n\n\n\nst_write(all_manual_reports,\n         dsn = \"sf_network/manualtickets_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting layer `manualtickets_clean_sf' to data source \n  `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting 395211 features with 9 fields and geometry type Point."
  },
  {
    "objectID": "1C_infractions.html#download-data",
    "href": "1C_infractions.html#download-data",
    "title": "Ticket Data",
    "section": "",
    "text": "if(!dir.exists(\"raw_data/bogota\")) {\n  if (!file.exists(\"raw_data/bogota_data.zip\")) {\n    options(timeout = 180)\n    download.file(\n      \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/bogota_data.zip\",\n      destfile = \"raw_data/bogota_data.zip\",\n      mode = \"wb\"\n    )\n    unzip(\"raw_data/bogota_data.zip\", exdir = \"raw_data\")\n  }\n}\n\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt;\n  st_transform(4326)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS"
  },
  {
    "objectID": "1C_infractions.html#loading-infractions",
    "href": "1C_infractions.html#loading-infractions",
    "title": "Ticket Data",
    "section": "",
    "text": "reports2019 &lt;- read_csv(\"raw_data/bogota/Comparendos_2019_Bogota_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          OBJECTID = col_double(),\n                          NUM_COMPARENDO = col_character(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURRENCIA = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETECCION = col_character(),\n                          CLASE_VEHICULO = col_character(),\n                          TIPO_SERVICIO = col_character(), \n                          INFRACCION = col_character(),\n                          DES_INFRACCION = col_character(),\n                          LOCALIDAD = col_character(),\n                          MUNICIPIO = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double(),\n                          GlobalID = col_character()\n                        ))\n\nreports2020 &lt;- read_csv(\"raw_data/bogota/Comparendos_DEI_2020_Bogot%C3%A1_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          FID = col_double(),\n                          OBJECTID = col_double(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURR = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETE = col_character(),\n                          CLASE_VEHI = col_character(),\n                          TIPO_SERVI = col_character(),\n                          INFRACCION = col_character(),\n                          DES_INFRAC = col_character(),\n                          MUNICIPIO = col_character(),\n                          PAIS = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double()))\n\nA list of the codes with codes of offences related to vehicles circulating.\n\nselected_codes &lt;- read.csv(\"list_infractions.csv\")$INFRACCION"
  },
  {
    "objectID": "1C_infractions.html#exploring-types-of-infractions",
    "href": "1C_infractions.html#exploring-types-of-infractions",
    "title": "Ticket Data",
    "section": "",
    "text": "reports2019 |&gt; \n  count(INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.90)) |&gt; \n  ggplot(aes(y = fct_reorder(INFRACCION,n),\n             x = n))+\n  geom_col()+\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\ncount_infractions &lt;- reports2019 |&gt; \n  count(INFRACCION,DES_INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.70)) |&gt; \n  arrange(-n)"
  },
  {
    "objectID": "1C_infractions.html#wrong-way-infraction",
    "href": "1C_infractions.html#wrong-way-infraction",
    "title": "Ticket Data",
    "section": "",
    "text": "Creating an sf object with the reports for driving in the wrong-way\n\nwrong_way_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nwrong_way_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\n\nmanual_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nmanual_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)"
  },
  {
    "objectID": "1C_infractions.html#tidying-up-the-datasets",
    "href": "1C_infractions.html#tidying-up-the-datasets",
    "title": "Ticket Data",
    "section": "",
    "text": "We will select only relevant columns and also classify the records by type of day. For that, we will use the bank holiday list we used for the speed data processing.\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nwwd_2019_clean &lt;- wrong_way_2019_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\n\nwwd_2020_clean &lt;- wrong_way_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nAll manual tickets\n\nmanual_2019_clean &lt;-  manual_2019_sf|&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\n\nmanual_2020_clean &lt;- manual_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nAnd consolidating the two datasets\n\nall_wwd_reports &lt;- bind_rows(wwd_2019_clean,wwd_2020_clean)[urban_perimeter,] |&gt; \n  mutate(veh_class = str_to_lower(veh_class)) |&gt; drop_na()\n\nall_manual_reports &lt;- bind_rows(manual_2019_clean,manual_2020_clean)[urban_perimeter,] |&gt; \n  mutate(veh_class = str_to_lower(veh_class)) |&gt; drop_na()"
  },
  {
    "objectID": "1C_infractions.html#eda",
    "href": "1C_infractions.html#eda",
    "title": "Ticket Data",
    "section": "",
    "text": "A daily profile\n\nggplot(all_wwd_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()+\n  labs(title = \"WWD reports\")+\n  facet_grid(.~day_type)\n\n\n\n\n\n\n\nggplot(all_manual_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()+\n  labs(title = \"All manual reports\")+\n  facet_grid(.~day_type)\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\nall_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nA check of the reports per day\n\nexp_dates_count &lt;- all_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_wwd_reports$date_time)),\n                      max(date(all_wwd_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nexp_dates_count_all &lt;- all_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_manual_reports$date_time)),\n                      max(date(all_manual_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nA quick check of the timeline reveals gaps in the reports of 2019. We do not know the causes.\n\nexp_dates_count |&gt; \n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  ggplot(aes(x = date,y = n))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nexp_dates_count |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)+\n  labs(title = \"Daily WWD reports\")\n\n\n\n\n\n\n\nexp_dates_count_all |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)+\n  labs(title = \"Daily manual reports (all)\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(all_wwd_reports,aes(col = factor(year)))+\n  geom_sf(alpha = 0.2,size = 0.5)+\n  theme_void()   \n\n\n\n\n\n\n\n\nGenerating a grid covering the urban perimeter\n\ngrid_bog &lt;- urban_perimeter |&gt; \n  st_transform(3116) |&gt; \n  st_make_grid(cellsize = 1e3) |&gt; \n  st_as_sf() \n\n\ngrid_bog &lt;- grid_bog[urban_perimeter |&gt; \n                       st_transform(3116) |&gt;\n                       st_union() |&gt; \n                       st_convex_hull(),] |&gt; \n  st_transform(st_crs(all_manual_reports)) |&gt; \n  rowid_to_column(\"cell_id\")\n\nAssigning the reports to the grid cells\n\nall_manual_grid &lt;- all_manual_reports\n\nall_manual_grid$grid_cell &lt;- st_intersects(all_manual_reports,\n                                              grid_bog) |&gt; \n  unlist()\n\n\nexp_hourly_grid &lt;- expand_grid(grid_cell = all_manual_grid$grid_cell |&gt; unique(),\n                               hour=all_manual_grid$hour |&gt; unique(),\n                               date = seq(min(date(all_manual_grid$date_time)),\n                                          max(date(all_manual_grid$date_time)),\n                                          by = \"1 day\"))\n\nhourly_grid &lt;- all_manual_grid |&gt; \n  st_drop_geometry() |&gt; \n  count(grid_cell,date = date(timestamp),hour) |&gt;\n  right_join(exp_hourly_grid,\n             by = join_by(date, hour,grid_cell)) |&gt;\n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  mutate(year = year(date),\n         day_type = case_when(date %in% bank_holidays~\"weekend\",\n                              wday(date, week_start = 1)&lt;=4~\"weekday\",\n                              wday(date, week_start = 1) == 5~\"friday\",\n                              T~\"weekend\")) |&gt; \n  summarise(across(n,\n                   list(mean = mean,\n                        median = median)),\n            .by = c(grid_cell,hour,year,day_type)\n            ) \n\n\np &lt;- grid_bog |&gt;\n  right_join(hourly_grid |&gt;\n               expand(grid_cell,hour,year,day_type),\n             by = c(\"cell_id\" = \"grid_cell\")) |&gt;\n  left_join(hourly_grid |&gt; filter(n_mean&gt;0),\n            by = c(\"cell_id\" = \"grid_cell\",\"hour\",\"year\",\"day_type\")) |&gt;\n            # by = c(\"cell_id\" = \"grid_cell\")) |&gt;\n  filter(day_type == \"weekday\") |&gt;\n  ggplot() +\n  geom_sf(aes(fill = n_mean,\n              group = cell_id),\n          alpha = 0.6) +\n  scale_fill_viridis_b(na.value = \"gray80\") +\n  theme_void() +\n  facet_grid(. ~ year) +\n  labs(title = \"Mean hourly reports\",\n    subtitle = 'Hour: {closest_state}') +\n  transition_states(hour, transition_length = 2, state_length = 1) +\n  enter_appear()\n\nanim_save(animation = p,filename = \"sf_network/map_animated.gif\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us check if there is any patter related to the day of the week\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nThere seems to be a pattern that might be related to the sampling i.e. how the enforcement officers are assigned along the week.\nLet’s compare with other offences reported manually by officers\n\nall_manual_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nWe can also inspect the median number of tickets per weekday for all offences that are reported by traffic management officers in 2019 and 2020\n\nall_manual_reports |&gt; \n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nIs it a problem with the data, as some months are not reported? Let’s see if the pattern is similar across months\n\nall_manual_reports |&gt; \n  st_drop_geometry() |&gt;  \n  mutate(month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month,year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month,year)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")+\n  theme_minimal()+\n  facet_grid(.~year)+\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nVisually, there seems to be a consistent pattern of a low number of reports during Friday and Saturday, followed by a surge on Sundays.\n\n\n\nLet’s assign all reports to the nearest link to explore where the enforcement offices tend to catch the offenders\n\nnet_bog_2019 &lt;-\n  st_read(file.path(\"sf_network\", \"bogota_osm_network.gpkg\")) |&gt;\n  st_transform(3116)\n\nMultiple layers are present in data source /home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg, reading layer `network_2019'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54535 features and 17 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nmajor_network &lt;- net_bog_2019 |&gt;\n  filter(!(highway %in% c(\"residential\",\"unclassified\")))\n\n\nmanual_reports_2019 &lt;- reports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\",INFRACCION %in% selected_codes) |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION,\n         LONGITUD,LATITUD) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326) |&gt; \n  st_transform(3116)\n\n\nmanual_reports_2019$near_index &lt;- st_nearest_feature(manual_reports_2019,net_bog_2019)\nmanual_reports_2019$near_major_index &lt;- st_nearest_feature(manual_reports_2019,major_network)\n\n\nmanual_reports_2019$highway &lt;- net_bog_2019$highway[manual_reports_2019$near_index]\nmanual_reports_2019$oneway &lt;- net_bog_2019$oneway[manual_reports_2019$near_index]\n\n\nmanual_reports_2019$dist_to_major &lt;- st_distance(\n  manual_reports_2019,\n  major_network[manual_reports_2019$near_major_index,],\n  by_element = T) |&gt; as.numeric()\n\nType of road where the offence was reported\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt; \n  ggplot(aes(x = fct_reorder(highway,n,.desc = F),y = n))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\n\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt;\n  mutate(highway = factor(highway,\n                          levels =\n                            c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                          ordered = T)) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt;\n  arrange(highway) |&gt; \n  mutate(perc = n/sum(n)) |&gt;\n  # mutate(prev = cumsum(perc)-perc) |&gt; \n  # select(-n) |&gt; \n  # pivot_longer(-highway) |&gt; \n  ggplot(aes(x = 1,y = perc,fill = highway))+\n  geom_col(position = \"stack\")+\n  # scale_alpha_manual(values = c(1,0.2))+\n  coord_flip()+\n  theme_minimal()+\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nMedian daily reports in residential roads\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  filter(highway == \"residential\") |&gt;\n  count(date = date(timestamp),offence) |&gt; \n  summarise(n = median(n),.by = c(offence)) |&gt;\n  slice_max(n,n=15) |&gt; \n  ggplot(aes(x = fct_reorder(offence,n), y = n))+\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\n\n\nEuclidean distance to major network vs all reports\n\nggplot()+\n  geom_histogram(data = manual_reports_2019,\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nggplot()+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.2)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"),offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\n\nnet_points &lt;- net_bog_2019 |&gt;\n  st_cast(\"POINT\") |&gt;\n  slice_head(by = osm_id)\nnet_points &lt;- net_bog_2019 |&gt; st_centroid()\n\nnet_points$near_id &lt;- st_nearest_feature(net_points,major_network)\nnet_points$dist_to_major &lt;- st_distance(net_points,\n                                        major_network[net_points$near_id,],\n                                        by_element = T) |&gt;\n  as.numeric()\n\n\nggplot()+\n  geom_density(data = net_points|&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_density(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt(breaks = c(0,0.1,0.25,.5,1,2.5,5,7.5,10)*1e3)\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntm_shape(major_network)+\n  tm_lines()+\n  tm_shape(manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"))+\n  tm_dots(\"dist_to_major\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt;  \n  count(veh_class,year) |&gt; \n  arrange(-n) |&gt; \n  mutate(n = n/sum(n),.by=year) |&gt; \n  ggplot(aes(x = fct_reorder(veh_class,n,.desc = F),  y = n, fill = factor(year)))+\n  geom_col(position = \"dodge\",)+\n  coord_flip()+\n  scale_y_continuous(labels = scales::label_percent())+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"dodgerblue3\",\"firebrick3\"))"
  },
  {
    "objectID": "1C_infractions.html#saving-clean-datasets",
    "href": "1C_infractions.html#saving-clean-datasets",
    "title": "Ticket Data",
    "section": "",
    "text": "st_write(all_wwd_reports,\n         dsn = \"sf_network/wwd_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting layer `wwd_clean_sf' to data source \n  `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting 6869 features with 8 fields and geometry type Point.\n\n\n\nst_write(all_manual_reports,\n         dsn = \"sf_network/manualtickets_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting layer `manualtickets_clean_sf' to data source \n  `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting 395211 features with 9 fields and geometry type Point."
  },
  {
    "objectID": "D3_Offences_joining.html",
    "href": "D3_Offences_joining.html",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\",\n    \"ggExtra\",\n    \"effectsize\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap     ggExtra  effectsize \n       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n\n\n\n\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\n# off_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\noff_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 396988 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.21482 ymin: 4.469726 xmax: -74.01338 ymax: 4.822895\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\ncent_tests_wkend &lt;- read_csv(\"sf_network/cent_tests_wkend.csv\",\n                             lazy = F)\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\nsummary_pairs_wkend &lt;- cent_tests_wkend |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n  #        .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\noff_sf_wkend &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekend\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\nminor_offences_wkend &lt;- off_sf_wkend[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\nminor_offences_wkend$near_index &lt;- st_nearest_feature(minor_offences_wkend,simpl_network_sf)\nminor_offences_wkend$pair_id &lt;- simpl_network_sf$pair_id[minor_offences_wkend$near_index]\n\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nmodel_data_wkend &lt;- (summary_pairs_wkend |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences_wkend$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\n## Congested\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n## Weekend\nmodel_data_wkend |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\ntest1_wkend &lt;- model_data_wkend |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_wkend &lt;- model_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nmod_pred_0_wkend |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs &lt;- glm_models_0 |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\nmod_0_coefs_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\n\ncontrol_rates &lt;- mod_0_coefs_wkend |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\noddsratio_to_riskratio(glm_models_0$model_rel[[1]])\n\nParameter   | Risk Ratio |       95% CI\n---------------------------------------\n(Intercept) |       0.02 |             \nlogdiff max |       0.46 | [0.27, 0.82]\n\nRR_summary &lt;-mod_0_coefs |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary |&gt; \n  ggplot(aes(x = RR, y = dist.th, col =wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nmodel_reports &lt;- model_data |&gt;\n  filter(offence_bool)\n\n\nrand_absences_data &lt;- bind_rows(\n  model_reports,\n  model_data |&gt;\n    filter(!offence_bool) |&gt;\n    sample_n(size = nrow(model_reports), replace = F)\n)\n\n\n# Same links are considered\nrand_absences_data_wkend &lt;- model_data_wkend |&gt; \n  semi_join(rand_absences_data,by = join_by(pair_id,wwd.speed,dist.th))\n\n\nglm_models_0_rand &lt;- rand_absences_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand &lt;- glm_models_0_rand |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_rand_wkend &lt;- rand_absences_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand_wkend &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nmod_pred_0_rand |&gt;\n  filter(wwd.speed == 12) |&gt;\n  ggplot(aes(\n    x = logdiff_max,\n    y = offence_bool,\n    group = id,\n    col = dist.th\n  )) +\n  geom_line(alpha = 0.3) +\n  geom_line(\n    data = mod_pred_0_rand_wkend |&gt;\n      filter(wwd.speed == 12),\n    linetype = \"dashed\",\n    alpha = 0.6\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal() +\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs_rand &lt;- glm_models_0_rand |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\ncontrol_rates_rand &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\")) |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\nRR_summary_rand &lt;- mod_0_coefs_rand |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates_rand,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary_rand |&gt; \n  ggplot(aes(y = RR, x = e.value, col = dist.th))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"E-value\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt; \n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  },
  {
    "objectID": "D3_Offences_joining.html#loading-data",
    "href": "D3_Offences_joining.html#loading-data",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "bog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\n# off_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\noff_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 396988 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.21482 ymin: 4.469726 xmax: -74.01338 ymax: 4.822895\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\ncent_tests_wkend &lt;- read_csv(\"sf_network/cent_tests_wkend.csv\",\n                             lazy = F)\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\nsummary_pairs_wkend &lt;- cent_tests_wkend |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n  #        .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\noff_sf_wkend &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekend\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\nminor_offences_wkend &lt;- off_sf_wkend[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\nminor_offences_wkend$near_index &lt;- st_nearest_feature(minor_offences_wkend,simpl_network_sf)\nminor_offences_wkend$pair_id &lt;- simpl_network_sf$pair_id[minor_offences_wkend$near_index]\n\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nmodel_data_wkend &lt;- (summary_pairs_wkend |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences_wkend$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\n## Congested\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n## Weekend\nmodel_data_wkend |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\ntest1_wkend &lt;- model_data_wkend |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\ntest1_wkend |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_wkend &lt;- model_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nmod_pred_0_wkend |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs &lt;- glm_models_0 |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\nmod_0_coefs_wkend &lt;- glm_models_0_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\n\ncontrol_rates &lt;- mod_0_coefs_wkend |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\noddsratio_to_riskratio(glm_models_0$model_rel[[1]])\n\nParameter   | Risk Ratio |       95% CI\n---------------------------------------\n(Intercept) |       0.02 |             \nlogdiff max |       0.46 | [0.27, 0.82]\n\nRR_summary &lt;-mod_0_coefs |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary |&gt; \n  ggplot(aes(x = RR, y = dist.th, col =wwd.speed))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nmodel_reports &lt;- model_data |&gt;\n  filter(offence_bool)\n\n\nrand_absences_data &lt;- bind_rows(\n  model_reports,\n  model_data |&gt;\n    filter(!offence_bool) |&gt;\n    sample_n(size = nrow(model_reports), replace = F)\n)\n\n\n# Same links are considered\nrand_absences_data_wkend &lt;- model_data_wkend |&gt; \n  semi_join(rand_absences_data,by = join_by(pair_id,wwd.speed,dist.th))\n\n\nglm_models_0_rand &lt;- rand_absences_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand &lt;- glm_models_0_rand |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nglm_models_0_rand_wkend &lt;- rand_absences_data_wkend |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ 1,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0_rand_wkend &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\n\nmod_pred_0_rand |&gt;\n  filter(wwd.speed == 12) |&gt;\n  ggplot(aes(\n    x = logdiff_max,\n    y = offence_bool,\n    group = id,\n    col = dist.th\n  )) +\n  geom_line(alpha = 0.3) +\n  geom_line(\n    data = mod_pred_0_rand_wkend |&gt;\n      filter(wwd.speed == 12),\n    linetype = \"dashed\",\n    alpha = 0.6\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal() +\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\nmod_0_coefs_rand &lt;- glm_models_0_rand |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\"))\n\ncontrol_rates_rand &lt;- glm_models_0_rand_wkend |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"logdiff_max\",replacement = \"slope\")) |&gt; \n  filter(term == \"intercept\") |&gt; \n  mutate(p0 = exp(estimate)) |&gt; \n  select(wwd.speed:dist.th,p0)\n\n\nRR_summary_rand &lt;- mod_0_coefs_rand |&gt; \n  filter(term == \"slope\") |&gt;\n  left_join(control_rates_rand,\n            by = join_by(wwd.speed, dist.th)\n            ) |&gt; \n  mutate(OR = exp(estimate),\n         RR = OR / (1 - p0 + (p0 * OR)),         #risk ratios: RR = OR / (1 - p + (p x OR))\n         e.value = case_when(\n           RR &gt;= 1 ~ RR + sqrt(RR * (RR - 1)),\n           RR &lt; 1 ~ RR ^ (-1) + sqrt(RR ^ (-1)*(RR^(-1)-1))))\n\n\nRR_summary_rand |&gt; \n  ggplot(aes(y = RR, x = e.value, col = dist.th))+\n  # geom_vline(xintercept = 0,linetype = \"dashed\",col= \"gray70\")+\n  geom_point()+\n  # coord_fixed()+\n  theme_minimal()+\n  labs(y=\"E-value\")\n\n\n\n\n\n\n\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank()\n        )\n\nList of 3\n $ axis.text.y       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor.y: list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt; \n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html",
    "href": "3B_Analysis_WWD_allowed.html",
    "title": "Results",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"tmap\",\n    \"apng\",\n    \"gganimate\",\n    \"kableExtra\",\n    \"lme4\",\n    \"equatiomatic\",\n    \"marginaleffects\"\n)\n\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n             sf       tidyverse            tmap            apng       gganimate \n           TRUE            TRUE            TRUE            TRUE            TRUE \n     kableExtra            lme4    equatiomatic marginaleffects \n           TRUE            TRUE            TRUE            TRUE \n\n\n\n\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt;\n  st_transform(3116)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\noff_sf_all &lt;- (st_read(\"sf_network/manualtickets_clean_sf.gpkg\") |&gt;\n                 st_transform(3116))[urban_perimeter,] |&gt; filter(year == 2019,day_type != \"friday\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 395211 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.21482 ymin: 4.469726 xmax: -74.01338 ymax: 4.822895\nGeodetic CRS:  WGS 84\n\n# off_sf_all &lt;- (st_read(\"sf_network/wwd_clean_sf.gpkg\") |&gt;\n#                  st_transform(3116))[urban_perimeter,] |&gt; filter(year == 2019,day_type != \"friday\")\n\n\n\n\n\nsf_net_exp &lt;- st_read(\"sf_network/full_sf_network.gpkg\") |&gt;\n  st_transform(3116)\n\nReading layer `full_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/full_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 242294 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  WGS 84\n\nsf_net &lt;- sf_net_exp[urban_perimeter,]\n\n\n\n\n\ncent_results &lt;- read_csv(\"sf_network/hourly_cent_results.csv\",\n                       lazy = F)\n\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(sf_net,sf_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsf_net$pair_id &lt;- simp_groups$pair_id\n\nWe will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_results |&gt; \n  right_join(sf_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,day_type,hour))\n\n\nsummary_pairs_dist.jct &lt;- \n  sf_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- sf_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nsimpl_subset_sf &lt;- simpl_network_sf |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    ))\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how the coordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- subset_net |&gt; \n  st_union()  |&gt; \n  st_buffer(20,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road.\n\noff_sf &lt;- off_sf_all[anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(major_net)+\n  tm_lines(\"gray60\",col_alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_fill(\"blue\",fill_alpha = 0.6)+\n  tm_shape(off_sf_all|&gt;\n                 filter(abs(hour - 9)&lt;=1,\n                        year == 2019,\n                        day_type == \"weekday\"))+\n  tm_dots(\"yellow\")+\n  tm_shape(minor_offences)+\n  tm_dots(\"red\")+\n  tm_layout(bg.color = \"grey10\")\n\n\n\n\n\n\n\n\n\n\nTotal length of road network\n\nsimpl_network_sf |&gt; mutate(distance = st_length(geom,)) |&gt; pull(distance) |&gt; sum()\n\n6589386 [m]\n\n\n\nsf_net |&gt; st_drop_geometry() |&gt;\n  filter(!str_detect(pattern = \"r\",way_id)) |&gt; \n  mutate(roadclass = str_remove(roadclass,\"_link\")) |&gt; \n  summarise(d_weighted = sum(d_weighted)/1e3,.by=c(roadclass)) |&gt; \n  mutate(d_weighted = round(d_weighted),\n         roadclass = factor(roadclass,\n                            levels = c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                            ordered = T)) |&gt; \n  arrange(roadclass) |&gt; \n  mutate(portion = round(d_weighted/sum(d_weighted)*100,1)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nd_weighted\nportion\n\n\n\n\ntrunk\n218\n2.0\n\n\nprimary\n622\n5.8\n\n\nsecondary\n532\n4.9\n\n\ntertiary\n1348\n12.5\n\n\nresidential\n8023\n74.3\n\n\nunclassified\n58\n0.5\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_subset_sf)\nminor_offences$pair_id &lt;- simpl_subset_sf$pair_id[minor_offences$near_index]\n\n\n\n\n\n\nmap_logdiff &lt;- lapply(\n  0:23,\n  \\(h){\n    simpl_network_sf |&gt;\n      left_join(summary_pairs,by = \"pair_id\") |&gt; \n      # filter(hour!=3) |&gt;\n      filter(day_type == \"weekday\", hour == h) |&gt; \n      mutate(logdiff_max = if_else(logdiff_max == 0,1,logdiff_max)) |&gt; \n      ggplot(aes(col =  logdiff_max, linewidth = abs(logdiff_max)))+\n      geom_sf()+\n      scale_color_steps2(mid = \"gray80\",high = \"dodgerblue2\",low = \"firebrick3\",\n                         breaks = c(-12,-8,-4,0,4,8,12),\n                         limits=c(-8,8)\n      )+\n      scale_linewidth_continuous(limits = c(0.00,12), range = c(0.01,20),trans = \"exp\")+\n      theme_void()+\n      labs(title = \"BC Changes\",\n           subtitle = paste0('Hour: ',h),\n           col = \"log \"\n      )+\n      guides(linewidth = \"none\")+\n      theme(legend.position = \"inside\",\n            legend.position.inside = c(0.1,0.8)\n            )\n    \n  }\n)\n\n\n\n\n\n\ntime_slots &lt;- summary_pairs |&gt; select(day_type,hour) |&gt; unique()\n\noffences_bool &lt;- plyr::join_all(\n  lapply(1:nrow(time_slots),\n       \\(j,\n         h_threshold = 1){\n         summary_pairs |&gt;\n           select(pair_id) |&gt; \n           unique() |&gt; \n           left_join(minor_offences |&gt;\n           st_drop_geometry() |&gt; \n           filter(day_type == time_slots$day_type[j],\n                  abs(hour - time_slots$hour[j])&lt;=h_threshold) |&gt; \n           count(pair_id),\n           by = join_by(pair_id)) |&gt; \n           rename_with(.fn =\\(x) {paste(time_slots$day_type[j],time_slots$hour[j],sep = \"_\")},.cols = \"n\")\n       }),by = \"pair_id\") |&gt; \n  mutate(across(-pair_id,\\(x){!is.na(x)})) |&gt;\n  pivot_longer(-pair_id,values_to = \"offence_bool\") |&gt;\n  separate_wider_delim(name, delim = \"_\",names = c(\"day_type\",\"hour\")) |&gt; \n  mutate(across(hour,as.numeric))\n\n\noffences_bool |&gt;  \n  filter(offence_bool) |&gt; \n  count(day_type,hour) |&gt; \n  ggplot(aes(hour,n,col = day_type,group = day_type))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmodel_data &lt;- (summary_pairs |&gt;\n  inner_join(simpl_subset_sf |&gt;\n               st_drop_geometry() |&gt;\n               select(pair_id,way_id,oneway),\n            by = \"pair_id\")) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id)) |&gt; \n  left_join(offences_bool,by = join_by(pair_id,hour,day_type))\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:offence_bool)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",day_type:hour,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = hour,linetype = day_type))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\ngrid_fill_p &lt;- expand_grid(day_type = c(\"weekday\",\"weekend\"),\n            hour = unique(minor_offences$hour)) |&gt;\n  arrange(day_type,hour)\n\n\nexp_probs &lt;-\n  do.call(bind_rows, lapply(\n    1:nrow(grid_fill_p),\n    # 4:5,\n    \\(i) {\n      ## Extracting the pair ids that have WWD reports at the day type-hour\n      t_pair_ids &lt;- minor_offences |&gt;\n        st_drop_geometry() |&gt;\n        filter(hour == grid_fill_p$hour[i], day_type == grid_fill_p$day_type[i]) |&gt;\n        pull(pair_id)\n      \n      \n      if (length(t_pair_ids) == 0) {\n        t_full_exp &lt;- data.frame(\n          from_id = NA_character_,\n          to_id = NA_character_,\n          p = NA_real_,\n          hour = grid_fill_p$hour[i],\n          day_type = grid_fill_p$day_type[i]\n        )\n      } else {\n        subset_net_offence &lt;- subset_net |&gt;\n          mutate(offence_bool = pair_id %in% t_pair_ids)\n        \n        \n        \n        \n        fill_probs &lt;- function(edges_df, direction = c(\"1\", \"-1\")) {\n          direction = match.arg(direction)\n          \n          if (direction == \"1\") {\n            do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n              subset_net_offence |&gt;\n                st_drop_geometry() |&gt;\n                select(from_id, to_id) |&gt;\n                filter(to_id == edges_df$from_id[j],\n                       from_id != edges_df$to_id[j]) |&gt;\n                mutate(p = edges_df$p[j] / n())\n            }))\n          } else {\n            do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n              subset_net_offence |&gt;\n                st_drop_geometry() |&gt;\n                select(from_id, to_id) |&gt;\n                filter(from_id == edges_df$to_id[j],\n                       to_id != edges_df$from_id[j]) |&gt;\n                mutate(p = edges_df$p[j] / n())\n            }))\n            \n          }\n        }\n        \n        \n        expand_reports &lt;- function(df, max_degree = 3) {\n          check0 &lt;- df |&gt;\n            st_drop_geometry() |&gt;\n            select(from_id, to_id) |&gt;\n            mutate(p = 1)\n          \n          check &lt;- list()\n          check[[1]] &lt;- fill_probs(check0)\n          for (i in 2:max_degree) {\n            if (nrow(check[[i - 1]]) &lt; 1)\n              break\n            check[[i]] &lt;- fill_probs(check[[i - 1]])\n          }\n          \n          checkr &lt;- list()\n          checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n          for (i in 2:max_degree) {\n            if (nrow(checkr[[i - 1]]) &lt; 1)\n              break\n            checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n          }\n          \n          ckeck_df &lt;- bind_rows(check0,\n                                do.call(bind_rows, check),\n                                do.call(bind_rows, checkr)) |&gt;\n            summarise(across(p, max), .by = c(from_id, to_id))\n          return(ckeck_df)\n        }\n        \n        t_full_exp &lt;- subset_net_offence |&gt;\n          filter(offence_bool) |&gt;\n          expand_reports() |&gt;\n          mutate(hour = grid_fill_p$hour[i], day_type = grid_fill_p$day_type[i])\n      }\n      \n      return(t_full_exp)\n    }\n  ))\n\n\nclean_probs &lt;- exp_probs |&gt;\n  drop_na(from_id, to_id) |&gt;\n  summarise(p = max(p),\n            .by = c(day_type, hour, from_id, to_id)) |&gt; \n  tibble()\n\n\nclean_probs_ids &lt;- subset_net |&gt;\n  st_drop_geometry() |&gt;\n  select(from_id, to_id,pair_id) |&gt; \n  inner_join(clean_probs,\n            by = join_by(from_id, to_id)) |&gt; \n  summarise(across(p,\\(x) max(x,na.rm = T)),\n            .by = c(day_type,hour,pair_id))\n\nA visual check\n\nsimpl_network_sf |&gt; \n  left_join(clean_probs_ids |&gt; \n              filter(day_type==\"weekday\",\n                     hour == 18),\n            by = \"pair_id\") |&gt; \n  tm_shape()+\n  tm_lines(\"p\")\n\n\n\n\n\n\n\n\n\nmodel_data_p &lt;- model_data |&gt; \n  left_join(clean_probs_ids,\n  by = join_by(pair_id,hour,day_type)) |&gt; \n  mutate(p = if_else(is.na(p),0,p))\n\n\n\n\nWe will use classification of Local Planning Units as analysis units. For that purpose, we load the gkpg file which has been extracted from the Reference map (available here).\n\ndir.create(\"raw_data\",showWarnings = F)\n\nif(!file.exists(file.path(\"raw_data\", \"UPL_Bogota.zip\"))) {\n  u &lt;-\n    \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/UPL_Bogota.zip\"\n  download.file(u, file.path(\"raw_data\", basename(u)), mode = \"wb\")\n  unzip(zipfile = file.path(\"raw_data\", basename(u)), exdir = \"raw_data\")\n}\n\nlpu_boundaries &lt;- st_read(\"raw_data/MR_VR0924_UPL.gpkg\")\n\nReading layer `mr_v0924_upl' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/MR_VR0924_UPL.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -74.44978 ymin: 3.73103 xmax: -73.98653 ymax: 4.836779\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nLPU have been classified according to official sources (see this). North and Northwest sectors have been merged. Only urban sectors are considered.\n\nlpu_sector &lt;- read_csv(\"raw_data/UPL_sectors.csv\",\n                       col_types = cols(\n                         CODIGO_UPL = col_character(),\n                         Sector = col_character()\n                         ))\n\nsector_boundaries &lt;- lpu_boundaries |&gt; \n  left_join(lpu_sector,\n            by = \"CODIGO_UPL\") |&gt; \n  mutate(Sector = str_remove(Sector,\"(?&lt;=North)west\")) |&gt; \n  group_by(Sector) |&gt; \n  summarise(geom = st_union(geom)) |&gt; \n  filter(Sector != \"Rural\") |&gt; \n  st_transform(st_crs(simpl_network_sf))\n\nThis shows\n\ntm_shape(sector_boundaries) +\n  tm_fill(\"Sector\",\n          fill.scale = tm_scale_categorical(values = \"brewer.set3\"),\n          fill_alpha = 0.7) +\n  tm_shape(major_net) +\n  tm_lines(\"gray60\",col_alpha = .6)\n\n\n\n\n\n\n\n\nSpatial Join\n\nsector_boundaries$n_reports &lt;- st_intersects(sector_boundaries,minor_offences) |&gt; vapply(length,numeric(1))\nsector_boundaries\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 983791.8 ymin: 985462.7 xmax: 1007261 ymax: 1026604\nProjected CRS: MAGNA-SIRGAS / Colombia Bogota zone\n# A tibble: 5 × 3\n  Sector                                                          geom n_reports\n* &lt;chr&gt;                                                  &lt;POLYGON [m]&gt;     &lt;dbl&gt;\n1 Extended Centre ((999176.2 999239, 999181.3 999235.8, 999309.9 9991…      9248\n2 North           ((1000188 1026603, 1000167 1026603, 1000145 1026601…      1771\n3 South           ((999531.2 992666, 999531.2 992666.6, 999531.5 9926…      1635\n4 Southwest       ((987874.4 1006112, 987821.7 1006129, 987792.1 1006…      2303\n5 West            ((997587.8 1013425, 997573.7 1013436, 997551.7 1013…      2508\n\n\n\ngrid_off &lt;- st_intersects(sector_boundaries,simpl_network_sf)\n\n\nminor_offences$sector &lt;- st_intersects(minor_offences,sector_boundaries) |&gt; vapply(\\(x) (x),numeric(1))\n\nsimpl_subset_sf$sector &lt;- \n  simpl_subset_sf |&gt;\n  st_centroid() |&gt;\n  st_intersects(sector_boundaries) |&gt;\n  vapply(\\(x) {\n    if (length(x) == 0) {\n      NA\n    } else{\n      x\n    }\n  }, numeric(1)) \n\npair_2_sector &lt;- simpl_subset_sf |&gt; \n  st_drop_geometry() |&gt; \n  select(pair_id,sector)\n\n\nsummary_sector_off &lt;- minor_offences |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n = n(),.by = c(day_type,hour,sector)) |&gt; \n  arrange(day_type,hour)\n\n\nggplot(summary_sector_off,\n       aes(x = hour, y = n,col = factor(sector)))+\n  geom_line()+\n  facet_grid(day_type~.)+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmodel_data_p_sec &lt;- model_data_p |&gt; \n  left_join(pair_2_sector, by = join_by(pair_id)) |&gt; \n  drop_na(sector)\n\n\n\n\nUnder the assumption that the transport police have the same number of officers allocated along the day. We can assume that the sample size (the number of monitored roads) is the same.\nIn this case we will assume that 10% of the roads are constantly monitored. Also, it is assumed that the allocation of officers is proportional to the size of the network.\n\nsample_size &lt;-  0.1\n\nCalculating the actual sample size for each sector.\n\nsample_size_sector &lt;- model_data_p_sec |&gt;\n  count(day_type,hour,sector,\n        name = \"n_links\") |&gt; \n  select(sector,n_links) |&gt; \n  unique() |&gt; \n  mutate(sample_size = round(sample_size*n_links,-1),.keep = \"none\",.by = sector)\nsample_size_sector |&gt; kable()\n\n\n\n\nsector\nsample_size\n\n\n\n\n1\n1380\n\n\n2\n1130\n\n\n3\n1710\n\n\n4\n1420\n\n\n5\n770\n\n\n\n\n\nBased on these sample sizes, we will randomly sample different roads within the sector as pseudo-absences. Pseudo-absences are used in ecology when working with presence-only data and no actual absences are recorded, for example, recording presence of exotic species.\n\nset.seed(1234)\nmodel_data_sampled &lt;- model_data_p_sec |&gt;  \n  nest(.by = c(day_type,hour,sector)) |&gt; \n  left_join(sample_size_sector,by=\"sector\") |&gt;\n  mutate(\n    n_true = map_dbl(data,\\(.x) {\n      .x |&gt; filter(offence_bool) |&gt; nrow()\n      }),\n    obs = map(data,\\(.x) {\n      .x |&gt; filter(offence_bool) \n      }),\n  pseudoabs = map2(.x = data,\n                       .y = sample_size-n_true,\n                       .f = \\(.x,.y) {\n                         .x |&gt;\n                           filter(p==0) |&gt;\n                           slice_sample(n = .y)\n                         }\n                       ),\n  model_data = map2(.x = obs,\n                    .y = pseudoabs,\n                    .f = \\(.x,.y){\n                      bind_rows(.x,.y)\n                    })\n         ) |&gt; \n  select(day_type:sector,n_true,sample_size,model_data) |&gt; \n  unnest(model_data)\n\n\nmodel_data_sampled_mlm &lt;- model_data_sampled |&gt;\n  filter(day_type == \"weekday\",between(hour,5,20)) |&gt; \n  mutate(logdiff_max_gmc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour)) |&gt; \n  mutate(logdiff_max_smc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour,sector))\n\n\n# saveRDS(model_data_sampled_mlm,\"model_data.RDS\")\n# model_data_sampled_mlm &lt;- readRDS(\"model_data.RDS\") |&gt; data.frame() |&gt; filter()\n\n\n\n\n\nm0a &lt;- glmer(offence_bool ~ (1|sector),\n            data = model_data_sampled_mlm,family = \"binomial\")\nsummary(m0a)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ (1 | sector)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 91466.8  91485.9 -45731.4  91462.8   102558 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.8298 -0.4163 -0.3947 -0.3105  3.2206 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n sector (Intercept) 0.4312   0.6566  \nNumber of obs: 102560, groups:  sector, 5\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5556     0.1717   -9.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm0b &lt;- glmer(offence_bool ~ (1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\nsummary(m0b)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 97935.0  97954.1 -48965.5  97931.0   102558 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.6249 -0.5429 -0.4448 -0.3477  3.1251 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n hour   (Intercept) 0.1464   0.3827  \nNumber of obs: 102560, groups:  hour, 16\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.49099    0.09468  -15.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThe chances of having a offence report is explained by between-sector differences\n\nicca &lt;- m0a@theta[1]^2/ (m0a@theta[1]^2 + (pi^2/3))\nicca\n\n[1] 0.1158773\n\n\nThe chances of having a offence report is explained by between-hour differences\n\niccb &lt;- m0b@theta[1]^2/ (m0b@theta[1]^2 + (pi^2/3))\niccb\n\n[1] 0.04261601\n\n\n\n\n\n\nCIM &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(CIM)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 89058.4  89096.5 -44525.2  89050.4   102556 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.2475 -0.4725 -0.3549 -0.2470  5.6198 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n hour   (Intercept) 0.1873   0.4328  \n sector (Intercept) 0.4401   0.6634  \nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.613180   0.323466  -4.987 6.13e-07 ***\nlogdiff_max_gmc  0.033151   0.002056  16.127  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.002\n\n\n\nAIMa &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc|sector)+(1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMa)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc | sector) +  \n    (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 88918.5  88975.8 -44453.3  88906.5   102554 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1964 -0.4698 -0.3560 -0.2434  6.0832 \n\nRandom effects:\n Groups Name            Variance  Std.Dev. Corr \n hour   (Intercept)     0.1870607 0.43251       \n sector (Intercept)     0.4448695 0.66699       \n        logdiff_max_gmc 0.0007079 0.02661  -0.42\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.60968    0.31378   -5.13  2.9e-07 ***\nlogdiff_max_gmc  0.04181    0.01208    3.46 0.000539 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.387\n\n\n\nAIMb &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1 + logdiff_max_gmc||hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMb)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \noffence_bool ~ logdiff_max_gmc + (1 | sector) + (1 + logdiff_max_gmc ||  \n    hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 89059.6  89107.3 -44524.8  89049.6   102555 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.2447 -0.4725 -0.3548 -0.2471  5.5924 \n\nRandom effects:\n Groups Name            Variance  Std.Dev.\n hour   logdiff_max_gmc 2.221e-05 0.004713\n hour.1 (Intercept)     1.875e-01 0.433067\n sector (Intercept)     4.400e-01 0.663352\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.613429   0.308150  -5.236 1.64e-07 ***\nlogdiff_max_gmc  0.033243   0.002382  13.956  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.002\n\n\n\nAIMc &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc||sector)+(1 + logdiff_max_gmc||hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMc)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc || sector) +  \n    (1 + logdiff_max_gmc || hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 88918.0  88975.3 -44453.0  88906.0   102554 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1908 -0.4691 -0.3565 -0.2423  6.0439 \n\nRandom effects:\n Groups   Name            Variance  Std.Dev.\n hour     logdiff_max_gmc 3.398e-05 0.005829\n hour.1   (Intercept)     1.879e-01 0.433422\n sector   logdiff_max_gmc 7.124e-04 0.026690\n sector.1 (Intercept)     4.450e-01 0.667115\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.61010    0.31074  -5.182  2.2e-07 ***\nlogdiff_max_gmc  0.04197    0.01223   3.431 0.000601 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm 0.000 \n\n\n\nanova(CIM,AIMa,AIMb,AIMc)\n\nData: model_data_sampled_mlm\nModels:\nCIM: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 | hour)\nAIMb: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 + logdiff_max_gmc || hour)\nAIMa: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc | sector) + (1 | hour)\nAIMc: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc || sector) + (1 + logdiff_max_gmc || hour)\n     npar   AIC   BIC logLik deviance    Chisq Df Pr(&gt;Chisq)    \nCIM     4 89058 89097 -44525    89050                           \nAIMb    5 89060 89107 -44525    89050   0.7269  1     0.3939    \nAIMa    6 88919 88976 -44453    88907 143.0934  1     &lt;2e-16 ***\nAIMc    6 88918 88975 -44453    88906   0.4945  0               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nextract_eq(AIMa)\n\n\\[\n\\begin{aligned}\n  \\operatorname{offence\\_bool}_{i}  &\\sim \\operatorname{Binomial}(n = 1, \\operatorname{prob}_{\\operatorname{offence\\_bool} = 1} = \\widehat{P}) \\\\\n    \\log\\left[\\frac{\\hat{P}}{1 - \\hat{P}} \\right] &=\\alpha_{j[i],k[i]} + \\beta_{1k[i]}(\\operatorname{\\logdiff\\_max\\_gmc}) \\\\    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for hour j = 1,} \\dots \\text{,J} \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{k} \\\\\n      &\\beta_{1k}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\mu_{\\alpha_{k}} \\\\\n      &\\mu_{\\beta_{1k}}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{k}} & \\rho_{\\alpha_{k}\\beta_{1k}} \\\\\n     \\rho_{\\beta_{1k}\\alpha_{k}} & \\sigma^2_{\\beta_{1k}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for sector k = 1,} \\dots \\text{,K}\n\\end{aligned}\n\\]\n\n\n$$\n\\[\\begin{aligned}\n  \\operatorname{offence\\_bool}_{i}  &\\sim \\operatorname{Binomial}(n = 1, \\operatorname{prob}_{\\operatorname{offence\\_bool} = 1} = \\widehat{P}) \\\\\n    \\log\\left[\\frac{\\hat{P}}{1 - \\hat{P}} \\right] &=\\alpha_{j[i],k[i]} + \\beta_{1}(\\operatorname{diff\\_max\\_gmc}) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for hour j = 1,} \\dots \\text{,J} \\\\\n    \\alpha_{k}  &\\sim N \\left(\\mu_{\\alpha_{k}}, \\sigma^2_{\\alpha_{k}} \\right)\n    \\text{, for sector k = 1,} \\dots \\text{,K}\n\\end{aligned}\\]\n$$ CIM\n\nplot_slopes(CIM,\n            variables = \"logdiff_max_gmc\",\n            by = c(\"hour\",\"sector\"))+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  scale_fill_brewer(type = \"qual\",\n                      palette = \"Set3\")+\n  theme_minimal()+\n  scale_y_continuous(limits = c(0,0.01))+\n  labs(col = \"Sector\",\n       fill = \"Sector\")\n\n\n\n\n\n\n\n\nAIMa\n\nplot_slopes(AIMa,\n            variables = \"logdiff_max_gmc\",\n            by = c(\"hour\",\"sector\"))+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  scale_fill_brewer(type = \"qual\",\n                      palette = \"Set3\")+\n  theme_minimal()+\n  scale_y_continuous(limits = c(0,0.01))+\n  labs(col = \"Sector\",\n       fill = \"Sector\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn_d10_logdiff_max &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt;  \n  mutate(d10_bool = logdiff_max &gt;= quantile(logdiff_max, 0.90,na.rm = T),\n         .by=c(hour,day_type,roadclass)) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n_d10 = sum(d10_bool,na.rm = T),.by = c(pair_id,day_type)) |&gt; \n  pivot_wider(names_from = day_type,values_from = n_d10)\n\n\nmap_ratruns_wk &lt;- simpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      }))) |&gt;\n  ggplot(aes(col =  weekday,\n             linewidth = weekday_ld)) +\n  geom_sf() +\n  scale_colour_viridis_b(na.value = \"gray30\",\n                         option = \"plasma\",\n                         direction = -1) +\n  scale_linewidth_continuous(\n    limits = c(0, 24),\n    range = c(0.05, 0.5),\n    transform = \"exp\"\n  ) +\n  theme_void() +\n  guides(linewidth = \"none\") +\n  labs(col = \"Rat-run potential\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.7))\n\nmap_ratruns_wk\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      })))\n\nSimple feature collection with 91970 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 979293.3 ymin: 985597.3 xmax: 1007251 ymax: 1025644\nProjected CRS: MAGNA-SIRGAS / Colombia Bogota zone\nFirst 10 features:\n   lanes     way_id    roadclass oneway lanes.1 surface maxspeed       time\n1     NA   89742442  residential  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  36.379850\n2      1   86192048   trunk_link   TRUE       1 asphalt     &lt;NA&gt;   6.131124\n3      2   90894261     tertiary   TRUE       2 asphalt     &lt;NA&gt;  28.521182\n4      1   24798427 primary_link   TRUE       1 asphalt     &lt;NA&gt;   6.488302\n5     NA  117826480     tertiary  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  12.262932\n6      2   24075320   trunk_link   TRUE       2 asphalt     &lt;NA&gt;  19.056994\n7     NA  689403428  residential  FALSE    &lt;NA&gt; unpaved     &lt;NA&gt; 122.464815\n8      2   31509948  residential  FALSE       2 asphalt     &lt;NA&gt;   6.420331\n9     NA  523839186  residential  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  11.820349\n10    NA 550938368r  residential   TRUE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  29.528495\n   time_weighted component pair_id weekday weekend\n1      36.379850         1       1       1       0\n2       6.131124         1       2      NA      NA\n3      28.521182         1       3      NA      NA\n4       6.488302         1       4      NA      NA\n5      12.262932         1       5      NA      NA\n6      19.056994         1       6      NA      NA\n7     122.464815         1       7       1       0\n8       6.420331         1       8       4       4\n9      11.820349         1       9       9      11\n10     29.528495         1      10       1       0\n                             geom weekday_ld weekend_ld\n1  LINESTRING (995528.4 995623...          1          0\n2  LINESTRING (996495.7 101233...          1          1\n3  LINESTRING (998739.2 101067...          1          1\n4  LINESTRING (997806.9 100455...          1          1\n5  LINESTRING (998659.5 996788...          1          1\n6  LINESTRING (1002287 1009965...          1          1\n7  LINESTRING (988912.4 100125...          1          0\n8  LINESTRING (990692.7 100298...          4          4\n9  LINESTRING (996803.1 997474...          9         11\n10 LINESTRING (986133.3 100201...          1          0"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html#loading-data",
    "href": "3B_Analysis_WWD_allowed.html#loading-data",
    "title": "Results",
    "section": "",
    "text": "urban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt;\n  st_transform(3116)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\noff_sf_all &lt;- (st_read(\"sf_network/manualtickets_clean_sf.gpkg\") |&gt;\n                 st_transform(3116))[urban_perimeter,] |&gt; filter(year == 2019,day_type != \"friday\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 395211 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.21482 ymin: 4.469726 xmax: -74.01338 ymax: 4.822895\nGeodetic CRS:  WGS 84\n\n# off_sf_all &lt;- (st_read(\"sf_network/wwd_clean_sf.gpkg\") |&gt;\n#                  st_transform(3116))[urban_perimeter,] |&gt; filter(year == 2019,day_type != \"friday\")\n\n\n\n\n\nsf_net_exp &lt;- st_read(\"sf_network/full_sf_network.gpkg\") |&gt;\n  st_transform(3116)\n\nReading layer `full_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/full_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 242294 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  WGS 84\n\nsf_net &lt;- sf_net_exp[urban_perimeter,]\n\n\n\n\n\ncent_results &lt;- read_csv(\"sf_network/hourly_cent_results.csv\",\n                       lazy = F)"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html#assigning-reports-to-the-network",
    "href": "3B_Analysis_WWD_allowed.html#assigning-reports-to-the-network",
    "title": "Results",
    "section": "",
    "text": "We need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(sf_net,sf_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsf_net$pair_id &lt;- simp_groups$pair_id\n\nWe will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_results |&gt; \n  right_join(sf_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:logreldiff.ff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,day_type,hour))\n\n\nsummary_pairs_dist.jct &lt;- \n  sf_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- sf_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nsimpl_subset_sf &lt;- simpl_network_sf |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    ))\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how the coordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- subset_net |&gt; \n  st_union()  |&gt; \n  st_buffer(20,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road.\n\noff_sf &lt;- off_sf_all[anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(major_net)+\n  tm_lines(\"gray60\",col_alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_fill(\"blue\",fill_alpha = 0.6)+\n  tm_shape(off_sf_all|&gt;\n                 filter(abs(hour - 9)&lt;=1,\n                        year == 2019,\n                        day_type == \"weekday\"))+\n  tm_dots(\"yellow\")+\n  tm_shape(minor_offences)+\n  tm_dots(\"red\")+\n  tm_layout(bg.color = \"grey10\")\n\n\n\n\n\n\n\n\n\n\nTotal length of road network\n\nsimpl_network_sf |&gt; mutate(distance = st_length(geom,)) |&gt; pull(distance) |&gt; sum()\n\n6589386 [m]\n\n\n\nsf_net |&gt; st_drop_geometry() |&gt;\n  filter(!str_detect(pattern = \"r\",way_id)) |&gt; \n  mutate(roadclass = str_remove(roadclass,\"_link\")) |&gt; \n  summarise(d_weighted = sum(d_weighted)/1e3,.by=c(roadclass)) |&gt; \n  mutate(d_weighted = round(d_weighted),\n         roadclass = factor(roadclass,\n                            levels = c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                            ordered = T)) |&gt; \n  arrange(roadclass) |&gt; \n  mutate(portion = round(d_weighted/sum(d_weighted)*100,1)) |&gt; \n  kableExtra::kable()\n\n\n\n\nroadclass\nd_weighted\nportion\n\n\n\n\ntrunk\n218\n2.0\n\n\nprimary\n622\n5.8\n\n\nsecondary\n532\n4.9\n\n\ntertiary\n1348\n12.5\n\n\nresidential\n8023\n74.3\n\n\nunclassified\n58\n0.5\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_subset_sf)\nminor_offences$pair_id &lt;- simpl_subset_sf$pair_id[minor_offences$near_index]"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html#exploring-bc-changes-in-the-network-with-wwd-allowed",
    "href": "3B_Analysis_WWD_allowed.html#exploring-bc-changes-in-the-network-with-wwd-allowed",
    "title": "Results",
    "section": "",
    "text": "map_logdiff &lt;- lapply(\n  0:23,\n  \\(h){\n    simpl_network_sf |&gt;\n      left_join(summary_pairs,by = \"pair_id\") |&gt; \n      # filter(hour!=3) |&gt;\n      filter(day_type == \"weekday\", hour == h) |&gt; \n      mutate(logdiff_max = if_else(logdiff_max == 0,1,logdiff_max)) |&gt; \n      ggplot(aes(col =  logdiff_max, linewidth = abs(logdiff_max)))+\n      geom_sf()+\n      scale_color_steps2(mid = \"gray80\",high = \"dodgerblue2\",low = \"firebrick3\",\n                         breaks = c(-12,-8,-4,0,4,8,12),\n                         limits=c(-8,8)\n      )+\n      scale_linewidth_continuous(limits = c(0.00,12), range = c(0.01,20),trans = \"exp\")+\n      theme_void()+\n      labs(title = \"BC Changes\",\n           subtitle = paste0('Hour: ',h),\n           col = \"log \"\n      )+\n      guides(linewidth = \"none\")+\n      theme(legend.position = \"inside\",\n            legend.position.inside = c(0.1,0.8)\n            )\n    \n  }\n)"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html#modelling",
    "href": "3B_Analysis_WWD_allowed.html#modelling",
    "title": "Results",
    "section": "",
    "text": "time_slots &lt;- summary_pairs |&gt; select(day_type,hour) |&gt; unique()\n\noffences_bool &lt;- plyr::join_all(\n  lapply(1:nrow(time_slots),\n       \\(j,\n         h_threshold = 1){\n         summary_pairs |&gt;\n           select(pair_id) |&gt; \n           unique() |&gt; \n           left_join(minor_offences |&gt;\n           st_drop_geometry() |&gt; \n           filter(day_type == time_slots$day_type[j],\n                  abs(hour - time_slots$hour[j])&lt;=h_threshold) |&gt; \n           count(pair_id),\n           by = join_by(pair_id)) |&gt; \n           rename_with(.fn =\\(x) {paste(time_slots$day_type[j],time_slots$hour[j],sep = \"_\")},.cols = \"n\")\n       }),by = \"pair_id\") |&gt; \n  mutate(across(-pair_id,\\(x){!is.na(x)})) |&gt;\n  pivot_longer(-pair_id,values_to = \"offence_bool\") |&gt;\n  separate_wider_delim(name, delim = \"_\",names = c(\"day_type\",\"hour\")) |&gt; \n  mutate(across(hour,as.numeric))\n\n\noffences_bool |&gt;  \n  filter(offence_bool) |&gt; \n  count(day_type,hour) |&gt; \n  ggplot(aes(hour,n,col = day_type,group = day_type))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmodel_data &lt;- (summary_pairs |&gt;\n  inner_join(simpl_subset_sf |&gt;\n               st_drop_geometry() |&gt;\n               select(pair_id,way_id,oneway),\n            by = \"pair_id\")) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id)) |&gt; \n  left_join(offences_bool,by = join_by(pair_id,hour,day_type))\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:offence_bool)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ logdiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(logdiff_max = .y$logdiff_max,\n           offence_bool = predict(.x,\n                                  newdata = .y,\n                                  type = \"response\"))\n    })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",day_type:hour,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = logdiff_max,\n             y = offence_bool,\n             group = id,\n             col = hour,linetype = day_type))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\ngrid_fill_p &lt;- expand_grid(day_type = c(\"weekday\",\"weekend\"),\n            hour = unique(minor_offences$hour)) |&gt;\n  arrange(day_type,hour)\n\n\nexp_probs &lt;-\n  do.call(bind_rows, lapply(\n    1:nrow(grid_fill_p),\n    # 4:5,\n    \\(i) {\n      ## Extracting the pair ids that have WWD reports at the day type-hour\n      t_pair_ids &lt;- minor_offences |&gt;\n        st_drop_geometry() |&gt;\n        filter(hour == grid_fill_p$hour[i], day_type == grid_fill_p$day_type[i]) |&gt;\n        pull(pair_id)\n      \n      \n      if (length(t_pair_ids) == 0) {\n        t_full_exp &lt;- data.frame(\n          from_id = NA_character_,\n          to_id = NA_character_,\n          p = NA_real_,\n          hour = grid_fill_p$hour[i],\n          day_type = grid_fill_p$day_type[i]\n        )\n      } else {\n        subset_net_offence &lt;- subset_net |&gt;\n          mutate(offence_bool = pair_id %in% t_pair_ids)\n        \n        \n        \n        \n        fill_probs &lt;- function(edges_df, direction = c(\"1\", \"-1\")) {\n          direction = match.arg(direction)\n          \n          if (direction == \"1\") {\n            do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n              subset_net_offence |&gt;\n                st_drop_geometry() |&gt;\n                select(from_id, to_id) |&gt;\n                filter(to_id == edges_df$from_id[j],\n                       from_id != edges_df$to_id[j]) |&gt;\n                mutate(p = edges_df$p[j] / n())\n            }))\n          } else {\n            do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n              subset_net_offence |&gt;\n                st_drop_geometry() |&gt;\n                select(from_id, to_id) |&gt;\n                filter(from_id == edges_df$to_id[j],\n                       to_id != edges_df$from_id[j]) |&gt;\n                mutate(p = edges_df$p[j] / n())\n            }))\n            \n          }\n        }\n        \n        \n        expand_reports &lt;- function(df, max_degree = 3) {\n          check0 &lt;- df |&gt;\n            st_drop_geometry() |&gt;\n            select(from_id, to_id) |&gt;\n            mutate(p = 1)\n          \n          check &lt;- list()\n          check[[1]] &lt;- fill_probs(check0)\n          for (i in 2:max_degree) {\n            if (nrow(check[[i - 1]]) &lt; 1)\n              break\n            check[[i]] &lt;- fill_probs(check[[i - 1]])\n          }\n          \n          checkr &lt;- list()\n          checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n          for (i in 2:max_degree) {\n            if (nrow(checkr[[i - 1]]) &lt; 1)\n              break\n            checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n          }\n          \n          ckeck_df &lt;- bind_rows(check0,\n                                do.call(bind_rows, check),\n                                do.call(bind_rows, checkr)) |&gt;\n            summarise(across(p, max), .by = c(from_id, to_id))\n          return(ckeck_df)\n        }\n        \n        t_full_exp &lt;- subset_net_offence |&gt;\n          filter(offence_bool) |&gt;\n          expand_reports() |&gt;\n          mutate(hour = grid_fill_p$hour[i], day_type = grid_fill_p$day_type[i])\n      }\n      \n      return(t_full_exp)\n    }\n  ))\n\n\nclean_probs &lt;- exp_probs |&gt;\n  drop_na(from_id, to_id) |&gt;\n  summarise(p = max(p),\n            .by = c(day_type, hour, from_id, to_id)) |&gt; \n  tibble()\n\n\nclean_probs_ids &lt;- subset_net |&gt;\n  st_drop_geometry() |&gt;\n  select(from_id, to_id,pair_id) |&gt; \n  inner_join(clean_probs,\n            by = join_by(from_id, to_id)) |&gt; \n  summarise(across(p,\\(x) max(x,na.rm = T)),\n            .by = c(day_type,hour,pair_id))\n\nA visual check\n\nsimpl_network_sf |&gt; \n  left_join(clean_probs_ids |&gt; \n              filter(day_type==\"weekday\",\n                     hour == 18),\n            by = \"pair_id\") |&gt; \n  tm_shape()+\n  tm_lines(\"p\")\n\n\n\n\n\n\n\n\n\nmodel_data_p &lt;- model_data |&gt; \n  left_join(clean_probs_ids,\n  by = join_by(pair_id,hour,day_type)) |&gt; \n  mutate(p = if_else(is.na(p),0,p))\n\n\n\n\nWe will use classification of Local Planning Units as analysis units. For that purpose, we load the gkpg file which has been extracted from the Reference map (available here).\n\ndir.create(\"raw_data\",showWarnings = F)\n\nif(!file.exists(file.path(\"raw_data\", \"UPL_Bogota.zip\"))) {\n  u &lt;-\n    \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/UPL_Bogota.zip\"\n  download.file(u, file.path(\"raw_data\", basename(u)), mode = \"wb\")\n  unzip(zipfile = file.path(\"raw_data\", basename(u)), exdir = \"raw_data\")\n}\n\nlpu_boundaries &lt;- st_read(\"raw_data/MR_VR0924_UPL.gpkg\")\n\nReading layer `mr_v0924_upl' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/MR_VR0924_UPL.gpkg' \n  using driver `GPKG'\nSimple feature collection with 33 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -74.44978 ymin: 3.73103 xmax: -73.98653 ymax: 4.836779\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nLPU have been classified according to official sources (see this). North and Northwest sectors have been merged. Only urban sectors are considered.\n\nlpu_sector &lt;- read_csv(\"raw_data/UPL_sectors.csv\",\n                       col_types = cols(\n                         CODIGO_UPL = col_character(),\n                         Sector = col_character()\n                         ))\n\nsector_boundaries &lt;- lpu_boundaries |&gt; \n  left_join(lpu_sector,\n            by = \"CODIGO_UPL\") |&gt; \n  mutate(Sector = str_remove(Sector,\"(?&lt;=North)west\")) |&gt; \n  group_by(Sector) |&gt; \n  summarise(geom = st_union(geom)) |&gt; \n  filter(Sector != \"Rural\") |&gt; \n  st_transform(st_crs(simpl_network_sf))\n\nThis shows\n\ntm_shape(sector_boundaries) +\n  tm_fill(\"Sector\",\n          fill.scale = tm_scale_categorical(values = \"brewer.set3\"),\n          fill_alpha = 0.7) +\n  tm_shape(major_net) +\n  tm_lines(\"gray60\",col_alpha = .6)\n\n\n\n\n\n\n\n\nSpatial Join\n\nsector_boundaries$n_reports &lt;- st_intersects(sector_boundaries,minor_offences) |&gt; vapply(length,numeric(1))\nsector_boundaries\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 983791.8 ymin: 985462.7 xmax: 1007261 ymax: 1026604\nProjected CRS: MAGNA-SIRGAS / Colombia Bogota zone\n# A tibble: 5 × 3\n  Sector                                                          geom n_reports\n* &lt;chr&gt;                                                  &lt;POLYGON [m]&gt;     &lt;dbl&gt;\n1 Extended Centre ((999176.2 999239, 999181.3 999235.8, 999309.9 9991…      9248\n2 North           ((1000188 1026603, 1000167 1026603, 1000145 1026601…      1771\n3 South           ((999531.2 992666, 999531.2 992666.6, 999531.5 9926…      1635\n4 Southwest       ((987874.4 1006112, 987821.7 1006129, 987792.1 1006…      2303\n5 West            ((997587.8 1013425, 997573.7 1013436, 997551.7 1013…      2508\n\n\n\ngrid_off &lt;- st_intersects(sector_boundaries,simpl_network_sf)\n\n\nminor_offences$sector &lt;- st_intersects(minor_offences,sector_boundaries) |&gt; vapply(\\(x) (x),numeric(1))\n\nsimpl_subset_sf$sector &lt;- \n  simpl_subset_sf |&gt;\n  st_centroid() |&gt;\n  st_intersects(sector_boundaries) |&gt;\n  vapply(\\(x) {\n    if (length(x) == 0) {\n      NA\n    } else{\n      x\n    }\n  }, numeric(1)) \n\npair_2_sector &lt;- simpl_subset_sf |&gt; \n  st_drop_geometry() |&gt; \n  select(pair_id,sector)\n\n\nsummary_sector_off &lt;- minor_offences |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n = n(),.by = c(day_type,hour,sector)) |&gt; \n  arrange(day_type,hour)\n\n\nggplot(summary_sector_off,\n       aes(x = hour, y = n,col = factor(sector)))+\n  geom_line()+\n  facet_grid(day_type~.)+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nmodel_data_p_sec &lt;- model_data_p |&gt; \n  left_join(pair_2_sector, by = join_by(pair_id)) |&gt; \n  drop_na(sector)\n\n\n\n\nUnder the assumption that the transport police have the same number of officers allocated along the day. We can assume that the sample size (the number of monitored roads) is the same.\nIn this case we will assume that 10% of the roads are constantly monitored. Also, it is assumed that the allocation of officers is proportional to the size of the network.\n\nsample_size &lt;-  0.1\n\nCalculating the actual sample size for each sector.\n\nsample_size_sector &lt;- model_data_p_sec |&gt;\n  count(day_type,hour,sector,\n        name = \"n_links\") |&gt; \n  select(sector,n_links) |&gt; \n  unique() |&gt; \n  mutate(sample_size = round(sample_size*n_links,-1),.keep = \"none\",.by = sector)\nsample_size_sector |&gt; kable()\n\n\n\n\nsector\nsample_size\n\n\n\n\n1\n1380\n\n\n2\n1130\n\n\n3\n1710\n\n\n4\n1420\n\n\n5\n770\n\n\n\n\n\nBased on these sample sizes, we will randomly sample different roads within the sector as pseudo-absences. Pseudo-absences are used in ecology when working with presence-only data and no actual absences are recorded, for example, recording presence of exotic species.\n\nset.seed(1234)\nmodel_data_sampled &lt;- model_data_p_sec |&gt;  \n  nest(.by = c(day_type,hour,sector)) |&gt; \n  left_join(sample_size_sector,by=\"sector\") |&gt;\n  mutate(\n    n_true = map_dbl(data,\\(.x) {\n      .x |&gt; filter(offence_bool) |&gt; nrow()\n      }),\n    obs = map(data,\\(.x) {\n      .x |&gt; filter(offence_bool) \n      }),\n  pseudoabs = map2(.x = data,\n                       .y = sample_size-n_true,\n                       .f = \\(.x,.y) {\n                         .x |&gt;\n                           filter(p==0) |&gt;\n                           slice_sample(n = .y)\n                         }\n                       ),\n  model_data = map2(.x = obs,\n                    .y = pseudoabs,\n                    .f = \\(.x,.y){\n                      bind_rows(.x,.y)\n                    })\n         ) |&gt; \n  select(day_type:sector,n_true,sample_size,model_data) |&gt; \n  unnest(model_data)\n\n\nmodel_data_sampled_mlm &lt;- model_data_sampled |&gt;\n  filter(day_type == \"weekday\",between(hour,5,20)) |&gt; \n  mutate(logdiff_max_gmc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour)) |&gt; \n  mutate(logdiff_max_smc = logdiff_max - mean(logdiff_max,na.rm = T),.by = c(hour,sector))\n\n\n# saveRDS(model_data_sampled_mlm,\"model_data.RDS\")\n# model_data_sampled_mlm &lt;- readRDS(\"model_data.RDS\") |&gt; data.frame() |&gt; filter()\n\n\n\n\n\nm0a &lt;- glmer(offence_bool ~ (1|sector),\n            data = model_data_sampled_mlm,family = \"binomial\")\nsummary(m0a)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ (1 | sector)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 91466.8  91485.9 -45731.4  91462.8   102558 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.8298 -0.4163 -0.3947 -0.3105  3.2206 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n sector (Intercept) 0.4312   0.6566  \nNumber of obs: 102560, groups:  sector, 5\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5556     0.1717   -9.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm0b &lt;- glmer(offence_bool ~ (1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\nsummary(m0b)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 97935.0  97954.1 -48965.5  97931.0   102558 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.6249 -0.5429 -0.4448 -0.3477  3.1251 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n hour   (Intercept) 0.1464   0.3827  \nNumber of obs: 102560, groups:  hour, 16\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.49099    0.09468  -15.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThe chances of having a offence report is explained by between-sector differences\n\nicca &lt;- m0a@theta[1]^2/ (m0a@theta[1]^2 + (pi^2/3))\nicca\n\n[1] 0.1158773\n\n\nThe chances of having a offence report is explained by between-hour differences\n\niccb &lt;- m0b@theta[1]^2/ (m0b@theta[1]^2 + (pi^2/3))\niccb\n\n[1] 0.04261601\n\n\n\n\n\n\nCIM &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(CIM)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 89058.4  89096.5 -44525.2  89050.4   102556 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.2475 -0.4725 -0.3549 -0.2470  5.6198 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n hour   (Intercept) 0.1873   0.4328  \n sector (Intercept) 0.4401   0.6634  \nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.613180   0.323466  -4.987 6.13e-07 ***\nlogdiff_max_gmc  0.033151   0.002056  16.127  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.002\n\n\n\nAIMa &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc|sector)+(1|hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMa)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc | sector) +  \n    (1 | hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 88918.5  88975.8 -44453.3  88906.5   102554 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1964 -0.4698 -0.3560 -0.2434  6.0832 \n\nRandom effects:\n Groups Name            Variance  Std.Dev. Corr \n hour   (Intercept)     0.1870607 0.43251       \n sector (Intercept)     0.4448695 0.66699       \n        logdiff_max_gmc 0.0007079 0.02661  -0.42\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.60968    0.31378   -5.13  2.9e-07 ***\nlogdiff_max_gmc  0.04181    0.01208    3.46 0.000539 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.387\n\n\n\nAIMb &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1|sector)+(1 + logdiff_max_gmc||hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMb)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \noffence_bool ~ logdiff_max_gmc + (1 | sector) + (1 + logdiff_max_gmc ||  \n    hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 89059.6  89107.3 -44524.8  89049.6   102555 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.2447 -0.4725 -0.3548 -0.2471  5.5924 \n\nRandom effects:\n Groups Name            Variance  Std.Dev.\n hour   logdiff_max_gmc 2.221e-05 0.004713\n hour.1 (Intercept)     1.875e-01 0.433067\n sector (Intercept)     4.400e-01 0.663352\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.613429   0.308150  -5.236 1.64e-07 ***\nlogdiff_max_gmc  0.033243   0.002382  13.956  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm -0.002\n\n\n\nAIMc &lt;- glmer(offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc||sector)+(1 + logdiff_max_gmc||hour),\n            data = model_data_sampled_mlm,family = \"binomial\")\n\nsummary(AIMc)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc || sector) +  \n    (1 + logdiff_max_gmc || hour)\n   Data: model_data_sampled_mlm\n\n     AIC      BIC   logLik deviance df.resid \n 88918.0  88975.3 -44453.0  88906.0   102554 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.1908 -0.4691 -0.3565 -0.2423  6.0439 \n\nRandom effects:\n Groups   Name            Variance  Std.Dev.\n hour     logdiff_max_gmc 3.398e-05 0.005829\n hour.1   (Intercept)     1.879e-01 0.433422\n sector   logdiff_max_gmc 7.124e-04 0.026690\n sector.1 (Intercept)     4.450e-01 0.667115\nNumber of obs: 102560, groups:  hour, 16; sector, 5\n\nFixed effects:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.61010    0.31074  -5.182  2.2e-07 ***\nlogdiff_max_gmc  0.04197    0.01223   3.431 0.000601 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlgdff_mx_gm 0.000 \n\n\n\nanova(CIM,AIMa,AIMb,AIMc)\n\nData: model_data_sampled_mlm\nModels:\nCIM: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 | hour)\nAIMb: offence_bool ~ logdiff_max_gmc + (1 | sector) + (1 + logdiff_max_gmc || hour)\nAIMa: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc | sector) + (1 | hour)\nAIMc: offence_bool ~ logdiff_max_gmc + (1 + logdiff_max_gmc || sector) + (1 + logdiff_max_gmc || hour)\n     npar   AIC   BIC logLik deviance    Chisq Df Pr(&gt;Chisq)    \nCIM     4 89058 89097 -44525    89050                           \nAIMb    5 89060 89107 -44525    89050   0.7269  1     0.3939    \nAIMa    6 88919 88976 -44453    88907 143.0934  1     &lt;2e-16 ***\nAIMc    6 88918 88975 -44453    88906   0.4945  0               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nextract_eq(AIMa)\n\n\\[\n\\begin{aligned}\n  \\operatorname{offence\\_bool}_{i}  &\\sim \\operatorname{Binomial}(n = 1, \\operatorname{prob}_{\\operatorname{offence\\_bool} = 1} = \\widehat{P}) \\\\\n    \\log\\left[\\frac{\\hat{P}}{1 - \\hat{P}} \\right] &=\\alpha_{j[i],k[i]} + \\beta_{1k[i]}(\\operatorname{\\logdiff\\_max\\_gmc}) \\\\    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for hour j = 1,} \\dots \\text{,J} \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{k} \\\\\n      &\\beta_{1k}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\mu_{\\alpha_{k}} \\\\\n      &\\mu_{\\beta_{1k}}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{k}} & \\rho_{\\alpha_{k}\\beta_{1k}} \\\\\n     \\rho_{\\beta_{1k}\\alpha_{k}} & \\sigma^2_{\\beta_{1k}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for sector k = 1,} \\dots \\text{,K}\n\\end{aligned}\n\\]\n\n\n$$\n\\[\\begin{aligned}\n  \\operatorname{offence\\_bool}_{i}  &\\sim \\operatorname{Binomial}(n = 1, \\operatorname{prob}_{\\operatorname{offence\\_bool} = 1} = \\widehat{P}) \\\\\n    \\log\\left[\\frac{\\hat{P}}{1 - \\hat{P}} \\right] &=\\alpha_{j[i],k[i]} + \\beta_{1}(\\operatorname{diff\\_max\\_gmc}) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for hour j = 1,} \\dots \\text{,J} \\\\\n    \\alpha_{k}  &\\sim N \\left(\\mu_{\\alpha_{k}}, \\sigma^2_{\\alpha_{k}} \\right)\n    \\text{, for sector k = 1,} \\dots \\text{,K}\n\\end{aligned}\\]\n$$ CIM\n\nplot_slopes(CIM,\n            variables = \"logdiff_max_gmc\",\n            by = c(\"hour\",\"sector\"))+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  scale_fill_brewer(type = \"qual\",\n                      palette = \"Set3\")+\n  theme_minimal()+\n  scale_y_continuous(limits = c(0,0.01))+\n  labs(col = \"Sector\",\n       fill = \"Sector\")\n\n\n\n\n\n\n\n\nAIMa\n\nplot_slopes(AIMa,\n            variables = \"logdiff_max_gmc\",\n            by = c(\"hour\",\"sector\"))+\n  scale_color_brewer(type = \"qual\",\n                     palette = \"Set3\")+\n  scale_fill_brewer(type = \"qual\",\n                      palette = \"Set3\")+\n  theme_minimal()+\n  scale_y_continuous(limits = c(0,0.01))+\n  labs(col = \"Sector\",\n       fill = \"Sector\")"
  },
  {
    "objectID": "3B_Analysis_WWD_allowed.html#daily-potential",
    "href": "3B_Analysis_WWD_allowed.html#daily-potential",
    "title": "Results",
    "section": "",
    "text": "n_d10_logdiff_max &lt;- simpl_network_sf |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt;  \n  mutate(d10_bool = logdiff_max &gt;= quantile(logdiff_max, 0.90,na.rm = T),\n         .by=c(hour,day_type,roadclass)) |&gt; \n  st_drop_geometry() |&gt; \n  summarise(n_d10 = sum(d10_bool,na.rm = T),.by = c(pair_id,day_type)) |&gt; \n  pivot_wider(names_from = day_type,values_from = n_d10)\n\n\nmap_ratruns_wk &lt;- simpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      }))) |&gt;\n  ggplot(aes(col =  weekday,\n             linewidth = weekday_ld)) +\n  geom_sf() +\n  scale_colour_viridis_b(na.value = \"gray30\",\n                         option = \"plasma\",\n                         direction = -1) +\n  scale_linewidth_continuous(\n    limits = c(0, 24),\n    range = c(0.05, 0.5),\n    transform = \"exp\"\n  ) +\n  theme_void() +\n  guides(linewidth = \"none\") +\n  labs(col = \"Rat-run potential\") +\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.7))\n\nmap_ratruns_wk\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt;\n  left_join(n_d10_logdiff_max, by = \"pair_id\") |&gt;\n  mutate(across(weekday:weekend, \\(x) {\n    if_else(roadclass == \"residential\", x, NA)\n    })) |&gt;\n  mutate(across(weekday:weekend, list(\n    ld = \\(x) {\n      if_else(roadclass != \"residential\", 1, x)\n      })))\n\nSimple feature collection with 91970 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 979293.3 ymin: 985597.3 xmax: 1007251 ymax: 1025644\nProjected CRS: MAGNA-SIRGAS / Colombia Bogota zone\nFirst 10 features:\n   lanes     way_id    roadclass oneway lanes.1 surface maxspeed       time\n1     NA   89742442  residential  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  36.379850\n2      1   86192048   trunk_link   TRUE       1 asphalt     &lt;NA&gt;   6.131124\n3      2   90894261     tertiary   TRUE       2 asphalt     &lt;NA&gt;  28.521182\n4      1   24798427 primary_link   TRUE       1 asphalt     &lt;NA&gt;   6.488302\n5     NA  117826480     tertiary  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  12.262932\n6      2   24075320   trunk_link   TRUE       2 asphalt     &lt;NA&gt;  19.056994\n7     NA  689403428  residential  FALSE    &lt;NA&gt; unpaved     &lt;NA&gt; 122.464815\n8      2   31509948  residential  FALSE       2 asphalt     &lt;NA&gt;   6.420331\n9     NA  523839186  residential  FALSE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  11.820349\n10    NA 550938368r  residential   TRUE    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;  29.528495\n   time_weighted component pair_id weekday weekend\n1      36.379850         1       1       1       0\n2       6.131124         1       2      NA      NA\n3      28.521182         1       3      NA      NA\n4       6.488302         1       4      NA      NA\n5      12.262932         1       5      NA      NA\n6      19.056994         1       6      NA      NA\n7     122.464815         1       7       1       0\n8       6.420331         1       8       4       4\n9      11.820349         1       9       9      11\n10     29.528495         1      10       1       0\n                             geom weekday_ld weekend_ld\n1  LINESTRING (995528.4 995623...          1          0\n2  LINESTRING (996495.7 101233...          1          1\n3  LINESTRING (998739.2 101067...          1          1\n4  LINESTRING (997806.9 100455...          1          1\n5  LINESTRING (998659.5 996788...          1          1\n6  LINESTRING (1002287 1009965...          1          1\n7  LINESTRING (988912.4 100125...          1          0\n8  LINESTRING (990692.7 100298...          4          4\n9  LINESTRING (996803.1 997474...          9         11\n10 LINESTRING (986133.3 100201...          1          0"
  }
]