[
  {
    "objectID": "D2_congestion_tests.html",
    "href": "D2_congestion_tests.html",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\npackageVersion (\"dodgr\")\n\n[1] '0.4.1.37'\n\n\n\n\n\nsf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019_full &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 3) |&gt; \n  st_transform(4326)\n# zb_view(bog_zone)\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,]\n\n\n\n\n\ndodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nmain_component &lt;- graph_bogota |&gt; data.frame() |&gt; count(component) |&gt; slice_max(n) |&gt; pull(component)\nclear_dodgr_cache()\n\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\nFixing the weighted time column\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncongested_contracted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\ncongested_contracted$time_weighted[congested_contracted$highway == \"road_60\"] &lt;- (3.6*congested_contracted$d_weighted[congested_contracted$highway == \"road_60\"])/30\n\nCalculating the centrality\n\ncongested_centrality &lt;- congested_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nExporting the graph to sf object\n\nsf_net0 &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nsf_net &lt;- sf_net0 |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 11272 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis section includes the code for joining the speed data network with OSM network\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[bog_zone |&gt; st_transform(3116) ,]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\nA quick visualisation of the network that could be part of the correspondence\n\ntm_shape(sf_bog_major[speed_buffer,])+\n  tm_lines()+\n  tm_shape(speed_buffer)+\n  tm_polygons(\"blue\",alpha = 0.3)\n\n\n\n\n\n\n\n\nusing the spatial operation st_intersects we select the links that can be related to the buffer.\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nA quick check on the distribution of overlaps\n\noverlap_buffer |&gt; \n  ggplot(aes(n))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe following code will produce plots for all links with one-to-many correspondence. However, we will calculate the average of the speed of the observed speeds\n\n# for (j in 1:nrow(overlap_buffer)){\n#   mmap &lt;- tm_shape(speed_buffer[speed_buffer$TID %in% TID_to_edge_id$TID[TID_to_edge_id$edge_id==overlap_buffer$edge_id[j]],])+\n#     tm_polygons(\"TID\",alpha = 0.3)+\n#     tm_shape(sf_bog_major[sf_bog_major$edge_id == overlap_buffer$edge_id[j],])+\n#     tm_lines(\"yellow\")\n#   print(mmap)\n#   }\n\nTo check if there is any link in the speed dataset that has not been linked to any object of the road network.\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\n\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nRows: 110880 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): day_type\ndbl (11): TID, year, hour, d_min_speed, d_q1_speed, d_median_speed, d_mean_s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe identify the hour that showed the lowest speeds in average\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) \n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday     18           0.434\n2  2019 weekday     17           0.453\n3  2019 weekday     12           0.461\n4  2019 weekday      9           0.463\n5  2019 weekday     11           0.463\n\n\nThe morning peak is selected as the number of WWD reports is higher\n\nmin_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekday\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nThe following code produce some quick visualisation of the differences\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-2,2))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1226 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nlength(colSums(dist_matrix)[is.na(colSums(dist_matrix,na.rm = T))])\n\n[1] 0\n\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\n\n\n\nsf_net_jct &lt;- sf_net0 |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))\n\n\n\n\n\n\n\nWe assumed an initial speed of 10 km/h for the links that represent the wrong direction. But that choice is arbitrary. The following section will produce the results for multiple speeds, to compare the changes in centrality based on the assumed fre-flow speed of wrong-way links. First, we will test the changes on the free-flow network\n\ntest_centralities &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contracted\n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 *\n                                                                                                 bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities |&gt;\n  pivot_longer(cols = any_of(names(test_centralities)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contracted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\ntest_centralities_cong &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contr_adjusted \n                               \n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               \n                               \n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                               \n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities_cong |&gt;\n  pivot_longer(cols = any_of(names(test_centralities_cong)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contr_adjusted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities_cong) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\nwwlinks &lt;- bind_cols(\n  tibble(highway = bog_contr_adjusted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities_cong) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"ff_cent\"),\n  tibble(highway = bog_contracted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"cong_cent\")\n)\n\nNew names:\n• `highway` -&gt; `highway...1`\n• `name` -&gt; `name...2`\n• `highway` -&gt; `highway...4`\n• `name` -&gt; `name...5`\n\n\nA simple linear model\n\nm1 &lt;- lm(ff_cent~cong_cent+0, data = wwlinks)\nsummary(m1)\n\n\nCall:\nlm(formula = ff_cent ~ cong_cent + 0, data = wwlinks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3242640     -680        0     2543 13324726 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \ncong_cent 1.162182   0.007106   163.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217600 on 134384 degrees of freedom\nMultiple R-squared:  0.166, Adjusted R-squared:  0.166 \nF-statistic: 2.675e+04 on 1 and 134384 DF,  p-value: &lt; 2.2e-16\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 47857 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent)/(0.5*(cong_cent+ff_cent)),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 47857 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# t_thresholds = round(30*1.2^seq(0,30,2),-1) |&gt; unique()\nt_thresholds = seq(1,25,3)*60\n\ngrid_test &lt;- expand_grid(v = seq(0, 30, 3),\n                         th = t_thresholds)\n\n\ntest_centralities_threshold &lt;- do.call(bind_cols,\n                                       lapply(1:nrow(grid_test),\n                                              # lapply(1:2,\n                                              \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_adjusted\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntidy_test &lt;- test_centralities_threshold |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\nExploration of distributions for the entire network\n\ntidy_test |&gt;\n  arrange(dist.th) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nWarning: Removed 126431 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nFor one-way residential links\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 116216 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = logreldiff.ff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 116216 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nsample_ids &lt;- bog_contr_adjusted |&gt;\n  data.frame() |&gt;\n  filter(highway ==\"road_10\") |&gt;\n  pull(edge_id) |&gt;\n  sample(15)\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\",edge_id %in% sample_ids) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = dist.th,\n             y = logdiff,\n             group = factor(edge_id),col = factor(edge_id)))+\n  geom_line(alpha = 0.3)+\n  theme_minimal()+\n  facet_wrap(wwd.speed~.)+\n  scale_x_continuous(breaks = t_thresholds,labels = round(t_thresholds/60,1))+\n  theme(panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90))\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 470 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\nwrite_csv(tidy_test, file = \"sf_network/cent_tests.csv\")\n\nst_write(sf_net_jct, \"sf_network/small_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting layer `small_sf_network' to data source \n  `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting 37596 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "D2_congestion_tests.html#loading-network",
    "href": "D2_congestion_tests.html#loading-network",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "sf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019_full &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 3) |&gt; \n  st_transform(4326)\n# zb_view(bog_zone)\n\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,]"
  },
  {
    "objectID": "D2_congestion_tests.html#baseline-graph-building",
    "href": "D2_congestion_tests.html#baseline-graph-building",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "dodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nmain_component &lt;- graph_bogota |&gt; data.frame() |&gt; count(component) |&gt; slice_max(n) |&gt; pull(component)\nclear_dodgr_cache()\n\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\nAs the network might have several components, we can explore the size of the graph.\n\n# number of edges\nm_edges &lt;- bog_contracted |&gt;data.frame() |&gt; count(component)\n# Number of nodes\nn_nodes &lt;- bog_contracted |&gt; dodgr_vertices() |&gt; count(component)\n\nFixing the weighted time column\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncongested_contracted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\ncongested_contracted$time_weighted[congested_contracted$highway == \"road_60\"] &lt;- (3.6*congested_contracted$d_weighted[congested_contracted$highway == \"road_60\"])/30\n\nCalculating the centrality\n\ncongested_centrality &lt;- congested_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nExporting the graph to sf object\n\nsf_net0 &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nsf_net &lt;- sf_net0 |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 11272 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")"
  },
  {
    "objectID": "D2_congestion_tests.html#using-the-actual-speeds-for-adjusting-graph",
    "href": "D2_congestion_tests.html#using-the-actual-speeds-for-adjusting-graph",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "This section includes the code for joining the speed data network with OSM network\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[bog_zone |&gt; st_transform(3116) ,]\n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\nA quick visualisation of the network that could be part of the correspondence\n\ntm_shape(sf_bog_major[speed_buffer,])+\n  tm_lines()+\n  tm_shape(speed_buffer)+\n  tm_polygons(\"blue\",alpha = 0.3)\n\n\n\n\n\n\n\n\nusing the spatial operation st_intersects we select the links that can be related to the buffer.\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\nA quick check on the distribution of overlaps\n\noverlap_buffer |&gt; \n  ggplot(aes(n))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe following code will produce plots for all links with one-to-many correspondence. However, we will calculate the average of the speed of the observed speeds\n\n# for (j in 1:nrow(overlap_buffer)){\n#   mmap &lt;- tm_shape(speed_buffer[speed_buffer$TID %in% TID_to_edge_id$TID[TID_to_edge_id$edge_id==overlap_buffer$edge_id[j]],])+\n#     tm_polygons(\"TID\",alpha = 0.3)+\n#     tm_shape(sf_bog_major[sf_bog_major$edge_id == overlap_buffer$edge_id[j],])+\n#     tm_lines(\"yellow\")\n#   print(mmap)\n#   }\n\nTo check if there is any link in the speed dataset that has not been linked to any object of the road network.\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\n\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nRows: 110880 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): day_type\ndbl (11): TID, year, hour, d_min_speed, d_q1_speed, d_median_speed, d_mean_s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe identify the hour that showed the lowest speeds in average\n\nsummary_speed_ratios &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) \n\nsummary_speed_ratios |&gt;\n  filter(year==2019,day_type == \"weekday\") |&gt;\n  arrange(mean_norm_speed) |&gt; \n  head(5)\n\n# A tibble: 5 × 4\n   year day_type  hour mean_norm_speed\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1  2019 weekday     18           0.434\n2  2019 weekday     17           0.453\n3  2019 weekday     12           0.461\n4  2019 weekday      9           0.463\n5  2019 weekday     11           0.463\n\n\nThe morning peak is selected as the number of WWD reports is higher\n\nmin_speed_key &lt;- summary_speed_ratios |&gt;  \n  filter(year == 2019,day_type == \"weekday\",hour == 9) |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\")\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(edge_id = bog_centrality$edge_id,\n                   cent_bl = bog_centrality$centrality) |&gt; \n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt; \n  mutate(\n    diff = cent_cong - cent_bl,\n    reldiff = diff/(0.5*(cent_bl+cent_cong))\n  )\n\n\n\n\nThe following code produce some quick visualisation of the differences\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\nggplot(cent_all, aes(diff))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(cent_all, aes(reldiff))+\n  geom_histogram()+\n  scale_x_continuous(limits = c(-2,2))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1226 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nsf_net |&gt; \n  mutate(reldiff = if_else(abs(reldiff)&gt;2,\n                           NA_real_,\n                           reldiff)) |&gt; \n  ggplot(aes(col = reldiff))+\n  geom_sf()+\n  theme_void()+\n  scale_color_gradient2(midpoint = 0,\n                        low = \"red3\",\n                        high = \"green4\",\n                        mid = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance from the major network might be related to the reports of traffic offences. For this purpose, we need the following code to obtain the average distance of each minor link to the major network.\nIdentifying the junctions and classifying them based on the road hierarchy\n\nlow_hierarchy &lt;- c(\"residential\",\"unclassified\")\n\njunction_class_to &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt;\n  summarise(count = n(),.by = c(to_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count)|&gt; \n  rename(id = to_id)\n\njunction_class_from &lt;- sf_net |&gt; \n  st_drop_geometry() |&gt; \n  mutate(road_type = if_else(roadclass %in% low_hierarchy,\n                             \"minor\",\n                             \"major\")) |&gt; \n  summarise(count = n(),.by = c(from_id,road_type)) |&gt;\n  pivot_wider(names_from = road_type,values_from = count) |&gt;\n  rename(id = from_id)\n\njunctions_classed &lt;- junction_class_to |&gt; \n  full_join(junction_class_from,by = \"id\",suffix = c(\".to\",\".from\")) |&gt; \n  mutate(jct_type = case_when(is.na(minor.to)&is.na(minor.from)~\"major\",\n                              is.na(major.to)&is.na(major.from)~\"minor\",\n                              (!is.na(minor.to)&!is.na(major.from))|\n                                (!is.na(minor.from)&!is.na(major.to))~\"minmaj\")) |&gt; \n  select(-starts_with(\"m\"))\n\nCalculating the network distance for all nodes in the minor network to the major network\n\nminmaj_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minmaj\") |&gt; pull(id)\nminor_from_ids &lt;- junctions_classed |&gt; filter(jct_type == \"minor\") |&gt; pull(id)\n\ndist_matrix &lt;- dodgr_dists(bog_contracted,\n                           from = minor_from_ids,\n                           to = minmaj_ids,\n                           shortest = T)\n\n\nlength(colSums(dist_matrix)[is.na(colSums(dist_matrix,na.rm = T))])\n\n[1] 0\n\n\n\nfastest_all &lt;- tibble(\n  id.jct = minor_from_ids,\n  dist.jct =\n    apply(dist_matrix, 1,\\(x) min(x,na.rm = TRUE)\n      )\n  )\n\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\nWarning in min(x, na.rm = TRUE): no non-missing arguments to min; returning Inf\n\n\n\nsf_net_jct &lt;- sf_net0 |&gt;\n  left_join(fastest_all,\n            by = c(\"from_id\"=\"id.jct\"),\n            relationship = \"many-to-one\") |&gt; \n  mutate(dist.jct = case_when(is.na(dist.jct)&roadclass %in% low_hierarchy ~ 0,\n                              is.infinite(dist.jct)~NA,\n                              T~dist.jct))"
  },
  {
    "objectID": "D2_congestion_tests.html#tests-for-wrong-direction-speed",
    "href": "D2_congestion_tests.html#tests-for-wrong-direction-speed",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "We assumed an initial speed of 10 km/h for the links that represent the wrong direction. But that choice is arbitrary. The following section will produce the results for multiple speeds, to compare the changes in centrality based on the assumed fre-flow speed of wrong-way links. First, we will test the changes on the free-flow network\n\ntest_centralities &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contracted\n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 *\n                                                                                                 bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities |&gt;\n  pivot_longer(cols = any_of(names(test_centralities)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contracted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\ntest_centralities_cong &lt;- do.call(bind_cols,\n                             lapply(seq(0, 30, 1), \\(v) {\n                               bog_ff_test &lt;- bog_contr_adjusted \n                               \n                               dodgr::clear_dodgr_cache()\n                               bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;- (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / v\n                               \n                               \n                               test_centrality &lt;- bog_ff_test |&gt;\n                                 dodgr_centrality(column = \"time_weighted\")\n                               \n                               \n                               t &lt;- tibble(cent = test_centrality$centrality)\n                               names(t) &lt;- paste0(\"cent_\", v)\n                               return(t)\n                               \n                             }))\n\nVisualising the impact of the assigned speed on centrality distribution across the network (top)\n\ntest_centralities_cong |&gt;\n  pivot_longer(cols = any_of(names(test_centralities_cong)),\n               names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\ntibble(highway = bog_contr_adjusted[,c(\"highway\")]) |&gt; \n  bind_cols(test_centralities_cong) |&gt; \n  filter(highway == \"road_10\") |&gt; \n  pivot_longer(-highway,names_prefix = \"cent_\") |&gt;\n  mutate(name = as.integer(name)) |&gt; \n  ggplot(aes(x = factor(name),y = value+1))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\n\n\n\n\n\n\n\n\n\n\n\nwwlinks &lt;- bind_cols(\n  tibble(highway = bog_contr_adjusted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities_cong) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"ff_cent\"),\n  tibble(highway = bog_contracted[, c(\"highway\")]) |&gt;\n    bind_cols(test_centralities) |&gt;\n    filter(highway == \"road_10\") |&gt;\n    pivot_longer(-highway, names_prefix = \"cent_\",\n                 values_to = \"cong_cent\")\n)\n\nNew names:\n• `highway` -&gt; `highway...1`\n• `name` -&gt; `name...2`\n• `highway` -&gt; `highway...4`\n• `name` -&gt; `name...5`\n\n\nA simple linear model\n\nm1 &lt;- lm(ff_cent~cong_cent+0, data = wwlinks)\nsummary(m1)\n\n\nCall:\nlm(formula = ff_cent ~ cong_cent + 0, data = wwlinks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3242640     -680        0     2543 13324726 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \ncong_cent 1.162182   0.007106   163.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217600 on 134384 degrees of freedom\nMultiple R-squared:  0.166, Adjusted R-squared:  0.166 \nF-statistic: 2.675e+04 on 1 and 134384 DF,  p-value: &lt; 2.2e-16\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 47857 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nwwlinks |&gt;\n  mutate(name = as.integer(name...2),\n         sign = (cong_cent-ff_cent)/abs(cong_cent-ff_cent)) |&gt;\n  ggplot(aes(x = factor(name),y = abs(cong_cent-ff_cent)/(0.5*(cong_cent+ff_cent)),fill = factor(sign)))+\n  geom_boxplot(outlier.shape = NA)+\n  scale_y_log10()+\n  labs(title = \"Centrality distribution (only WW-links)\",\n       x = \"WWD links assigned speed\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 47857 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "D2_congestion_tests.html#tests-for-distance-threshold",
    "href": "D2_congestion_tests.html#tests-for-distance-threshold",
    "title": "Testing changes in centrality",
    "section": "",
    "text": "# t_thresholds = round(30*1.2^seq(0,30,2),-1) |&gt; unique()\nt_thresholds = seq(1,25,3)*60\n\ngrid_test &lt;- expand_grid(v = seq(0, 30, 3),\n                         th = t_thresholds)\n\n\ntest_centralities_threshold &lt;- do.call(bind_cols,\n                                       lapply(1:nrow(grid_test),\n                                              # lapply(1:2,\n                                              \\(i) {\n                                                bog_ff_test &lt;- bog_contracted\n                                                bog_cong_test &lt;- bog_contr_adjusted\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                bog_ff_test$time_weighted[bog_ff_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_ff_test$d_weighted[bog_ff_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                bog_cong_test$time_weighted[bog_cong_test$highway == \"road_10\"] &lt;-\n                                                  (3.6 * bog_cong_test$d_weighted[bog_cong_test$highway == \"road_10\"]) / grid_test$v[i]\n                                                \n                                                dodgr::clear_dodgr_cache()\n                                                \n                                                # Centrality calculations\n                                                \n                                                test_centrality_ff &lt;- bog_ff_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                test_centrality &lt;- bog_cong_test |&gt;\n                                                  dodgr_centrality(column = \"time_weighted\",\n                                                                   dist_threshold = grid_test$th[i])\n                                                \n                                                \n                                                t &lt;-\n                                                  tibble(cent.ff = test_centrality_ff$centrality,\n                                                         cent.cong = test_centrality$centrality)\n                                                \n                                                names(t) &lt;-\n                                                  paste(names(t), grid_test$v[i], grid_test$th[i], sep = \"_\")\n                                                \n                                                return(t)\n                                                \n                                              }))\n\n\ntidy_test &lt;- test_centralities_threshold |&gt;\n  pivot_longer(-edge_id,\n               names_to = \"test\",\n               values_to = \"cent\",names_prefix = \"cent.\") |&gt; \n  separate_wider_delim(test,delim = \"_\",names = c(\"network\",\"wwd.speed\",\"dist.th\")) |&gt; \n  pivot_wider(names_from = network,values_from = cent) |&gt;\n  left_join(tibble(edge_id = bog_contracted$edge_id,\n       component = bog_contracted$component) |&gt; \n  left_join(n_nodes, by = join_by(component)),\n  by = join_by(edge_id)\n  ) |&gt; \n  mutate(diff = cong - ff,\n         logdiff = sign(diff)*log10(abs(diff)),\n         reldiff = diff/(0.5*(ff+cong)),\n         logreldiff.ff = sign(diff)*log10(abs(diff/(ff+n-1)))) |&gt; \n  mutate(across(c(logdiff,logreldiff.ff),\\(x) if_else(diff == 0 & (ff+cong &gt; 0),0,x))) |&gt; \n  select(-n,-component)\n\nExploration of distributions for the entire network\n\ntidy_test |&gt;\n  arrange(dist.th) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nWarning: Removed 126431 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nFor one-way residential links\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = reldiff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 116216 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\") |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = factor(wwd.speed), y = logreldiff.ff))+\n  geom_boxplot()+\n  facet_wrap(dist.th~.)\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 116216 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nsample_ids &lt;- bog_contr_adjusted |&gt;\n  data.frame() |&gt;\n  filter(highway ==\"road_10\") |&gt;\n  pull(edge_id) |&gt;\n  sample(15)\n\n\ntidy_test |&gt;\n  left_join(tibble(edge_id = bog_contr_adjusted$edge_id,\n                   highway = bog_contr_adjusted$highway)) |&gt; \n  filter(highway ==\"road_10\",edge_id %in% sample_ids) |&gt; \n  mutate(across(wwd.speed:dist.th,as.integer)) |&gt; \n  ggplot(aes(x = dist.th,\n             y = logdiff,\n             group = factor(edge_id),col = factor(edge_id)))+\n  geom_line(alpha = 0.3)+\n  theme_minimal()+\n  facet_wrap(wwd.speed~.)+\n  scale_x_continuous(breaks = t_thresholds,labels = round(t_thresholds/60,1))+\n  theme(panel.grid.minor.x = element_blank(),\n        axis.text.x = element_text(angle = 90))\n\nJoining with `by = join_by(edge_id)`\n\n\nWarning: Removed 470 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\nwrite_csv(tidy_test, file = \"sf_network/cent_tests.csv\")\n\nst_write(sf_net_jct, \"sf_network/small_sf_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting layer `small_sf_network' to data source \n  `sf_network/small_sf_network.gpkg' using driver `GPKG'\nWriting 37596 features with 22 fields and geometry type Line String."
  },
  {
    "objectID": "B1_speed_data.html",
    "href": "B1_speed_data.html",
    "title": "Speed Data Processing",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"data.table\",\n    \"paletteer\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n        sf  tidyverse data.table  paletteer \n      TRUE       TRUE       TRUE       TRUE \n\nsetDTthreads(0)"
  },
  {
    "objectID": "B1_speed_data.html#preliminary-cleaning",
    "href": "B1_speed_data.html#preliminary-cleaning",
    "title": "Speed Data Processing",
    "section": "Preliminary cleaning",
    "text": "Preliminary cleaning\n\nall_data[,\n           INICIO := mdy_hms(INICIO,\n                             tz = \"America/Bogota\")\n         ][,\n           `:=`(year = year(INICIO))\n           ]\n\nNot all links have data for all months. So the first step is to identify the links with enough data per year.\n\nlinks_summary &lt;- unique(\n  all_data[,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"TID\",\"year\")]\n\n\nlinks_summary |&gt; \n  ggplot(aes(count))+\n  geom_histogram(binwidth = 1)+\n  scale_y_log10()+\n  scale_x_continuous(breaks = 0:12)+\n  facet_grid(year~.)\n\nLinks with data in 5 or more months per year will be used for this analysis.\n\nlinks_data = links_summary[,count := 1*(count&gt;=5)\n                           ][,.(tot_count = sum(count)),\n                             by = \"TID\"][\n                               tot_count == 2,\n                               ] |&gt; pull(TID)\n\nWe extract the data only for these links with the following code and clean the memory:\n\nclean_data &lt;- all_data[TID %in% links_data]\nrm(all_data,links_data,links_summary,lst_files)\ngc()"
  },
  {
    "objectID": "B1_speed_data.html#classifiying-the-data",
    "href": "B1_speed_data.html#classifiying-the-data",
    "title": "Speed Data Processing",
    "section": "Classifiying the data",
    "text": "Classifiying the data\nFirst, we will identify the days that were bank holidays in Colombia\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nclean_data[date(INICIO) %in% bank_holidays,\n           DIA_SEMANA := \"Festivo\"\n           ]\n\nclean_data[,day_type := fcase(\n             DIA_SEMANA == \"Domingo\",\"weekend\",\n             DIA_SEMANA == \"Sabado\",\"weekend\",\n             DIA_SEMANA == \"Festivo\",\"weekend\",\n             DIA_SEMANA == \"Viernes\",\"friday\",\n             default = \"weekday\")\n             ]\n\nThe following code extracts the 94th percentile of the speeds in each road link.\n\nmax_speeds &lt;- clean_data[,\n                             .(p94_speed = quantile(VEL_PROMEDIO,\n                                                       0.94,\n                                                       na.rm = T)),\n                             by =  c(\"TID\",\"year\")]\n\nNow, we calculate the median hourly speed for each road link by day type and road link.\n\nsummary_speeds &lt;- clean_data[, as.list(summary(VEL_PROMEDIO)),\n  by = .(TID, HORA, day_type, year)]\n\nsetnames(summary_speeds,\n         old = c(\"HORA\",\"Min.\",\"1st Qu.\",\"Median\",\"Mean\",\"3rd Qu.\",\"Max.\"),\n         new = c(\"hour\",\"d_min_speed\",\"d_q1_speed\",\"d_median_speed\",\"d_mean_speed\",\"d_q3_speed\",\"d_max_speed\"))\n\nNow we normalise the values by dividing by the 95th-percentile speed\n\nnorm_summary_spd &lt;- merge(summary_speeds,\n                          max_speeds,\n                          by = c(\"TID\",\"year\"))[,\n                                                d_norm_speed := d_median_speed/p94_speed\n                                                ]\n\n\nfwrite(norm_summary_spd,file = \"sf_network/summary_speeds.csv\")"
  },
  {
    "objectID": "B1_speed_data.html#some-visualisations-of-speed-distribution",
    "href": "B1_speed_data.html#some-visualisations-of-speed-distribution",
    "title": "Speed Data Processing",
    "section": "Some visualisations of speed distribution:",
    "text": "Some visualisations of speed distribution:\n\nHourly distributions\n\nNormalised speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  geom_boxplot(aes(group = hour), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  geom_jitter(alpha = 0.3, size = 0.3,col = \"gray60\")+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1,\n               alpha = 0.6,\n               show.legend = F)+\n  facet_grid(day_type~.)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Norm speed (observed/94th percentile)\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  theme(axis.text.x = element_text(angle = 90))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))\n\n\n\n\n\n\n\n\n\n\nAbsolute speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_median_speed))+\n  geom_boxplot(aes(group = hour), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  geom_jitter(alpha = 0.3, size = 0.3,col = \"gray60\")+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1,\n               alpha = 0.6,\n               show.legend = F)+\n  facet_grid(day_type~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Observed speed\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  theme(axis.text.x = element_text(angle = 90))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::lanonc_lancet\",n = 3))\n\n\n\n\n\n\n\n\n\n\n\nDaily profile by Link\n\nNormalised speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  # geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+\n  # geom_jitter(alpha = 0.3, size = 0.3,col = \"gray60\")+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type,\n                   group = TID),\n               linewidth = 0.01,\n               alpha = 0.04,\n               # show.legend = F\n               )+\n  facet_grid(day_type~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Speed ratio (observed/94th percentile)\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\")\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_summary()`).\n\n\n\n\n\n\n\n\n\n\n\nAbsolute speed\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_median_speed))+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type,\n                   group = TID),\n               linewidth = 0.01,\n               alpha = 0.04,\n               # show.legend = F\n               )+\n  facet_grid(day_type~year)+\n  geom_hline(yintercept = 60,col = \"#EE4C97\",linetype = \"dashed\",alpha = 0.6,linewidth = 1)+\n  annotate(geom = \"text\",x = 23,y = 63,label = \"Speed Limit \",vjust = 0,hjust = 1,face = \"italic\")+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Observed speed\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,100),breaks = seq(0,100,20))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\")\n\nWarning in annotate(geom = \"text\", x = 23, y = 63, label = \"Speed Limit \", :\nIgnoring unknown parameters: `face`\n\n\n\n\n\n\n\n\n\nMedian overall speed profile\n\nnorm_summary_spd |&gt; \n  ggplot(aes(x = hour,y = d_norm_speed))+\n  stat_summary(geom = \"line\",\n               fun = \"mean\",\n               aes(col = day_type),\n               linewidth = 1.5,\n               alpha = 0.6,\n               # show.legend = F\n               )+\n  facet_grid(.~year)+\n  theme_minimal()+\n  labs(x = \"Hour\", y = \"Speed ratio (observed/94th percentile)\")+\n  scale_x_continuous(breaks = 0:23,\n                     labels = sprintf(\"%02d:00\",0:23))+\n  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+\n  scale_colour_manual(values = paletteer_d(\"ggsci::default_nejm\",n = 3))+\n  theme(axis.text.x = element_text(angle = 90),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\")\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_summary()`)."
  },
  {
    "objectID": "B1_speed_data.html#spatial-data",
    "href": "B1_speed_data.html#spatial-data",
    "title": "Speed Data Processing",
    "section": "Spatial data",
    "text": "Spatial data\nUp to this point, all the data processing has not involved the spatial component. On the open data platform, it is possible to download the gpkg files for each month. With the following code, we will identify which file(s) is(are) needed to have all data.\nIdeally, we need a file that contains all 770 TID.\n\nunique(\n  clean_data[,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"year\",\"MES\")][count == max(count)]\n\n    year      MES count\n   &lt;int&gt;   &lt;char&gt; &lt;int&gt;\n1:  2019 February   767\n\n\nAs the file of February 2019 does not have all the links, we identify alternative files.\n\ntid_feb2019 &lt;- unique(clean_data[year == 2019 & MES == \"February\",\"TID\"])[,1]\ntid_missing &lt;- unique(clean_data[!(TID %in% tid_feb2019$TID),\"TID\"])\n\nunique(\n  clean_data[TID %in% tid_missing$TID,\n           c(\"TID\",\"year\",\"MES\")])[,\n                                   .(count = .N),\n                                   by = c(\"year\",\"MES\")][count == max(count)]\n\n     year       MES count\n    &lt;int&gt;    &lt;char&gt; &lt;int&gt;\n 1:  2020     April     3\n 2:  2019    August     3\n 3:  2020    August     3\n 4:  2019  December     3\n 5:  2020  December     3\n 6:  2020   January     3\n 7:  2020  February     3\n 8:  2019      July     3\n 9:  2020      July     3\n10:  2020      June     3\n11:  2020     March     3\n12:  2020       May     3\n13:  2019  November     3\n14:  2020  November     3\n15:  2019   October     3\n16:  2020   October     3\n17:  2019 September     3\n18:  2020 September     3\n\n\nThe files for February 2019 and October 2020 have been downloaded manually from the same source. As we are only interested in the geometries, we filter out all the other data.\n\nsf_feb2019&lt;- st_read(\n   \"raw_data/bogota/speed_data/Velocidades_Bitcarrier_Febrero_2019_1361955177739874723.gpkg\",\n   query=\"select TID,SHAPE from 'Velocidades_Bitcarrier_Febrero_2019'\"\n   ) |&gt;\n  filter(TID %in% tid_feb2019$TID) |&gt; select(TID) |&gt; slice_head(n = 1,by = TID)\n\nReading query `select TID,SHAPE from 'Velocidades_Bitcarrier_Febrero_2019''\nfrom data source `/home/juan/P1_ratruns_analysis/raw_data/bogota/speed_data/Velocidades_Bitcarrier_Febrero_2019_1361955177739874723.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1930309 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_oct2020 &lt;- st_read(\n  \"raw_data/bogota/speed_data/Velocidades_Bitcarrier_Octubre_2020_-1518838627102027019.gpkg\",\n  query=\"select TID,SHAPE from 'Velocidades_Bitcarrier_Octubre_2020'\") |&gt;\n  filter(TID %in% tid_missing$TID) |&gt; slice_head(n = 1,by = TID)\n\nReading query `select TID,SHAPE from 'Velocidades_Bitcarrier_Octubre_2020''\nfrom data source `/home/juan/P1_ratruns_analysis/raw_data/bogota/speed_data/Velocidades_Bitcarrier_Octubre_2020_-1518838627102027019.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2325212 features and 1 field\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.20333 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nsf_speed &lt;- bind_rows(sf_feb2019,sf_oct2020)\nrm(sf_feb2019,sf_oct2020)\ngc()\n\n            used   (Mb) gc trigger    (Mb)   max used    (Mb)\nNcells   1613672   86.2   35655492  1904.3   44569364  2380.3\nVcells 837899913 6392.7 1428733522 10900.4 1427172711 10888.5\n\n\nThe elements in the spatial objects contain MULTILINESTRINGS which cover long sections of the main corridors. Before saving the results, we will cast all features as LINESTRING to ease the process of working out a correspondence with the OSM network.\n\nsf_speed_cast &lt;- sf_speed |&gt; st_cast(\"LINESTRING\")\n\nWarning in st_cast.sf(sf_speed, \"LINESTRING\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\n\nSaving the network\n\nst_write(sf_speed_cast,dsn = \"sf_network/sf_speed_network.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/sf_speed_network.gpkg' using driver `GPKG'\nWriting layer `sf_speed_network' to data source \n  `sf_network/sf_speed_network.gpkg' using driver `GPKG'\nWriting 1267 features with 1 fields and geometry type Line String.\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\")\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : GDAL\nMessage 1: Non-conformant content for record 1 in column\nFECHA_ACTO_ADMINISTRATIVO, 2021-12-29T00:00:00.0Z, successfully parsed\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nA quick visualisation of the maximum speeds in 2019:\n\nsf_speed_cast[urban_perimeter,] |&gt; \n  left_join(max_speeds |&gt; filter(year==2019), by = \"TID\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = p94_speed),linewidth = 0.3,alpha = 0.7)+\n  scale_color_gradientn(colours = paletteer_c(\"grDevices::Plasma\", 30))+\n  theme_void()+\n  facet_grid(.~year)"
  },
  {
    "objectID": "D1_graph_centrality_tests.html",
    "href": "D1_graph_centrality_tests.html",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\n\n\n\n\nsf_bogota_2019_full &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                          layer = \"network_2019\")\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",n_circles = 2) |&gt;\n  st_transform(st_crs(sf_bogota_2019_full))\n# zb_view(bog_zone)\n\nThe following the code will clip the network; also, since we will be using wrong-way-driving, we have to allow both directions inone-way links to capture the changes of centrality.\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,] |&gt; \n  mutate(oneway = if_else(highway %in% c(\"residential\"),\"no\",oneway))\n\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\n\n\n\nIn order to include the actual speed along each corridor, I will specify 60 different classes to be applied to each road link. By default, speeds are assigned by road link by road type, e.g. any trunk road in Bogotá has a 60 km/h speed.\n\n# custom_wp &lt;- dodgr::weighting_profiles$weighting_profiles |&gt; filter(name==\"motorcar\")\ncustom_penalties &lt;- dodgr::weighting_profiles$penalties |&gt; filter(name==\"motorcar\")\ncustom_surfaces &lt;- dodgr::weighting_profiles$surface_speeds |&gt; filter(name==\"motorcar\")\n\ncustom_wp &lt;- data.frame(name = \"motorcar\",\n                        way = paste0(\"road_\",1:60),\n                        value = 1,\n                        max_speed = as.numeric(1:60))\n\ncustom_wp_list &lt;- list(weighting_profiles = custom_wp,\n                       surface_speeds = custom_surfaces,\n                       pealties = custom_penalties)\n\nwpj &lt;- jsonlite::toJSON (custom_wp_list, pretty = TRUE)\nwrite_lines(wpj,file = \"custom_wp_speeds.json\")\n\nAssigning a speed category based on the speed limit (baseline scenario)\n\nsf_bogota_2019_cwp &lt;- sf_bogota_2019 |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                                               TRUE ~ \"road_30\")) |&gt; \n  select(-highway)\n\nThe following code builds the graph using the new column instead of highway.\n\ngraph_bogota_custom &lt;- weight_streetnet(sf_bogota_2019_cwp,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\n\n\nWe calculate the centralities with both weighting profiles\n\ngraph_baseline_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\ngraph_cwp_centrality &lt;- graph_bogota_custom |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\nTo validate that the results are identical, we can run the following code\n\nidentical(graph_baseline_centrality$centrality,graph_cwp_centrality$centrality)\n\n[1] TRUE\n\n\n\n\n\n\nThere are four core alternatives for calculating the centrality using dodgr.\n\nbog_contracted[,\n    c(\"d_weighted\", \"d\", \"time_weighted\", \"time\")] |&gt;\n  pairs()\n\n\n\n\n\n\n\n\nAn initial check on the four alternatives reveal some inconsistencies in the time_weighted column.\nThere are 32 links in the contracted graph with differences in the weighted distance and the distance. The location of these edges will be inspected after calculating the centrality in Section 1.3.3.\nWhen multiple paths are available between two nodes in the contracted version of the graph, dodgr takes only the shortest weighted distance, which creates the difference. However, the resulting weighted time is not corrected and in some cases a 0 is returned.\nGiven this, we will recalculate the weighted time based on the weighted distance.\n\nbog_contracted$time_weighted[bog_contracted$time_weighted==0] &lt;- (3.6*bog_contracted$d_weighted[bog_contracted$time_weighted==0])/case_when(\n      bog_contracted$highway[bog_contracted$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\n\n\n\ncent_all &lt;- lapply(c(\"d_weighted\", \"d\", \"time_weighted\", \"time\"), \\(x) {\n  \n    df &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = x)\n  \n  tibble(edge_id = df$edge_id, cent = df$centrality) |&gt;\n    rename_with(.cols = \"cent\", .fn = \\(.y) {\n      paste(.y, x, sep = \"_\")\n    })\n  }) |&gt;\n  plyr::join_all(by = \"edge_id\",type = \"full\")\n\n\n\n\n\nigraph_bog &lt;- bog_contracted |&gt; \n  dodgr_to_igraph()\n\nLoading required namespace: igraph\n\n\n\ncent_all_ig &lt;- cent_all |&gt; \n  left_join(\n    tibble(edge_id = bog_contracted$edge_id,\n       cent_ig_d = igraph::edge_betweenness(graph = igraph_bog,\n                                            weights = bog_contracted$d),\n       cent_ig_d_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                     weights = bog_contracted$d_weighted),\n       cent_ig_time = igraph::edge_betweenness(graph = igraph_bog,weights = bog_contracted$time),\n       cent_ig_time_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                        weights = bog_contracted$time_weighted)\n       ),\n    by = \"edge_id\")\n\n\nsummary(cent_all_ig[,2:9])\n\n cent_d_weighted       cent_d        cent_time_weighted   cent_time      \n Min.   :      0   Min.   :      0   Min.   :      0    Min.   :      0  \n 1st Qu.:   4331   1st Qu.:   4330   1st Qu.:   4329    1st Qu.:   4329  \n Median :  16606   Median :  16482   Median :  14694    Median :  14569  \n Mean   :  79033   Mean   :  78951   Mean   :  80780    Mean   :  80660  \n 3rd Qu.:  71441   3rd Qu.:  70966   3rd Qu.:  53128    3rd Qu.:  52827  \n Max.   :1405281   Max.   :1408773   Max.   :2075299    Max.   :2069854  \n   cent_ig_d       cent_ig_d_weighted  cent_ig_time     cent_ig_time_weighted\n Min.   :      0   Min.   :      0    Min.   :      0   Min.   :      0      \n 1st Qu.:   4330   1st Qu.:   4331    1st Qu.:   4329   1st Qu.:   4329      \n Median :  16482   Median :  16606    Median :  14569   Median :  14694      \n Mean   :  78951   Mean   :  79033    Mean   :  80660   Mean   :  80780      \n 3rd Qu.:  70966   3rd Qu.:  71441    3rd Qu.:  52827   3rd Qu.:  53128      \n Max.   :1408773   Max.   :1405281    Max.   :2069854   Max.   :2075299      \n\n\nCheck if igraph results are identical to the ones obtained with dodgr\n\nwith(cent_all_ig,identical(cent_ig_d_weighted,cent_d_weighted))\n\n[1] TRUE\n\nwith(cent_all_ig,identical(cent_ig_time_weighted,cent_time_weighted))\n\n[1] TRUE\n\n\n\npairs(cent_all_ig[,2:9])\n\n\n\n\n\n\n\n\nJoining the results to the sf object\n\nsf_net &lt;- graph_bogota |&gt;\n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\n\n\n\nsf_net |&gt;  \n  select(cent_d_weighted:cent_time) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = log(cent),linewidth = cent+1))+\n  facet_wrap(type~.)+\n  scale_color_viridis_c(\n    # direction = -1\n    )+\n  scale_linewidth_continuous(range = c(0.05,0.5),\n                             transform = scales::transform_boxcox(p = 2))+\n  theme_void()+\n  guides(linewidth = \"none\",)\n\n\n\n\n\n\n\n\nVisualising differences\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot(aes(cent))+\n  geom_histogram()+\n  facet_wrap(type~.)+\n  scale_x_continuous(limits = c(-1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2582 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nInspecting significant changes\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  mutate(cent = case_when(abs(cent)&lt;0.1~0,\n                          cent &gt; 0.6 ~ 1,\n                          -cent &gt; 0.6 ~ -1,\n                          is.na(cent)~NA)) |&gt; \n  ggplot()+\n  geom_sf(aes(col = cent))+\n  facet_wrap(type~.)+\n  scale_color_distiller(palette = \"Spectral\", direction = 1)+\n  # scale_linewidth_continuous(range = c(0.05,0.5),\n  #                            transform = scales::transform_boxcox(p = 2))+\n  theme_void()\n\n\n\n\n\n\n\n\nIdentifying the links with differences &gt; 0 in d and weighted d\n\nsf_net |&gt; \n  mutate(diff_check = (d-d_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nSame for time\n\nsf_net |&gt; \n  mutate(diff_check = (time-time_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nA course assumption is that people decide the route they will use based on the ETA, also differences in speed limits can only captured if time is used. So weighted time will be used.\n\n\n\n\nlibrary(sfnetworks)\n\n\nsf_net$time_weighted[sf_net$time_weighted==0] &lt;- (3.6*sf_net$d_weighted[sf_net$time_weighted==0])/case_when(\n      sf_net$highway[sf_net$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\nbog_sfnet &lt;- sf_net |&gt; \n  as_sfnetwork(directed = TRUE)\n\n\nbog_sfnet &lt;- bog_sfnet |&gt;\n  activate(\"edges\") |&gt; \n  mutate(cent_sfn_time_weighted = tidygraph::centrality_edge_betweenness(\n    weights = time_weighted,\n    directed = T))\n\n\nbog_sfnet |&gt; \n  st_as_sf(\"edges\") |&gt; \nggplot()+\n  geom_sf(aes(col = cent_time_weighted))+\n  theme_void()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  select(cent_time_weighted,cent_sfn_time_weighted) |&gt; \n  pairs()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  with(identical(cent_time_weighted,cent_sfn_time_weighted))\n\n[1] TRUE\n\n\n\np1 = st_geometry(bog_sfnet, \"nodes\")[2401]\nst_crs(p1) = st_crs(bog_sfnet)\np2 = st_geometry(bog_sfnet, \"nodes\")[1407]\np3 = st_geometry(bog_sfnet, \"nodes\")[1509]\np4 = st_geometry(bog_sfnet, \"nodes\")[1840]\nst_crs(p3) = st_crs(bog_sfnet)\n\npaths = st_network_paths(bog_sfnet, from = p1, to = c(p2,p3,p4), weights = \"time_weighted\")\n\n\nplot_path = function(node_path) {\n  bog_sfnet %&gt;%\n    activate(\"nodes\") %&gt;%\n    slice(node_path) %&gt;%\n    plot(cex = 1.5, lwd = 1.5, add = TRUE)\n}\n\ncolors = sf.colors(4, categorical = TRUE)\n\nplot(bog_sfnet, col = \"grey\")\npaths %&gt;%\n  pull(node_paths) %&gt;%\n  walk(plot_path)\nplot(c(p1, p2, p3,p4), col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE)"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#loading-network",
    "href": "D1_graph_centrality_tests.html#loading-network",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "sf_bogota_2019_full &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                          layer = \"network_2019\")\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use a small subset of the network for this test to speed up the process.\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",n_circles = 2) |&gt;\n  st_transform(st_crs(sf_bogota_2019_full))\n# zb_view(bog_zone)\n\nThe following the code will clip the network; also, since we will be using wrong-way-driving, we have to allow both directions inone-way links to capture the changes of centrality.\n\nsf_bogota_2019 &lt;- sf_bogota_2019_full[bog_zone,] |&gt; \n  mutate(oneway = if_else(highway %in% c(\"residential\"),\"no\",oneway))\n\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#assignment-of-the-weighting-profile",
    "href": "D1_graph_centrality_tests.html#assignment-of-the-weighting-profile",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "In order to include the actual speed along each corridor, I will specify 60 different classes to be applied to each road link. By default, speeds are assigned by road link by road type, e.g. any trunk road in Bogotá has a 60 km/h speed.\n\n# custom_wp &lt;- dodgr::weighting_profiles$weighting_profiles |&gt; filter(name==\"motorcar\")\ncustom_penalties &lt;- dodgr::weighting_profiles$penalties |&gt; filter(name==\"motorcar\")\ncustom_surfaces &lt;- dodgr::weighting_profiles$surface_speeds |&gt; filter(name==\"motorcar\")\n\ncustom_wp &lt;- data.frame(name = \"motorcar\",\n                        way = paste0(\"road_\",1:60),\n                        value = 1,\n                        max_speed = as.numeric(1:60))\n\ncustom_wp_list &lt;- list(weighting_profiles = custom_wp,\n                       surface_speeds = custom_surfaces,\n                       pealties = custom_penalties)\n\nwpj &lt;- jsonlite::toJSON (custom_wp_list, pretty = TRUE)\nwrite_lines(wpj,file = \"custom_wp_speeds.json\")\n\nAssigning a speed category based on the speed limit (baseline scenario)\n\nsf_bogota_2019_cwp &lt;- sf_bogota_2019 |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                                               TRUE ~ \"road_30\")) |&gt; \n  select(-highway)\n\nThe following code builds the graph using the new column instead of highway.\n\ngraph_bogota_custom &lt;- weight_streetnet(sf_bogota_2019_cwp,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\"),\n                                 turn_penalty = F)\n\n\n\nWe calculate the centralities with both weighting profiles\n\ngraph_baseline_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\ngraph_cwp_centrality &lt;- graph_bogota_custom |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality()\n\nTo validate that the results are identical, we can run the following code\n\nidentical(graph_baseline_centrality$centrality,graph_cwp_centrality$centrality)\n\n[1] TRUE"
  },
  {
    "objectID": "D1_graph_centrality_tests.html#centrality-calculations",
    "href": "D1_graph_centrality_tests.html#centrality-calculations",
    "title": "Graph Centrality Tests",
    "section": "",
    "text": "There are four core alternatives for calculating the centrality using dodgr.\n\nbog_contracted[,\n    c(\"d_weighted\", \"d\", \"time_weighted\", \"time\")] |&gt;\n  pairs()\n\n\n\n\n\n\n\n\nAn initial check on the four alternatives reveal some inconsistencies in the time_weighted column.\nThere are 32 links in the contracted graph with differences in the weighted distance and the distance. The location of these edges will be inspected after calculating the centrality in Section 1.3.3.\nWhen multiple paths are available between two nodes in the contracted version of the graph, dodgr takes only the shortest weighted distance, which creates the difference. However, the resulting weighted time is not corrected and in some cases a 0 is returned.\nGiven this, we will recalculate the weighted time based on the weighted distance.\n\nbog_contracted$time_weighted[bog_contracted$time_weighted==0] &lt;- (3.6*bog_contracted$d_weighted[bog_contracted$time_weighted==0])/case_when(\n      bog_contracted$highway[bog_contracted$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\n\n\n\ncent_all &lt;- lapply(c(\"d_weighted\", \"d\", \"time_weighted\", \"time\"), \\(x) {\n  \n    df &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = x)\n  \n  tibble(edge_id = df$edge_id, cent = df$centrality) |&gt;\n    rename_with(.cols = \"cent\", .fn = \\(.y) {\n      paste(.y, x, sep = \"_\")\n    })\n  }) |&gt;\n  plyr::join_all(by = \"edge_id\",type = \"full\")\n\n\n\n\n\nigraph_bog &lt;- bog_contracted |&gt; \n  dodgr_to_igraph()\n\nLoading required namespace: igraph\n\n\n\ncent_all_ig &lt;- cent_all |&gt; \n  left_join(\n    tibble(edge_id = bog_contracted$edge_id,\n       cent_ig_d = igraph::edge_betweenness(graph = igraph_bog,\n                                            weights = bog_contracted$d),\n       cent_ig_d_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                     weights = bog_contracted$d_weighted),\n       cent_ig_time = igraph::edge_betweenness(graph = igraph_bog,weights = bog_contracted$time),\n       cent_ig_time_weighted = igraph::edge_betweenness(graph = igraph_bog,\n                                                        weights = bog_contracted$time_weighted)\n       ),\n    by = \"edge_id\")\n\n\nsummary(cent_all_ig[,2:9])\n\n cent_d_weighted       cent_d        cent_time_weighted   cent_time      \n Min.   :      0   Min.   :      0   Min.   :      0    Min.   :      0  \n 1st Qu.:   4331   1st Qu.:   4330   1st Qu.:   4329    1st Qu.:   4329  \n Median :  16606   Median :  16482   Median :  14694    Median :  14569  \n Mean   :  79033   Mean   :  78951   Mean   :  80780    Mean   :  80660  \n 3rd Qu.:  71441   3rd Qu.:  70966   3rd Qu.:  53128    3rd Qu.:  52827  \n Max.   :1405281   Max.   :1408773   Max.   :2075299    Max.   :2069854  \n   cent_ig_d       cent_ig_d_weighted  cent_ig_time     cent_ig_time_weighted\n Min.   :      0   Min.   :      0    Min.   :      0   Min.   :      0      \n 1st Qu.:   4330   1st Qu.:   4331    1st Qu.:   4329   1st Qu.:   4329      \n Median :  16482   Median :  16606    Median :  14569   Median :  14694      \n Mean   :  78951   Mean   :  79033    Mean   :  80660   Mean   :  80780      \n 3rd Qu.:  70966   3rd Qu.:  71441    3rd Qu.:  52827   3rd Qu.:  53128      \n Max.   :1408773   Max.   :1405281    Max.   :2069854   Max.   :2075299      \n\n\nCheck if igraph results are identical to the ones obtained with dodgr\n\nwith(cent_all_ig,identical(cent_ig_d_weighted,cent_d_weighted))\n\n[1] TRUE\n\nwith(cent_all_ig,identical(cent_ig_time_weighted,cent_time_weighted))\n\n[1] TRUE\n\n\n\npairs(cent_all_ig[,2:9])\n\n\n\n\n\n\n\n\nJoining the results to the sf object\n\nsf_net &lt;- graph_bogota |&gt;\n  dodgr_to_sf() |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\n\n\n\nsf_net |&gt;  \n  select(cent_d_weighted:cent_time) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot()+\n  geom_sf(aes(col = log(cent),linewidth = cent+1))+\n  facet_wrap(type~.)+\n  scale_color_viridis_c(\n    # direction = -1\n    )+\n  scale_linewidth_continuous(range = c(0.05,0.5),\n                             transform = scales::transform_boxcox(p = 2))+\n  theme_void()+\n  guides(linewidth = \"none\",)\n\n\n\n\n\n\n\n\nVisualising differences\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  ggplot(aes(cent))+\n  geom_histogram()+\n  facet_wrap(type~.)+\n  scale_x_continuous(limits = c(-1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2582 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nInspecting significant changes\n\nsf_net |&gt; \n  transmute(across(cent_d_weighted:cent_time,\n                   \\(x) (x - cent_d)/cent_d)) |&gt; \n  pivot_longer(names_to = \"type\",names_prefix = \"cent_\",\n               cols = -geometry,\n               values_to = \"cent\") |&gt; \n  mutate(cent = case_when(abs(cent)&lt;0.1~0,\n                          cent &gt; 0.6 ~ 1,\n                          -cent &gt; 0.6 ~ -1,\n                          is.na(cent)~NA)) |&gt; \n  ggplot()+\n  geom_sf(aes(col = cent))+\n  facet_wrap(type~.)+\n  scale_color_distiller(palette = \"Spectral\", direction = 1)+\n  # scale_linewidth_continuous(range = c(0.05,0.5),\n  #                            transform = scales::transform_boxcox(p = 2))+\n  theme_void()\n\n\n\n\n\n\n\n\nIdentifying the links with differences &gt; 0 in d and weighted d\n\nsf_net |&gt; \n  mutate(diff_check = (d-d_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nSame for time\n\nsf_net |&gt; \n  mutate(diff_check = (time-time_weighted)==0) |&gt; \n  ggplot(aes(col = diff_check))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nA course assumption is that people decide the route they will use based on the ETA, also differences in speed limits can only captured if time is used. So weighted time will be used.\n\n\n\n\nlibrary(sfnetworks)\n\n\nsf_net$time_weighted[sf_net$time_weighted==0] &lt;- (3.6*sf_net$d_weighted[sf_net$time_weighted==0])/case_when(\n      sf_net$highway[sf_net$time_weighted==0] %in% c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~60,\n      T~30)\n\nbog_sfnet &lt;- sf_net |&gt; \n  as_sfnetwork(directed = TRUE)\n\n\nbog_sfnet &lt;- bog_sfnet |&gt;\n  activate(\"edges\") |&gt; \n  mutate(cent_sfn_time_weighted = tidygraph::centrality_edge_betweenness(\n    weights = time_weighted,\n    directed = T))\n\n\nbog_sfnet |&gt; \n  st_as_sf(\"edges\") |&gt; \nggplot()+\n  geom_sf(aes(col = cent_time_weighted))+\n  theme_void()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  select(cent_time_weighted,cent_sfn_time_weighted) |&gt; \n  pairs()\n\n\n\n\n\n\n\n\n\nbog_sfnet |&gt; \n  activate(\"edges\") |&gt;\n  data.frame() |&gt; \n  with(identical(cent_time_weighted,cent_sfn_time_weighted))\n\n[1] TRUE\n\n\n\np1 = st_geometry(bog_sfnet, \"nodes\")[2401]\nst_crs(p1) = st_crs(bog_sfnet)\np2 = st_geometry(bog_sfnet, \"nodes\")[1407]\np3 = st_geometry(bog_sfnet, \"nodes\")[1509]\np4 = st_geometry(bog_sfnet, \"nodes\")[1840]\nst_crs(p3) = st_crs(bog_sfnet)\n\npaths = st_network_paths(bog_sfnet, from = p1, to = c(p2,p3,p4), weights = \"time_weighted\")\n\n\nplot_path = function(node_path) {\n  bog_sfnet %&gt;%\n    activate(\"nodes\") %&gt;%\n    slice(node_path) %&gt;%\n    plot(cex = 1.5, lwd = 1.5, add = TRUE)\n}\n\ncolors = sf.colors(4, categorical = TRUE)\n\nplot(bog_sfnet, col = \"grey\")\npaths %&gt;%\n  pull(node_paths) %&gt;%\n  walk(plot_path)\nplot(c(p1, p2, p3,p4), col = colors, pch = 8, cex = 2, lwd = 2, add = TRUE)"
  },
  {
    "objectID": "A4_Analysis.html",
    "href": "A4_Analysis.html",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "Joining the WWD reports\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\n\n\nWWD reports\n\nwwd_sf &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\n\nReading layer `wwd_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/wwd_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6915 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.24842 ymin: 4.475113 xmax: -74.01945 ymax: 4.819386\nGeodetic CRS:  WGS 84\n\n\n\nsf_net_exp &lt;- st_read(\"sf_network/full_sf_network_cent_results.gpkg\") |&gt; st_transform(3116)\n\nReading layer `full_sf_network_cent_results' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/full_sf_network_cent_results.gpkg' \n  using driver `GPKG'\nSimple feature collection with 242202 features and 25 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  WGS 84\n\n\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt;st_transform(3116)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : GDAL\nMessage 1: Non-conformant content for record 1 in column\nFECHA_ACTO_ADMINISTRATIVO, 2021-12-29T00:00:00.0Z, successfully parsed\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nsf_net &lt;- sf_net_exp[urban_perimeter,]\n\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) \n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- subset_net |&gt;\n  st_drop_geometry() |&gt;\n  # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt; \n  summarise(across(diff:reldiff,\n                   list(min=min, max=max, avg = mean)),\n            .by = pair_id)\n\nA simplified version of the sf object is produced extracting the first element of each pair, we discard will the columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # Filtering only the links that were inverted during the network creation and standard links\n  filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n         .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(20)\n\nA subset of the reports during peak hour +/- 2 hours\n\nsubset_wwd &lt;- (wwd_sf |&gt;\n                 filter(abs(hour - 18)&lt;=2) |&gt; st_transform(3116))[subset_buffer,]|&gt; \n  filter(year==2019)\n\n\n\nFinding the closest element of the network\n\nsubset_wwd$near_index &lt;- st_nearest_feature(subset_wwd,simpl_network_sf)\nsubset_wwd$pair_id &lt;- simpl_network_sf$pair_id[subset_wwd$near_index]\n\n\nmodel_data &lt;- simpl_network_sf |&gt; \n  filter(oneway) |&gt;\n  left_join(summary_pairs,by = \"pair_id\") |&gt; \n  mutate(wwd_bool = pair_id %in% subset_wwd$pair_id) \n\nA jitter plot to explore the distribution\n\nmodel_data |&gt; \n  ggplot(aes(x = (reldiff_max),y = wwd_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1))+\n  labs(x = \"Average relative change in BC\",\n       y = \"Wrong-way driving report\")\n\nWarning: Removed 2342 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n  ggplot(aes(x = (reldiff_max),fill = wwd_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2342 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\nmean_bl &lt;- mean(subset_net$cent_bl[subset_net$cent_bl&gt;0],na.rm = T)\nmean_cong &lt;- mean(subset_net$cent_cong[subset_net$cent_cong&gt;0],na.rm = T)\n\nlibrary(extrafont)\n\nRegistering fonts with R\n\nhist_centrality &lt;- subset_net |&gt;\n  st_drop_geometry() |&gt; \n  filter(oneway,cent_bl+cent_cong&gt;0) |&gt;\n  select(cent_bl,cent_cong) |&gt; \n  pivot_longer(cols = cent_bl:cent_cong) |&gt; \n  ggplot(aes(x = value+1,fill = name))+\n  geom_histogram(alpha = 0.3,col = \"white\",position = \"identity\")+\n  geom_vline(xintercept = mean_bl,col = \"dodgerblue\",alpha = 0.8,linewidth = 1.5)+\n  geom_vline(xintercept = mean_cong,col = \"firebrick\",alpha = 0.8,linewidth = 1.5)+\n  scale_x_log10(breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n    labels = scales::trans_format(\"log10\", scales::math_format(10^.x)))+\n  scale_y_continuous(labels = scales::label_comma())+\n  theme_minimal()+\n  labs(x = \"Centrality\",\n       fill = \"Network\",\n       title = \"Centrality distribution\",\n       subtitle = \"Only oneway residential links\")+\n  scale_fill_manual(values = c(\"dodgerblue\",\"firebrick\"),labels = c(\"Free-flow\",\"congested\"))+\n  theme(text = element_text(family = \"Roboto Light\"),\n        legend.position = \"top\")\n\nggsave(hist_centrality,filename = \"histogram_centralities.png\",dpi = 320,units = \"mm\",height = 110,width = 170)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\ntest1 &lt;- model_data |&gt; group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(wwd_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data. Unfortunately, the false positives do have a significant impact.\n\nref_plot &lt;- test1 |&gt;\n  mutate(wwd_bool = if_else(wwd_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = wwd_bool))+\n    geom_point(alpha = 0.1,col = \"dodgerblue3\")+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),\n              se = F,\n              col = \"goldenrod\")+\n  labs(title = \"Naive Logistic Regression\",\n       subtitle = \"Only one-way residential roads\",\n       y = \"Probability of report\",\n       x = \"Mean Relative Change in BC\")+\n  scale_x_continuous(labels = scales::label_percent(1))+\n  theme(text = element_text(family = \"Roboto Light\"),\n        legend.position = \"top\")\n\nWarning in geom_smooth(method = \"glm\", formula = y ~ x, family = \"binomial\", :\nIgnoring unknown parameters: `family`\n\nggsave(ref_plot,filename = \"reg_naive.png\",dpi = 320,units = \"mm\",height = 110,width = 170)\n\nWarning: Removed 758 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 758 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "A2_graph.html",
    "href": "A2_graph.html",
    "title": "Baseline Network Graph",
    "section": "",
    "text": "Libraries\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n       sf tidyverse \n     TRUE      TRUE \n\nrequire(dodgr)\n\nUsing the networks extracted before. The following code loads the sf objects.\n\nsf_bogota_2019 &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),layer = \"network_2019\")\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\n\nIn this step, a graph representation of the 2019 road network of Bogotá is processed using the dodgr package. Posted speed limits are used as standard speeds for the road links (edges). The corresponding weighting profile has been saved in the bogota_wp.json file. To speed up the calculation, a distance threshold is applied based on an maximum error of 0.001.\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"))\n\n# graph_bogota |&gt;\n#   dodgr_contract_graph() |&gt;\n#   estimate_centrality_threshold(tolerance = 1e-3)\n# converged on distance threshold of 14000\n\ngraph_bogota_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality(dist_threshold = 1.4e4,\n                   column = \"time_weighted\")\n\nsf_network &lt;- graph_bogota |&gt; \n    dodgr_to_sf ()\n\nExporting the results\n\nsf_net_cent &lt;- sf_network |&gt;\n  left_join(\n    tibble(edge_id = graph_bogota_centrality$edge_id,\n           centrality = graph_bogota_centrality$centrality),\n    by = \"edge_id\")\n\nst_write(sf_net_cent,\"sf_network/bogota_osm_network_cent.gpkg\",delete_dsn = F,delete_layer = T,layer = \"bog_cent_2019\")\n\n\n\nTo clip the network for visualisation, we will load the file with urban perimeter\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt; \n  st_transform(4326)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : GDAL\nMessage 1: Non-conformant content for record 1 in column\nFECHA_ACTO_ADMINISTRATIVO, 2021-12-29T00:00:00.0Z, successfully parsed\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS"
  },
  {
    "objectID": "A2_graph.html#building-the-graph",
    "href": "A2_graph.html#building-the-graph",
    "title": "Baseline Network Graph",
    "section": "",
    "text": "In this step, a graph representation of the 2019 road network of Bogotá is processed using the dodgr package. Posted speed limits are used as standard speeds for the road links (edges). The corresponding weighting profile has been saved in the bogota_wp.json file. To speed up the calculation, a distance threshold is applied based on an maximum error of 0.001.\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"bogota_wp.json\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols = c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\"))\n\n# graph_bogota |&gt;\n#   dodgr_contract_graph() |&gt;\n#   estimate_centrality_threshold(tolerance = 1e-3)\n# converged on distance threshold of 14000\n\ngraph_bogota_centrality &lt;- graph_bogota |&gt; \n  dodgr_deduplicate_graph() |&gt;\n  dodgr_contract_graph() |&gt;\n  dodgr_centrality(dist_threshold = 1.4e4,\n                   column = \"time_weighted\")\n\nsf_network &lt;- graph_bogota |&gt; \n    dodgr_to_sf ()\n\nExporting the results\n\nsf_net_cent &lt;- sf_network |&gt;\n  left_join(\n    tibble(edge_id = graph_bogota_centrality$edge_id,\n           centrality = graph_bogota_centrality$centrality),\n    by = \"edge_id\")\n\nst_write(sf_net_cent,\"sf_network/bogota_osm_network_cent.gpkg\",delete_dsn = F,delete_layer = T,layer = \"bog_cent_2019\")\n\n\n\nTo clip the network for visualisation, we will load the file with urban perimeter\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\") |&gt; \n  st_transform(4326)\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : GDAL\nMessage 1: Non-conformant content for record 1 in column\nFECHA_ACTO_ADMINISTRATIVO, 2021-12-29T00:00:00.0Z, successfully parsed\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rat-runs",
    "section": "",
    "text": "Rat-runs in Bogota\n\nNetwork data\n\nOSM\nIDECA malla vial integral\n\n\n\nInfractions data\n\n\nAssumptions:\n\nRecurring wrong-way infraction reports in residential streets is a result of rat-running\n\n\n\nHypothesis:"
  },
  {
    "objectID": "A1_network.html",
    "href": "A1_network.html",
    "title": "Extracting OSM networks",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"osmextract\",\n    \"rvest\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n        sf  tidyverse osmextract      rvest \n      TRUE       TRUE       TRUE       TRUE \n\n\nFor this study, we will use OpenStreetMap to obtain the road network data. First, we will download a spatial data file with the urban perimeter of Bogotá from Datos Abiertos de Bogotá (Bogotá’s Open Data platform).\n\ndir.create(\"raw_data\",showWarnings = F)\n\nif(!file.exists(file.path(\"raw_data\", \"perimetrourbano.gpkg\"))) {\n  u &lt;- \"https://datosabiertos.bogota.gov.co/dataset/12a704ee-e5bb-4c5d-bad6-a5069d12f90a/resource/bfc61e3c-fa58-4fe7-9581-7ead66c494cb/download/perimetrourbano.gpkg\"\n  download.file(u, file.path(\"raw_data\", basename(u)), mode = \"wb\")\n}\n\nurban_perimeter &lt;- st_read(\"raw_data/perimetrourbano.gpkg\")\n\nReading layer `PerimetroUrbano' from data source \n  `/home/juan/P1_ratruns_analysis/raw_data/perimetrourbano.gpkg' \n  using driver `GPKG'\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : GDAL\nMessage 1: Non-conformant content for record 1 in column\nFECHA_ACTO_ADMINISTRATIVO, 2021-12-29T00:00:00.0Z, successfully parsed\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.22358 ymin: 4.468614 xmax: -74.01206 ymax: 4.830661\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nWe will use the osmextract package to get OSM data in R. Please note that we will be using previous versions of the OSM data, as the network might have changed in recent years. For example, some sections of the roads are currently closed (2024) due to the works for the first metro line in Bogotá. We will use OSM networks for end of 2019 (January 1st, 2020) and 2020 (January 1st, 2021).\nFirst, we produce a boundary box for an area covering the urban perimeter and the neighbouring municipalities. This is done by producing a buffer of 20 km around Bogotá.\n\nbbox_bogota &lt;- urban_perimeter |&gt;\n  st_buffer(dist = 20e3) |&gt; \n  st_bbox() |&gt;\n  st_as_sfc() |&gt;\n  st_transform(crs = 4326)\n\nWe obtain the url of the pbf file in Geofabrik which contains the area we are interested in.\n\nbog_match &lt;- oe_match(bbox_bogota,provider = \"geofabrik\")\n\nThe input place was matched with Colombia. \n\n\nThe following code obtains the names of the files with the older versions of the OSM data and downloads the 2019 and 2020 pbf files.\n\nu &lt;- dirname(bog_match$url)\nf &lt;- basename(bog_match$url)\n\nid_files &lt;- gsub(\"latest\\\\.osm\\\\.pbf\",replacement = \"\",f)\n\nfiles_table &lt;- (rvest::read_html(u) |&gt; html_table())[[1]]\n\navailable_versions &lt;- files_table$Name[grep(paste0(id_files,\"\\\\d{6}\\\\.osm\\\\.pbf$\"),\n                                            files_table$Name)]\n\nnet_options &lt;- osmextract:::load_options_driving(NA_character_)\n\nnet_options$extra_tags &lt;- c(\"oneway\",\"lanes\",\"surface\",\"maxspeed\",net_options$extra_tags)\n\nnet_old_200101 &lt;- do.call(oe_read,\n                          c(file_path = paste0(u,\"/\",available_versions[7]),\n                            net_options[2:4]\n                     )\n                   )\n\nDownloading the OSM extract:\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nFile downloaded!\n\n\nStarting with the vectortranslate operations on the input file!\n\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\n\nFinished the vectortranslate operations on the input file!\n\n\nReading layer `lines' from data source \n  `/tmp/RtmpJ1Hkoa/geofabrik_colombia-200101.gpkg' using driver `GPKG'\nSimple feature collection with 461429 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -81.73539 ymin: -4.259884 xmax: -66.83441 ymax: 13.38586\nGeodetic CRS:  WGS 84\n\nnet_old_210101 &lt;- do.call(oe_read,\n                          c(file_path = paste0(u,\"/\",available_versions[8]),\n                            net_options[2:4]\n                     )\n                   )\n\nDownloading the OSM extract:\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nFile downloaded!\nStarting with the vectortranslate operations on the input file!\n\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\n\nFinished the vectortranslate operations on the input file!\n\n\nReading layer `lines' from data source \n  `/tmp/RtmpJ1Hkoa/geofabrik_colombia-210101.gpkg' using driver `GPKG'\nSimple feature collection with 650682 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -81.73539 ymin: -4.259656 xmax: -66.83441 ymax: 13.38586\nGeodetic CRS:  WGS 84\n\n\nOnce we have downloaded the data. We can clean and clip the network by using the boundary box we produced and by filtering only relevant road links (see the road_types vector).\n\nroad_types &lt;- c(\"tertiary\"       ,\n  \"residential\"    ,\n  \"primary_link\"   ,\n  \"primary\"      ,\n  \"secondary\"      ,\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  # \"service\"      ,\n  \"secondary_link\" ,\n  \"unclassified\"   ,\n  \"tertiary_link\"  \n  # \"living_street\"\n  # \"track\"          ,\n  # \"busway\"         ,\n  # \"raceway\"\n)\n\nosm_bogota_200101 &lt;- net_old_200101[bbox_bogota,] |&gt;\n  filter(highway %in% road_types) |&gt; st_transform(st_crs(urban_perimeter))\n\nosm_bogota_210101 &lt;- net_old_210101[bbox_bogota,] |&gt;\n  filter(highway %in% road_types) |&gt; st_transform(st_crs(urban_perimeter))\n\nrm(net_old_200101,net_old_210101)\n\n\n\nFinally, we save the sf objects as GeoPackages.\n\ndir.create(\"sf_network\",showWarnings = F)\nst_write(osm_bogota_200101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2019\",\n         delete_layer = T)\n\nDeleting layer `network_2019' using driver `GPKG'\nWriting layer `network_2019' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 54531 features with 15 fields and geometry type Line String.\n\nst_write(osm_bogota_210101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2020\",\n         delete_layer = T)\n\nDeleting layer `network_2020' using driver `GPKG'\nWriting layer `network_2020' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 62166 features with 15 fields and geometry type Line String."
  },
  {
    "objectID": "A1_network.html#save-network-in-sf-format",
    "href": "A1_network.html#save-network-in-sf-format",
    "title": "Extracting OSM networks",
    "section": "",
    "text": "Finally, we save the sf objects as GeoPackages.\n\ndir.create(\"sf_network\",showWarnings = F)\nst_write(osm_bogota_200101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2019\",\n         delete_layer = T)\n\nDeleting layer `network_2019' using driver `GPKG'\nWriting layer `network_2019' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 54531 features with 15 fields and geometry type Line String.\n\nst_write(osm_bogota_210101,\n         file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n         delete_dsn = F,\n         layer = \"network_2020\",\n         delete_layer = T)\n\nDeleting layer `network_2020' using driver `GPKG'\nWriting layer `network_2020' to data source \n  `sf_network/bogota_osm_network.gpkg' using driver `GPKG'\nWriting 62166 features with 15 fields and geometry type Line String."
  },
  {
    "objectID": "A3_Congested_graph.html",
    "href": "A3_Congested_graph.html",
    "title": "Congested Network",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap \n       TRUE        TRUE        TRUE        TRUE \n\nrequire(dodgr)\npackageVersion (\"dodgr\")\n\n[1] '0.4.1.37'\n\n\n\n\n\nsf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019 &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)\n\n\n\n\n\ndodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\",dist_threshold = 900)\n\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nRows: 110880 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): day_type\ndbl (11): TID, year, hour, d_min_speed, d_q1_speed, d_median_speed, d_mean_s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe identify the hour that showed the lowest speeds in average\n\nmin_speed_key &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) |&gt; \n  slice_min(mean_norm_speed,by = c(year,day_type)) |&gt; \n  filter(year == 2019,day_type == \"weekday\") |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\",dist_threshold = 900)\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(\n  edge_id = bog_centrality$edge_id,\n  cent_bl = bog_centrality$centrality) |&gt;\n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt;\n  mutate(diff = cent_cong - cent_bl,\n         reldiff = diff / (0.5 * (cent_bl + cent_cong)))\n\n\nsf_net_cent &lt;- sf_net |&gt; \n  left_join(cent_all,by = \"edge_id\")\n\n\n\n\n\n\nst_write(sf_net_cent,\"sf_network/full_sf_network_cent_results.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network_cent_results.gpkg' using driver `GPKG'\nWriting layer `full_sf_network_cent_results' to data source \n  `sf_network/full_sf_network_cent_results.gpkg' using driver `GPKG'\nWriting 242202 features with 25 fields and geometry type Line String."
  },
  {
    "objectID": "A3_Congested_graph.html#loading-network",
    "href": "A3_Congested_graph.html#loading-network",
    "title": "Congested Network",
    "section": "",
    "text": "sf_bogota_2019_raw &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\"),\n                              layer = \"network_2019\") |&gt;\n  mutate(way_speed = case_when(highway %in%\n                                 c(\"trunk_link\",\"primary_link\",\"primary\",\"trunk\")~\"road_60\",\n                               TRUE ~ \"road_30\")) |&gt; \n  rename(roadclass = highway) |&gt;\n  # mutate(oneway.raw = oneway,\n  #        oneway = if_else(roadclass %in% c(\"residential\",\"unclassified\"),\n  #                         \"no\",\n  #                         oneway)) |&gt; \n  st_transform(4326)\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\nInspecting the values of the oneway tag in residential and unclassified roads\n\nsf_bogota_2019_raw |&gt; filter(roadclass %in% c(\"residential\",\"unclassified\")) |&gt; pull(oneway) |&gt; unique()\n\n[1] NA           \"yes\"        \"no\"         \"reversible\" \"-1\"        \n[6] \"nolt\"      \n\n\nWe will reverse those links to allow the vehicles to travel in the wrong direction\n\noneway_minor_rev &lt;- sf_bogota_2019_raw |&gt; \n  filter(roadclass %in% c(\"residential\",\"unclassified\"),\n         str_detect(pattern = \"yes\",oneway)) |&gt; \n  st_reverse() |&gt; \n  mutate(osm_id = paste0(osm_id,\"r\"),\n         way_speed = \"road_10\")\n\n\nsf_bogota_2019 &lt;- bind_rows(sf_bogota_2019_raw,\n                                 oneway_minor_rev)"
  },
  {
    "objectID": "A3_Congested_graph.html#baseline-graph-building",
    "href": "A3_Congested_graph.html#baseline-graph-building",
    "title": "Congested Network",
    "section": "",
    "text": "dodgr::clear_dodgr_cache()\n\ngraph_bogota &lt;- weight_streetnet(sf_bogota_2019,\n                                 left_side = F,\n                                 wt_profile_file = \"custom_wp_speeds.json\",\n                                 type_col = \"way_speed\",\n                                 wt_profile = \"motorcar\",\n                                 keep_cols =\n                                   c(\"way_speed\",\"oneway\",\"lanes\",\"surface\",\"maxspeed\",\"roadclass\"),\n                                 turn_penalty = F)\n\nbog_contracted &lt;- graph_bogota |&gt; \n  dodgr_contract_graph()\n\n\ndodgr::clear_dodgr_cache()\n\nbog_contracted$time_weighted &lt;- 3.6*bog_contracted$d_weighted/as.numeric(str_extract(bog_contracted$highway,\"\\\\d{1,2}$\"))\n\nCalculating the centrality\n\nbog_centrality &lt;- bog_contracted |&gt;\n      dodgr_centrality(column = \"time_weighted\",dist_threshold = 900)\n\n\nsf_speed &lt;- st_read(\"sf_network/sf_speed_network.gpkg\") |&gt; st_transform(3116)\n\nReading layer `sf_speed_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/sf_speed_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1267 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.18353 ymin: 5.684342e-14 xmax: 1 ymax: 4.764635\nGeodetic CRS:  MAGNA-SIRGAS\n\nsf_speed_clean &lt;- sf_speed[sf_bogota_2019 |&gt;\n                             st_transform(3116) |&gt;\n                             st_union() |&gt;\n                             st_convex_hull(),]\n\n\nsf_net &lt;- graph_bogota |&gt; \n  dodgr_to_sf() \n\n\nroad_types &lt;- rev(c(\n  \"trunk\"          ,\n  \"trunk_link\"     ,\n  \"primary\"      ,\n  \"primary_link\"   ,\n  \"secondary\"      ,\n  \"secondary_link\" ,\n  \"tertiary\"       ,\n  \"tertiary_link\"  ,\n  \"unclassified\"   ,\n  \"residential\"    \n))\n\nsf_bog_major &lt;- sf_net |&gt;\n  mutate(roadclass = factor(roadclass,\n                          levels = road_types,\n                          ordered = T)) |&gt; \n  filter(as.integer(roadclass)&gt;2,str_detect(roadclass,\"link\",negate = T)) |&gt; \n  st_transform(3116)\n\nExtracting bearings and creating a buffer to produce a correspondence\n\nsf_speed_clean$s.bearing &lt;- stplanr::line_bearing(sf_speed_clean)\n\nspeed_buffer &lt;- sf_speed_clean  |&gt;  st_buffer(100,endCapStyle = \"FLAT\") \n\n\nspeed_corresp &lt;- st_intersects(speed_buffer,sf_bog_major)\n\nTID_to_edge_id &lt;- do.call(bind_rows,\n        lapply(seq_along(speed_corresp),\n               \\(i) {\n                 \n                 x &lt;- speed_corresp[[i]]\n                 \n                 subset_net &lt;- sf_bog_major[x, ]\n                 \n                 ref_bearing &lt;- speed_buffer$s.bearing[[i]]\n                 ref_TID &lt;- speed_buffer$TID[[i]]\n                 \n                 subset_net$bearing &lt;- stplanr::line_bearing(subset_net)\n                 \n                 sf_ids &lt;- subset_net |&gt;\n                   mutate(bearing_check = round(abs(bearing - ref_bearing))&lt;=15) |&gt;\n                   filter(bearing_check) |&gt;\n                   pull(edge_id)\n                 \n                 tibble(TID = ref_TID, edge_id = sf_ids)\n                 }\n               )\n        )\n\nIt is possible that some links in the road network are found to be linked to multiple links in the speed network, so we will resolve such situation. First, we identify the overlaps\n\noverlap_buffer &lt;- TID_to_edge_id |&gt; \n  unique() |&gt; \n  count(edge_id) |&gt; \n  arrange(-n) |&gt; \n  filter(n&gt;1)\n\n\nno_match_speed &lt;- sf_speed_clean |&gt;\n  anti_join(TID_to_edge_id |&gt;\n              select(TID) |&gt;\n              unique(),\n            by = \"TID\")\n\nnrow(no_match_speed)\n\n[1] 3\n\n\nThe following code loads the data\n\nspeed_data &lt;- read_csv(\"sf_network/summary_speeds.csv\")\n\nRows: 110880 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): day_type\ndbl (11): TID, year, hour, d_min_speed, d_q1_speed, d_median_speed, d_mean_s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe identify the hour that showed the lowest speeds in average\n\nmin_speed_key &lt;- speed_data |&gt;\n  summarise(mean_norm_speed = mean(d_norm_speed),.by = c(year,day_type,hour)) |&gt; \n  slice_min(mean_norm_speed,by = c(year,day_type)) |&gt; \n  filter(year == 2019,day_type == \"weekday\") |&gt; \n  select(-mean_norm_speed)\n\nWe will extract the observed speed for the hour we just identified\n\nspeed_tbl &lt;- speed_data |&gt;\n  semi_join(min_speed_key,\n            by = join_by(year,day_type,hour)) |&gt; \n  select(TID,d_mean_speed)\n\nWe join the speed data to the correspondence we produced before\n\nobs_speeds_edges &lt;- TID_to_edge_id |&gt;\n  left_join(speed_tbl,by = \"TID\") |&gt; \n  summarise(obs_speed = mean(d_mean_speed),.by = edge_id)\n\nUsing the observed speed we recalculate the time_weighted.\n\nbog_contr_adjusted &lt;- bog_contracted\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$obs_speed &lt;- tibble(edge_id = bog_contr_adjusted$edge_id) |&gt; \n  left_join(obs_speeds_edges,by = \"edge_id\") |&gt; \n  pull(obs_speed)\ndodgr::clear_dodgr_cache()\nbog_contr_adjusted$time_weighted[!is.na(bog_contr_adjusted$obs_speed)] &lt;- (3.6*bog_contr_adjusted$d_weighted[!is.na(bog_contr_adjusted$obs_speed)])/bog_contr_adjusted$obs_speed[!is.na(bog_contr_adjusted$obs_speed)]\n\nWe calculate the centrality for the congested graph.\n\ncongested_centrality &lt;- bog_contr_adjusted |&gt;\n      dodgr_centrality(column = \"time_weighted\",dist_threshold = 900)\n\n\n\nWe consolidate the values of free-flow network and congested network into a single dataset\n\ncent_all &lt;- tibble(\n  edge_id = bog_centrality$edge_id,\n  cent_bl = bog_centrality$centrality) |&gt;\n  left_join(\n    tibble(edge_id = congested_centrality$edge_id,\n           cent_cong = congested_centrality$centrality),\n    by = \"edge_id\"\n  ) |&gt;\n  mutate(diff = cent_cong - cent_bl,\n         reldiff = diff / (0.5 * (cent_bl + cent_cong)))\n\n\nsf_net_cent &lt;- sf_net |&gt; \n  left_join(cent_all,by = \"edge_id\")"
  },
  {
    "objectID": "A3_Congested_graph.html#saving-results-for-further-analysis",
    "href": "A3_Congested_graph.html#saving-results-for-further-analysis",
    "title": "Congested Network",
    "section": "",
    "text": "st_write(sf_net_cent,\"sf_network/full_sf_network_cent_results.gpkg\",delete_dsn = T)\n\nDeleting source `sf_network/full_sf_network_cent_results.gpkg' using driver `GPKG'\nWriting layer `full_sf_network_cent_results' to data source \n  `sf_network/full_sf_network_cent_results.gpkg' using driver `GPKG'\nWriting 242202 features with 25 fields and geometry type Line String."
  },
  {
    "objectID": "C1_infractions.html",
    "href": "C1_infractions.html",
    "title": "Ticket Data",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"tmap\",\n    \"webshot2\"\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n       sf tidyverse      tmap  webshot2 \n     TRUE      TRUE      TRUE      TRUE \n\ntmap_mode(\"plot\")\n\n\n\n\nif(!dir.exists(\"raw_data/bogota\")) {\n  if (!file.exists(\"raw_data/bogota_data.zip\")) {\n    options(timeout = 180)\n    download.file(\n      \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/bogota_data.zip\",\n      destfile = \"raw_data/bogota_data.zip\",\n      mode = \"wb\"\n    )\n    unzip(\"raw_data/bogota_data.zip\", exdir = \"raw_data\")\n  }\n}\n\n\n\n\n\nreports2019 &lt;- read_csv(\"raw_data/bogota/Comparendos_2019_Bogota_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          OBJECTID = col_double(),\n                          NUM_COMPARENDO = col_character(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURRENCIA = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETECCION = col_character(),\n                          CLASE_VEHICULO = col_character(),\n                          TIPO_SERVICIO = col_character(), \n                          INFRACCION = col_character(),\n                          DES_INFRACCION = col_character(),\n                          LOCALIDAD = col_character(),\n                          MUNICIPIO = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double(),\n                          GlobalID = col_character()\n                        ))\n\nreports2020 &lt;- read_csv(\"raw_data/bogota/Comparendos_DEI_2020_Bogot%C3%A1_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          FID = col_double(),\n                          OBJECTID = col_double(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURR = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETE = col_character(),\n                          CLASE_VEHI = col_character(),\n                          TIPO_SERVI = col_character(),\n                          INFRACCION = col_character(),\n                          DES_INFRAC = col_character(),\n                          MUNICIPIO = col_character(),\n                          PAIS = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double()))\n\nA list of the codes with codes of offences related to vehicles circulating.\n\nselected_codes &lt;- read.csv(\"list_infractions.csv\")$INFRACCION\n\n\n\n\n\nreports2019 |&gt; \n  count(INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.90)) |&gt; \n  ggplot(aes(y = fct_reorder(INFRACCION,n),\n             x = n))+\n  geom_col()+\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\ncount_infractions &lt;- reports2019 |&gt; \n  count(INFRACCION,DES_INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.70)) |&gt; \n  arrange(-n)\n\n\n\n\nCreating an sf object with the reports for driving in the wrong-way\n\nwrong_way_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nwrong_way_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\n\nmanual_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nmanual_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\n\n\n\nWe will select only relevant columns and also classify the records by type of day. For that, we will use the bank holiday list we used for the speed data processing.\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nwwd_2019_clean &lt;- wrong_way_2019_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\nwwd_2020_clean &lt;- wrong_way_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\nAll manual tickets\n\nmanual_2019_clean &lt;-  manual_2019_sf|&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\nmanual_2020_clean &lt;- manual_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\nAnd consolidating the two datasets\n\nall_wwd_reports &lt;- bind_rows(wwd_2019_clean,wwd_2020_clean) |&gt; \n  mutate(veh_class = str_to_lower(veh_class))\n\nall_manual_reports &lt;- bind_rows(manual_2019_clean,manual_2020_clean) |&gt; \n  mutate(veh_class = str_to_lower(veh_class))\n\n\n\n\n\n\n\nggplot(all_wwd_reports,aes(col = factor(year)))+\n         geom_sf(alpha = 0.2,size = 0.5)+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nA daily profile\n\nggplot(all_wwd_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nA check of the reports per day\n\nexp_dates_count &lt;- all_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_wwd_reports$date_time)),\n                      max(date(all_wwd_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nA quick check of the timeline reveals gaps in the reports of 2019. We do not know the causes.\n\nexp_dates_count |&gt; \n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  ggplot(aes(x = date,y = n))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nexp_dates_count |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)\n\n\n\n\n\n\n\n\n\n\nLet us check if there is any patter related to the day of the week\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nThere seems to be a pattern that might be related to the sampling i.e. how the enforcement officers are assigned along the week.\nLet’s compare with other types of infractions reported by officers\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp),offence) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,offence)) |&gt; \n  filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(offence)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nWe can also inspect the median number of tickets per weekday for all offences that are reported by traffic management officers in 2019 and 2020\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp)) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nreports2020 |&gt; \n  select(-DES_INFRAC) |&gt;\n  filter(MEDIO_DETE == \"DISPOSITIVOS EN VÍA\") |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp)) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nIs it a problem with the data, as some months are not reported? Let’s see if the pattern is similar across months\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nreports2020 |&gt; \n  select(-DES_INFRAC) |&gt;\n  filter(MEDIO_DETE == \"DISPOSITIVOS EN VÍA\") |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nVisually, there seems to be a consistent pattern of a low number of reports during friday and saturday, followed by a surge on Sundays.\n\n\n\n\nLet’s assign all reports to the nearest link to explore where the enforcement offices tend to catch the offenders\n\nnet_bog_2019 &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\")) |&gt; st_transform(3116)\n\nMultiple layers are present in data source /home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg, reading layer `network_2019'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nmajor_network &lt;- net_bog_2019 |&gt; filter(!(highway %in% c(\"residential\",\"unclassified\")))\n\n\nmanual_reports_2019 &lt;- reports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\",INFRACCION %in% selected_codes) |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION,\n         LONGITUD,LATITUD) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326) |&gt; \n  st_transform(3116)\n\n\nmanual_reports_2019$near_index &lt;- st_nearest_feature(manual_reports_2019,net_bog_2019)\nmanual_reports_2019$near_major_index &lt;- st_nearest_feature(manual_reports_2019,major_network)\n\n\nmanual_reports_2019$highway &lt;- net_bog_2019$highway[manual_reports_2019$near_index]\nmanual_reports_2019$oneway &lt;- net_bog_2019$oneway[manual_reports_2019$near_index]\n\n\nmanual_reports_2019$dist_to_major &lt;- st_distance(\n  manual_reports_2019,\n  major_network[manual_reports_2019$near_major_index,],\n  by_element = T) |&gt; as.numeric()\n\nType of road where the offence was reported\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt; \n  ggplot(aes(x = fct_reorder(highway,n,.desc = F),y = n))+\n  geom_col()+\n  coord_flip()\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt;\n  mutate(highway = factor(highway,\n                          levels =\n                            c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                          ordered = T)) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt;\n  arrange(highway) |&gt; \n  mutate(perc = n/sum(n)) |&gt;\n  # mutate(prev = cumsum(perc)-perc) |&gt; \n  # select(-n) |&gt; \n  # pivot_longer(-highway) |&gt; \n  ggplot(aes(x = 1,y = perc,fill = highway))+\n  geom_col(position = \"stack\")+\n  # scale_alpha_manual(values = c(1,0.2))+\n  coord_flip()+\n  theme_minimal()+\n  scale_y_continuous(labels = scales::label_percent())\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nMedian daily reports in residential roads\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  filter(highway == \"residential\") |&gt;\n  count(date = date(timestamp),offence) |&gt; \n  summarise(n = median(n),.by = c(offence)) |&gt;\n  slice_max(n,n=15) |&gt; \n  ggplot(aes(x = fct_reorder(offence,n), y = n))+\n  geom_col()+\n  coord_flip()\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nEuclidean distance to major network vs all reports\n\nggplot()+\n  geom_histogram(data = manual_reports_2019,\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_log10()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot()+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.2)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"),offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nnet_points &lt;- net_bog_2019 |&gt;\n  st_cast(\"POINT\") |&gt;\n  slice_head(by = osm_id)\n\nWarning in st_cast.sf(net_bog_2019, \"POINT\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\nnet_points &lt;- net_bog_2019 |&gt; st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nnet_points$near_id &lt;- st_nearest_feature(net_points,major_network)\nnet_points$dist_to_major &lt;- st_distance(net_points,\n                                        major_network[net_points$near_id,],\n                                        by_element = T) |&gt;\n  as.numeric()\n\n\nggplot()+\n  geom_density(data = net_points|&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_density(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt(breaks = c(0,0.1,0.25,.5,1,2.5,5,7.5,10)*1e3)\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(major_network)+\n  tm_lines()+\n  tm_shape(manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"))+\n  tm_dots(\"dist_to_major\")\n\n\n\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt;  \n  count(veh_class,year) |&gt; \n  arrange(-n) |&gt; \n  mutate(n = n/sum(n),.by=year) |&gt; \n  ggplot(aes(x = fct_reorder(veh_class,n,.desc = F),  y = n, fill = factor(year)))+\n  geom_col(position = \"dodge\",)+\n  coord_flip()+\n  scale_y_continuous(labels = scales::label_percent())+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"dodgerblue3\",\"firebrick3\"))\n\n\n\n\n\n\n\n\n\n\n\n\nst_write(all_wwd_reports,\n         dsn = \"sf_network/wwd_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting layer `wwd_clean_sf' to data source \n  `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting 6915 features with 8 fields and geometry type Point.\n\n\n\nst_write(all_manual_reports,\n         dsn = \"sf_network/manualtickets_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting layer `manualtickets_clean_sf' to data source \n  `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting 455598 features with 9 fields and geometry type Point."
  },
  {
    "objectID": "C1_infractions.html#download-data",
    "href": "C1_infractions.html#download-data",
    "title": "Ticket Data",
    "section": "",
    "text": "if(!dir.exists(\"raw_data/bogota\")) {\n  if (!file.exists(\"raw_data/bogota_data.zip\")) {\n    options(timeout = 180)\n    download.file(\n      \"https://github.com/juanfonsecaLS1/P1_ratruns_analysis/releases/download/v0/bogota_data.zip\",\n      destfile = \"raw_data/bogota_data.zip\",\n      mode = \"wb\"\n    )\n    unzip(\"raw_data/bogota_data.zip\", exdir = \"raw_data\")\n  }\n}"
  },
  {
    "objectID": "C1_infractions.html#loading-infractions",
    "href": "C1_infractions.html#loading-infractions",
    "title": "Ticket Data",
    "section": "",
    "text": "reports2019 &lt;- read_csv(\"raw_data/bogota/Comparendos_2019_Bogota_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          OBJECTID = col_double(),\n                          NUM_COMPARENDO = col_character(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURRENCIA = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETECCION = col_character(),\n                          CLASE_VEHICULO = col_character(),\n                          TIPO_SERVICIO = col_character(), \n                          INFRACCION = col_character(),\n                          DES_INFRACCION = col_character(),\n                          LOCALIDAD = col_character(),\n                          MUNICIPIO = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double(),\n                          GlobalID = col_character()\n                        ))\n\nreports2020 &lt;- read_csv(\"raw_data/bogota/Comparendos_DEI_2020_Bogot%C3%A1_D_C.csv\",\n                        col_types = cols(\n                          X = col_double(),\n                          Y = col_double(),\n                          FID = col_double(),\n                          OBJECTID = col_double(),\n                          FECHA_HORA = col_character(),\n                          ANO = col_double(),\n                          HORA_OCURR = col_character(),\n                          MES = col_character(),\n                          MEDIO_DETE = col_character(),\n                          CLASE_VEHI = col_character(),\n                          TIPO_SERVI = col_character(),\n                          INFRACCION = col_character(),\n                          DES_INFRAC = col_character(),\n                          MUNICIPIO = col_character(),\n                          PAIS = col_character(),\n                          LATITUD = col_double(),\n                          LONGITUD = col_double()))\n\nA list of the codes with codes of offences related to vehicles circulating.\n\nselected_codes &lt;- read.csv(\"list_infractions.csv\")$INFRACCION"
  },
  {
    "objectID": "C1_infractions.html#exploring-types-of-infractions",
    "href": "C1_infractions.html#exploring-types-of-infractions",
    "title": "Ticket Data",
    "section": "",
    "text": "reports2019 |&gt; \n  count(INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.90)) |&gt; \n  ggplot(aes(y = fct_reorder(INFRACCION,n),\n             x = n))+\n  geom_col()+\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\ncount_infractions &lt;- reports2019 |&gt; \n  count(INFRACCION,DES_INFRACCION) |&gt; \n  filter(n&gt;quantile(n,0.70)) |&gt; \n  arrange(-n)"
  },
  {
    "objectID": "C1_infractions.html#wrong-way-infraction",
    "href": "C1_infractions.html#wrong-way-infraction",
    "title": "Ticket Data",
    "section": "",
    "text": "Creating an sf object with the reports for driving in the wrong-way\n\nwrong_way_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nwrong_way_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION == \"D03\") |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\n\nmanual_2019_sf &lt;- reports2019 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRACCION) |&gt;\n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nmanual_2020_sf &lt;- reports2020 |&gt;\n  filter(INFRACCION %in% selected_codes) |&gt; \n  select(-DES_INFRAC) |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt;\n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse"
  },
  {
    "objectID": "C1_infractions.html#tidying-up-the-datasets",
    "href": "C1_infractions.html#tidying-up-the-datasets",
    "title": "Ticket Data",
    "section": "",
    "text": "We will select only relevant columns and also classify the records by type of day. For that, we will use the bank holiday list we used for the speed data processing.\n\nbank_holidays &lt;- read.csv(\"raw_data/bogota/bank_holidays.csv\") |&gt;\n  mutate(bank_holiday = dmy(bank_holiday)) |&gt;\n  pull(bank_holiday)\n\n\nwwd_2019_clean &lt;- wrong_way_2019_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\nwwd_2020_clean &lt;- wrong_way_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\nAll manual tickets\n\nmanual_2019_clean &lt;-  manual_2019_sf|&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\nmanual_2020_clean &lt;- manual_2020_sf |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         off_code = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  mutate(day_type = case_when(date(timestamp) %in% bank_holidays~\"weekend\",\n                              wday(timestamp,week_start = 1)&lt;=4~\"weekday\",\n                              wday(timestamp,week_start = 1) == 5~\"friday\",\n                              T~\"weekend\"))\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\nAnd consolidating the two datasets\n\nall_wwd_reports &lt;- bind_rows(wwd_2019_clean,wwd_2020_clean) |&gt; \n  mutate(veh_class = str_to_lower(veh_class))\n\nall_manual_reports &lt;- bind_rows(manual_2019_clean,manual_2020_clean) |&gt; \n  mutate(veh_class = str_to_lower(veh_class))"
  },
  {
    "objectID": "C1_infractions.html#eda",
    "href": "C1_infractions.html#eda",
    "title": "Ticket Data",
    "section": "",
    "text": "ggplot(all_wwd_reports,aes(col = factor(year)))+\n         geom_sf(alpha = 0.2,size = 0.5)+\n  theme_void()"
  },
  {
    "objectID": "C1_infractions.html#temporal-distribution",
    "href": "C1_infractions.html#temporal-distribution",
    "title": "Ticket Data",
    "section": "",
    "text": "A daily profile\n\nggplot(all_wwd_reports,\n         aes(x = time, col = factor(year)))+\n  geom_density(alpha = 0.5,linewidth = 2)+\n    scale_x_time()+\n  scale_colour_manual(values = c(\"dodgerblue3\",\"firebrick3\"))+\n  # scale_y_continuous(labels = scales::label_percent(accuracy = 2))+\ntheme_minimal()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date(timestamp),day_type,hour,year) |&gt; \n  ggplot(aes(x=factor(hour),y = n,col = day_type))+\n  geom_jitter(alpha = 0.05)+\n  geom_boxplot(alpha = 0.5,outlier.shape = NA)+\n  facet_grid(year~.)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nA check of the reports per day\n\nexp_dates_count &lt;- all_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp)) |&gt; \n  right_join(\n    tibble(date = seq(min(date(all_wwd_reports$date_time)),\n                      max(date(all_wwd_reports$date_time)),\n                      by = \"1 day\")),by = \"date\")\n\nA quick check of the timeline reveals gaps in the reports of 2019. We do not know the causes.\n\nexp_dates_count |&gt; \n  mutate(n = if_else(is.na(n),0,n)) |&gt; \n  ggplot(aes(x = date,y = n))+\n  geom_line()+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nexp_dates_count |&gt; \n  mutate(wday = wday(date,week_start = 2),\n         week = week(date)) |&gt; \n  ggplot(aes(y = factor(wday),x = factor(week),fill = n))+\n  geom_tile()+\n  theme_minimal()+\n  facet_grid(year(date)~.)+\n  scale_fill_viridis_c(direction = -1)\n\n\n\n\n\n\n\n\n\n\nLet us check if there is any patter related to the day of the week\n\nall_wwd_reports |&gt;\n  st_drop_geometry() |&gt; \n  count(date = date(timestamp),year) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,year)) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(year)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\n\n\n\n\n\n\n\nThere seems to be a pattern that might be related to the sampling i.e. how the enforcement officers are assigned along the week.\nLet’s compare with other types of infractions reported by officers\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp),offence) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday,offence)) |&gt; \n  filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n, col = factor(offence)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nWe can also inspect the median number of tickets per weekday for all offences that are reported by traffic management officers in 2019 and 2020\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp)) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nreports2020 |&gt; \n  select(-DES_INFRAC) |&gt;\n  filter(MEDIO_DETE == \"DISPOSITIVOS EN VÍA\") |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp)) |&gt; \n  count(date = date(timestamp)) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = mean(n),.by = c(wday)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nIs it a problem with the data, as some months are not reported? Let’s see if the pattern is similar across months\n\nreports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\") |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nreports2020 |&gt; \n  select(-DES_INFRAC) |&gt;\n  filter(MEDIO_DETE == \"DISPOSITIVOS EN VÍA\") |&gt; \n  mutate(time = hms(HORA_OCURR)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETE,\n         CLASE_VEHI,\n         time,\n         INFRACCION) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETE,\n         veh_class = CLASE_VEHI,\n         offence = INFRACCION) |&gt;\n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),month) |&gt; \n  mutate(wday = wday(date,week_start = 2)) |&gt; \n  summarise(n = median(n),.by = c(wday,month)) |&gt; \n  # filter(sum(n)&gt;250,.by = offence) |&gt; \n  ggplot(aes(x= wday,y = n,col =  factor(month)))+\n  geom_line()+\n  scale_x_continuous(breaks = 1:7,\n                     labels = weekdays(as.Date(4,\"1970-01-01\",tz=\"GMT\")+0:6))+\n  labs(x=\"\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `time = hms(HORA_OCURR)`.\nCaused by warning in `.parse_hms()`:\n! Some strings failed to parse\n\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nVisually, there seems to be a consistent pattern of a low number of reports during friday and saturday, followed by a surge on Sundays."
  },
  {
    "objectID": "C1_infractions.html#spatial-sampling-bias",
    "href": "C1_infractions.html#spatial-sampling-bias",
    "title": "Ticket Data",
    "section": "",
    "text": "Let’s assign all reports to the nearest link to explore where the enforcement offices tend to catch the offenders\n\nnet_bog_2019 &lt;- st_read(file.path(\"sf_network\",\"bogota_osm_network.gpkg\")) |&gt; st_transform(3116)\n\nMultiple layers are present in data source /home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg, reading layer `network_2019'.\nUse `st_layers' to list all layer names and their type in a data source.\nSet the `layer' argument in `st_read' to read a particular layer.\n\n\nWarning in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, :\nautomatically selected the first layer in a data source containing more than\none.\n\n\nReading layer `network_2019' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/bogota_osm_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 54531 features and 15 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.441 ymin: 4.236862 xmax: -73.72583 ymax: 5.042826\nGeodetic CRS:  MAGNA-SIRGAS\n\n\n\nmajor_network &lt;- net_bog_2019 |&gt; filter(!(highway %in% c(\"residential\",\"unclassified\")))\n\n\nmanual_reports_2019 &lt;- reports2019 |&gt;\n  select(-DES_INFRACCION) |&gt;\n  filter(MEDIO_DETECCION == \"LAPIZ\",INFRACCION %in% selected_codes) |&gt; \n  mutate(time = hms(HORA_OCURRENCIA)) |&gt; \n  select(FECHA_HORA,\n         MEDIO_DETECCION,\n         CLASE_VEHICULO,\n         time,\n         INFRACCION,\n         LONGITUD,LATITUD) |&gt;\n  rename(date_time = FECHA_HORA,\n         detect_device = MEDIO_DETECCION,\n         veh_class = CLASE_VEHICULO,\n         offence = INFRACCION) |&gt; \n  st_as_sf(coords = c(\"LONGITUD\",\"LATITUD\"),crs = 4326) |&gt; \n  st_transform(3116)\n\n\nmanual_reports_2019$near_index &lt;- st_nearest_feature(manual_reports_2019,net_bog_2019)\nmanual_reports_2019$near_major_index &lt;- st_nearest_feature(manual_reports_2019,major_network)\n\n\nmanual_reports_2019$highway &lt;- net_bog_2019$highway[manual_reports_2019$near_index]\nmanual_reports_2019$oneway &lt;- net_bog_2019$oneway[manual_reports_2019$near_index]\n\n\nmanual_reports_2019$dist_to_major &lt;- st_distance(\n  manual_reports_2019,\n  major_network[manual_reports_2019$near_major_index,],\n  by_element = T) |&gt; as.numeric()\n\nType of road where the offence was reported\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt; \n  ggplot(aes(x = fct_reorder(highway,n,.desc = F),y = n))+\n  geom_col()+\n  coord_flip()\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt;\n  mutate(highway = factor(highway,\n                          levels =\n                            c(\"trunk\",\"primary\",\"secondary\",\"tertiary\",\"residential\",\"unclassified\"),\n                          ordered = T)) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  count(date = date(timestamp),highway) |&gt; \n  summarise(n = median(n),.by = c(highway)) |&gt;\n  arrange(highway) |&gt; \n  mutate(perc = n/sum(n)) |&gt;\n  # mutate(prev = cumsum(perc)-perc) |&gt; \n  # select(-n) |&gt; \n  # pivot_longer(-highway) |&gt; \n  ggplot(aes(x = 1,y = perc,fill = highway))+\n  geom_col(position = \"stack\")+\n  # scale_alpha_manual(values = c(1,0.2))+\n  coord_flip()+\n  theme_minimal()+\n  scale_y_continuous(labels = scales::label_percent())\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nMedian daily reports in residential roads\n\nmanual_reports_2019 |&gt; \n  st_drop_geometry() |&gt; \n  mutate(highway = str_remove(highway,\"_link\")) |&gt; \n  mutate(date_time = str_replace(date_time,\"\\\\+00\",\"-05\")) |&gt; \n  mutate(timestamp = ymd_hms(date_time, tz = \"America/Bogota\"),\n         hour = hour(time),\n         year = year(timestamp),\n         month = month(timestamp)) |&gt; \n  filter(highway == \"residential\") |&gt;\n  count(date = date(timestamp),offence) |&gt; \n  summarise(n = median(n),.by = c(offence)) |&gt;\n  slice_max(n,n=15) |&gt; \n  ggplot(aes(x = fct_reorder(offence,n), y = n))+\n  geom_col()+\n  coord_flip()\n\nDate in ISO8601 format; converting timezone from UTC to \"America/Bogota\".\n\n\n\n\n\n\n\n\n\nEuclidean distance to major network vs all reports\n\nggplot()+\n  geom_histogram(data = manual_reports_2019,\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_log10()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot()+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n  aes(dist_to_major),\n  alpha = 0.2)+\n  geom_histogram(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"),offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nnet_points &lt;- net_bog_2019 |&gt;\n  st_cast(\"POINT\") |&gt;\n  slice_head(by = osm_id)\n\nWarning in st_cast.sf(net_bog_2019, \"POINT\"): repeating attributes for all\nsub-geometries for which they may not be constant\n\nnet_points &lt;- net_bog_2019 |&gt; st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nnet_points$near_id &lt;- st_nearest_feature(net_points,major_network)\nnet_points$dist_to_major &lt;- st_distance(net_points,\n                                        major_network[net_points$near_id,],\n                                        by_element = T) |&gt;\n  as.numeric()\n\n\nggplot()+\n  geom_density(data = net_points|&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\")),\n                 aes(dist_to_major),\n                 alpha = 0.1)+\n  geom_density(data = manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"),\n  aes(dist_to_major),\n  alpha = 0.3,\n  fill = \"dodgerblue4\",\n  col = \"dodgerblue3\")+\n  scale_x_sqrt(breaks = c(0,0.1,0.25,.5,1,2.5,5,7.5,10)*1e3)\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(major_network)+\n  tm_lines()+\n  tm_shape(manual_reports_2019 |&gt; \n  filter(highway %in% c(\"residential\",\"unclassified\"), offence == \"D03\"))+\n  tm_dots(\"dist_to_major\")"
  },
  {
    "objectID": "C1_infractions.html#summary-by-vehicle-type",
    "href": "C1_infractions.html#summary-by-vehicle-type",
    "title": "Ticket Data",
    "section": "",
    "text": "all_wwd_reports |&gt;\n  st_drop_geometry() |&gt;  \n  count(veh_class,year) |&gt; \n  arrange(-n) |&gt; \n  mutate(n = n/sum(n),.by=year) |&gt; \n  ggplot(aes(x = fct_reorder(veh_class,n,.desc = F),  y = n, fill = factor(year)))+\n  geom_col(position = \"dodge\",)+\n  coord_flip()+\n  scale_y_continuous(labels = scales::label_percent())+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"dodgerblue3\",\"firebrick3\"))"
  },
  {
    "objectID": "C1_infractions.html#saving-clean-datasets",
    "href": "C1_infractions.html#saving-clean-datasets",
    "title": "Ticket Data",
    "section": "",
    "text": "st_write(all_wwd_reports,\n         dsn = \"sf_network/wwd_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting layer `wwd_clean_sf' to data source \n  `sf_network/wwd_clean_sf.gpkg' using driver `GPKG'\nWriting 6915 features with 8 fields and geometry type Point.\n\n\n\nst_write(all_manual_reports,\n         dsn = \"sf_network/manualtickets_clean_sf.gpkg\",\n         delete_dsn = T)\n\nDeleting source `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting layer `manualtickets_clean_sf' to data source \n  `sf_network/manualtickets_clean_sf.gpkg' using driver `GPKG'\nWriting 455598 features with 9 fields and geometry type Point."
  },
  {
    "objectID": "D3_WWD_joining.html",
    "href": "D3_WWD_joining.html",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!require(\"remotes\")) install.packages(\"remotes\")\npkgs = c(\n    \"sf\",\n    \"tidyverse\",\n    \"zonebuilder\",\n    \"tmap\",\n    \"ggExtra\"\n    # \"dodgr\" # Using the developer version of dodgr\n)\nremotes::install_cran(pkgs)\nsapply(pkgs, require, character.only = TRUE)\n\n         sf   tidyverse zonebuilder        tmap     ggExtra \n       TRUE        TRUE        TRUE        TRUE        TRUE \n\n\n\n\n\nbog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\n# off_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\noff_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 455598 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.24842 ymin: 4.453367 xmax: -74.00963 ymax: 4.825923\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\nRows: 3722004 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): edge_id\ndbl (8): wwd.speed, dist.th, ff, cong, diff, logdiff, reldiff, logreldiff.ff\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:reldiff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n  #        .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_ecdf()`).\n\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\nWarning: Removed 541 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 541 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning: Removed 245 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 245 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning: Removed 245 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 245 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ reldiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      offence_bool = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = reldiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\nWarning: Removed 99 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt; \n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\nVariable(s) \"logdiff_max\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\nWarning: Removed 882 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 882 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning in geom_smooth(method = \"glm\", formula = y ~ x, family = \"binomial\", :\nIgnoring unknown parameters: `family`\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\nWarning: Removed 99 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  },
  {
    "objectID": "D3_WWD_joining.html#loading-data",
    "href": "D3_WWD_joining.html#loading-data",
    "title": "Joining the WWD reports",
    "section": "",
    "text": "bog_zone &lt;- zonebuilder::zb_zone(\"Bogota\",\n                                 n_circles = 2) |&gt; \n  st_transform(3116)\n# zb_view(bog_zone)\n\n\n\n\n# off_sf_all &lt;- st_read(\"sf_network/wwd_clean_sf.gpkg\")\noff_sf_all &lt;- st_read(\"sf_network/manualtickets_clean_sf.gpkg\")\n\nReading layer `manualtickets_clean_sf' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/manualtickets_clean_sf.gpkg' \n  using driver `GPKG'\nSimple feature collection with 455598 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.24842 ymin: 4.453367 xmax: -74.00963 ymax: 4.825923\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nsf_net &lt;- st_read(\"sf_network/small_sf_network.gpkg\")\n\nReading layer `small_sf_network' from data source \n  `/home/juan/P1_ratruns_analysis/sf_network/small_sf_network.gpkg' \n  using driver `GPKG'\nSimple feature collection with 37596 features and 22 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -74.15632 ymin: 4.594991 xmax: -74.03324 ymax: 4.752852\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\ncent_tests &lt;- read_csv(\"sf_network/cent_tests.csv\",\n                       lazy = F)\n\nRows: 3722004 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): edge_id\ndbl (8): wwd.speed, dist.th, ff, cong, diff, logdiff, reldiff, logreldiff.ff\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nWe need to assign the reports to the network. As we are interested in the residential roads, any bi-directional road is represented in the sf object as two LINESTRING objects. Since we do not have information to know which specific direction the reports correspond to, we will need to simplify the spatial object. Our target variable is the betweenness centrality, so we are going to keep the two centrality values for each bi-directional element.\nFirst, we will create a subset of the residential and unclassified roads, and another with all other roads\n\nsubset_net &lt;- sf_net |&gt; \n  filter(roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\n\nmajor_net &lt;- sf_net |&gt; \n  filter(!roadclass %in% c(\n    \"residential\",\n    \"unclassified\"\n    )) |&gt; \n  st_transform(3116)\n\nFrom this subset, we first find the pairs of links with the st_contains function.\n\nsimplifed_net_indexes &lt;- lapply(st_contains(subset_net,subset_net),\n            \\(x){\n              x[order(x)]\n}) |&gt; unique()\n\nEach pair is then assigned a unique id.\n\nsimp_groups &lt;- do.call(bind_rows,\n        lapply(seq_along(simplifed_net_indexes),\n               \\(i){\n                 tibble(id = simplifed_net_indexes[[i]],\n                        pair_id = i)\n               })) |&gt; \n  arrange(id)\n\n\nsubset_net$pair_id &lt;- simp_groups$pair_id\n\nUsing the pair_id we extract the minimum, maximum and average change in centrality for each pair.\n\nsummary_pairs &lt;- cent_tests |&gt; \n  right_join(subset_net |&gt;\n               st_drop_geometry() |&gt;\n               select(edge_id,way_id,pair_id),\n            by = \"edge_id\") |&gt; \n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt;\n  summarise(across(diff:reldiff,\n                   list(min=min, max=max, avg = mean)),\n            .by = c(pair_id,wwd.speed,dist.th))\n\n\nsummary_pairs_dist.jct &lt;- \n  subset_net |&gt;\n  st_drop_geometry() |&gt;\n  summarise(across(dist.jct,\\(x) mean(x) |&gt; round()),\n            .by = c(pair_id))\n\nA simplified version of the sf object is produced extracting the first element of each pair, we will discard columns with the centrality metrics from this object to avoid confusion\n\n# simpl_network_sf &lt;- subset_net[vapply(simplifed_net_indexes,\\(x) x[1],numeric(1)),] |&gt; select(lanes:component,pair_id)\n\nsimpl_network_sf &lt;- subset_net |&gt;\n  # # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),\n  #        .by = pair_id) |&gt;\n  slice_head(n = 1,by = pair_id) |&gt;\n  select(lanes:component,pair_id)\n\nWe are interested in the reports on residential and unclassified streets. For this, we will create two buffers from the subsets created before. It is uncertain how thecoordinates of each report are recorded, there might be some error associated with the use of GPS devices, and also, some uncertainty in the way the officers do it.\n\nanti_buffer &lt;- major_net |&gt;\n  st_union() |&gt; \n  st_buffer(10,endCapStyle = \"FLAT\")\n\n\nsubset_buffer &lt;- simpl_network_sf |&gt; \n  st_union()  |&gt; \n  st_buffer(15,endCapStyle = \"FLAT\")\n\nAs some reports might be associated to the major network, we will first filter reports within 10 meters from a major road, so these do not get wrongly assigned to a minor road. We are also going to subset reports during the morning peak hour (+/- 2 hours) in 2019.\n\noff_sf &lt;- (off_sf_all |&gt;\n                 filter(abs(hour - 9)&lt;=2,\n                        year == 2019,\n                        day_type == \"weekday\") |&gt;\n                 st_transform(3116))[bog_zone,][anti_buffer,,op = st_disjoint]\n\nThe offences that are assumed to happen in the minor offences are assumed to be within 20 meters.\n\nminor_offences &lt;- off_sf[subset_buffer,,op = st_intersects]\n\n\n# tmap_mode(\"view\")\ntm_shape(anti_buffer)+\n  tm_polygons(\"gray60\",alpha = 0.6)+\n  tm_shape(subset_buffer)+\n  tm_polygons(\"blue\",alpha = 0.6)+\n  tm_shape(minor_offences)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\nminor_offences$near_index &lt;- st_nearest_feature(minor_offences,simpl_network_sf)\nminor_offences$pair_id &lt;- simpl_network_sf$pair_id[minor_offences$near_index]\n\n\n\n\nsimpl_network_sf |&gt;\n  st_drop_geometry() |&gt; \n  filter(pair_id %in% minor_offences$pair_id) |&gt; \n  ggplot(aes(oneway))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\nsimpl_network_sf |&gt; \n  ggplot(aes(col = oneway))+\n  geom_sf()+\n  theme_void()\n\n\n\n\n\n\n\n\nThe following plot compares the cumulative probability of distance to the major network looking for a sampling bias\n\nsimpl_network_sf |&gt;\n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct,col = offence_bool))+\n  stat_ecdf(alpha = 0.7)+\n  theme_minimal()\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_ecdf()`).\n\n\n\n\n\n\n\n\n\nThe following produces a histogram with the distribution\n\nsimpl_network_sf |&gt; \n  filter(oneway) |&gt; \n  left_join(summary_pairs_dist.jct, by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt;\n  st_drop_geometry() |&gt; \n  ggplot(aes(dist.jct, fill = offence_bool))+\n  geom_histogram(alpha = 0.7,col=\"white\")+\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLet’s try see if a naive logistic regression can be fit with the data. For this, we subset the data for one-way links\n\nmodel_data &lt;- (summary_pairs |&gt;\n  semi_join(simpl_network_sf[bog_zone,],\n            by = \"pair_id\")) |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\nA jitter plot to explore the distribution\n\nmodel_data |&gt; \n  filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_jitter(alpha = 0.1)+\n  theme_minimal()\n\nWarning: Removed 541 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nDistribution of average relative change for the data\n\nmodel_data |&gt; \n    filter(wwd.speed == 9, dist.th == 1140) |&gt; \n  ggplot(aes(x = (reldiff_max),fill = offence_bool))+\n  geom_histogram(alpha = 0.4)+\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 541 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nSome OSM links have been split, so we will simplify the data by summarising the results by OSM way id\n\ntest1 &lt;- model_data |&gt;\n  filter(wwd.speed == 9, dist.th == 1140) |&gt;\n  left_join(simpl_network_sf |&gt;\n              st_drop_geometry() |&gt; \n              select(pair_id,way_id),\n            by = \"pair_id\") |&gt; \n  group_by(way_id) |&gt; \n  summarise(across(c(diff_max,reldiff_max,logdiff_max),\n                   \\(x) mean(x,na.rm = T)),\n            across(offence_bool,\\(x) sum(x)&gt;=1))\n\nThe following code shows how a logistic regression fits the data in one of the scenarios. Unfortunately, the false positives do have a significant impact.\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (reldiff_max),y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning: Removed 245 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 245 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\ntest1 |&gt;\n  mutate(offence_bool = if_else(offence_bool,1,0)) |&gt; \n  ggplot(aes(x = (logdiff_max),\n             y = offence_bool))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning: Removed 245 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 245 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_0 &lt;- model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(offence_bool ~ reldiff_max,\n                     data = .x,\n                     family = binomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred_0 &lt;- glm_models_0 |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      offence_bool = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred_0 |&gt; \n  ggplot(aes(x = reldiff_max,\n             y = offence_bool,\n             group = id,\n             col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\nWarning: Removed 99 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubset_net_offence &lt;- subset_net |&gt; \n  # left_join(summary_pairs,\n  #           by = \"pair_id\") |&gt; \n  mutate(offence_bool = pair_id %in% minor_offences$pair_id)\n\nLet’s take one link with a wwd report\n\nsample_offence &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; slice_head(n=1)\n\n\nbuf_sample &lt;- sample_offence |&gt; st_buffer(500)\n\n\nnet_sample &lt;- subset_net_offence[buf_sample,]\n\nnet_sample |&gt; \n  tm_shape()+tm_lines(\"gray80\")+\n  tm_shape(sample_offence)+tm_lines(\"dodgerblue\")\n\n\n\n\n\n\n\n\n\nfill_probs &lt;- function(edges_df,\n                       direction = c(\"1\",\"-1\")\n                       ) {\n  \n  direction = match.arg(direction)\n\n    if (direction == \"1\") {\n      do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n        subset_net_offence |&gt;\n          st_drop_geometry() |&gt;\n          select(from_id, to_id) |&gt;\n          filter(to_id == edges_df$from_id[j],\n                 from_id != edges_df$to_id[j]) |&gt;\n          mutate(p = edges_df$p[j] / n())\n      }))\n  } else {\n    do.call(bind_rows, lapply(1:nrow(edges_df), \\(j) {\n      subset_net_offence |&gt;\n        st_drop_geometry() |&gt;\n        select(from_id, to_id) |&gt;\n        filter(from_id == edges_df$to_id[j],\n               to_id != edges_df$from_id[j]) |&gt;\n        mutate(p = edges_df$p[j] / n())\n    }))\n    \n  }\n}\n\n\nexpand_reports &lt;- function(\n    df,\n    max_degree = 6\n) {\n  \n  check0 &lt;- df |&gt;\n    st_drop_geometry() |&gt;\n    select(from_id, to_id) |&gt;\n    mutate(p = 1)\n  \n  check &lt;- list()\n  check[[1]] &lt;- fill_probs(check0)\n  for (i in 2:max_degree) {\n    if (nrow(check[[i-1]]) &lt; 1) break\n    check[[i]] &lt;- fill_probs(check[[i - 1]])\n  }\n  \n  checkr &lt;- list()\n  checkr[[1]] &lt;- fill_probs(check0, direction = \"-1\")\n  for (i in 2:max_degree) {\n    if (nrow(checkr[[i - 1]]) &lt; 1) break\n    checkr[[i]] &lt;- fill_probs(checkr[[i - 1]], direction = \"-1\")\n  }\n  \n  ckeck_df &lt;- bind_rows(check0, do.call(bind_rows, check), do.call(bind_rows, checkr)) |&gt;\n    summarise(across(p, max), .by = c(from_id, to_id))\n  return(ckeck_df)\n}\n\n\nsample_exp &lt;- expand_reports(df = sample_offence)\n\n\nnet_sample |&gt; \n  left_join(sample_exp,\n            by = join_by(from_id,to_id)) |&gt; \n  tm_shape()+\n  tm_lines(\"p\",lwd = 2,alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nfull_exp &lt;- subset_net_offence |&gt; filter(offence_bool) |&gt; expand_reports()\n\n\nsummary_probs_adj &lt;- subset_net_offence |&gt;\n  st_drop_geometry() |&gt;\n  left_join(full_exp,\n            by = join_by(from_id, to_id)) |&gt;\n  mutate(p = if_else(is.na(p),0,p)) |&gt; \n  # Filtering only the links that were inverted during the network creation and standard links\n  # filter(!any(str_detect(way_id,\"r$\"))|str_detect(way_id,\"r$\"),.by = pair_id) |&gt; \n  summarise(across(p,\\(x) sum(x,na.rm = T)),\n            .by = pair_id) |&gt;\n  mutate(p = if_else(p&gt;1,1,p)) |&gt; \n  left_join(summary_pairs_dist.jct,by = join_by(pair_id))\n\n\nnet_offence_p &lt;- simpl_network_sf[bog_zone,] |&gt;\n  left_join(summary_probs_adj,\n            by = \"pair_id\")\n\n\ntm_shape(net_offence_p)+\n  tm_lines(\"p\",style = \"fisher\")\n\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n\nnet_offence_p |&gt; \n  left_join(summary_pairs,\n            by = join_by(pair_id)) |&gt; \n  filter(wwd.speed==6,dist.th == 1140) |&gt; \n  tm_shape()+\n  tm_lines(\"logdiff_max\",\n           style = \"fisher\")+\n  tm_shape(minor_offences)+\n  tm_dots()\n\nVariable(s) \"logdiff_max\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data &lt;- \n  summary_pairs |&gt; \n  right_join(net_offence_p |&gt; \n              st_drop_geometry(),\n            by = join_by(pair_id))\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = reldiff_avg,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args=list(family=\"quasibinomial\"),se = F)\n\nWarning: Removed 882 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 882 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nadjusted_probs_model_data|&gt;\n  filter(wwd.speed == 6, dist.th == 1140) |&gt;\n  ggplot(aes(x = dist.jct,\n             y = p))+\n    geom_point(alpha = 0.1)+\n  theme_minimal()+\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              family = \"binomial\",\n              method.args=list(family=\"binomial\"),se = F)\n\nWarning in geom_smooth(method = \"glm\", formula = y ~ x, family = \"binomial\", :\nIgnoring unknown parameters: `family`\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm_models_probs &lt;- adjusted_probs_model_data |&gt; \n  nest(data = c(pair_id,\n                diff_min:dist.jct)) |&gt; \n  mutate(\n    model_rel = map(data,\n                \\(.x) {\n                 glm(p ~ reldiff_max,\n                     data = .x,\n                     family = quasibinomial(link = \"logit\"))\n                # },\n                # model_abs = map(data,\n                # \\(.x) {\n                #  glm(p ~ logdiff_max,\n                #      data = .x,\n                #      family = binomial(link = \"logit\"))\n                }\n                )\n                )\n\n\nmod_pred &lt;- glm_models_probs |&gt; \n  mutate(predicted = map2(model_rel, data ,\\(.x,.y) {\n    tibble(reldiff_max = .y$reldiff_max,\n      p = predict(.x, newdata = .y,type = \"response\"))\n  })) |&gt; \n  select(-data,-model_rel) |&gt; \n  unnest(cols = predicted) |&gt; \n  unite(\"id\",wwd.speed:dist.th,remove = F) |&gt; unique()\n\nA visual of the fitted lines\n\nmod_pred |&gt; \n  ggplot(aes(x = reldiff_max,y = p,group = id, col = wwd.speed))+\n  geom_line(alpha = 0.1)+\n  scale_y_continuous(limits = c(0,1))+\n  theme_minimal()+\n  scale_color_viridis_c(option = \"plasma\")\n\nWarning: Removed 99 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nA visual exploration of the coefficients\n\nmod_coefs &lt;- glm_models_probs |&gt; \n  mutate(coefs = map(model_rel,\\(.x) {\n    broom::tidy(.x)\n  }\n  )\n  ) |&gt;\n  select(wwd.speed,dist.th,coefs) |&gt; \n  unnest(coefs) |&gt; \n  # pivot_wider(names_from = term,values_from = estimate) |&gt; \n  mutate(term = term |&gt; str_replace(pattern = \"\\\\(Intercept\\\\)\",replacement = \"intercept\"),\n         term = term |&gt; str_replace(pattern = \"reldiff_max\",replacement = \"slope\"))\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = wwd.speed,\n           y = estimate,\n           # col = wwd.speed,\n           col = dist.th\n           ))+\n  geom_line(aes(group = dist.th))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"slope\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th\n           ))+\n  geom_line(aes(group = wwd.speed))+\n  geom_point(aes(size = p.value),alpha = 0.3)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"plasma\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\nmod_coefs |&gt; \n  filter(term == \"intercept\") |&gt; \nggplot(aes(x = dist.th,\n           y = estimate,\n           col = wwd.speed,\n           # col = dist.th,\n           ))+\n  geom_line(aes(group = wwd.speed),alpha = 0.2)+\n  geom_point(aes(size = std.error),alpha = 0.4)+\n  theme_minimal()+\n  scale_color_viridis_b(option = \"A\")+\n  scale_size_binned(transform = \"reciprocal\")\n\n\n\n\n\n\n\n\n\n# p &lt;- mod_coefs |&gt; \n#   \n#   \n# ggplot(aes(x = intercept,y = slope,col = wwd.speed, alpha = dist.th))+\n#   geom_point()+\n#   theme_minimal()+\n#   scale_color_viridis_b(option = \"plasma\")\n# \n#   ggMarginal(p,type = \"histogram\")\n#"
  }
]