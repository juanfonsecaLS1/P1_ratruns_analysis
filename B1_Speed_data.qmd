# Speed Data Processing

```{r,message=FALSE}
#| label: libraries
#| message: false

options(repos = c(CRAN = "https://cloud.r-project.org"))
if (!require("remotes")) install.packages("remotes")
pkgs = c(
    "sf",
    "tidyverse",
    "data.table",
    "paletteer"
)
remotes::install_cran(pkgs)
sapply(pkgs, require, character.only = TRUE)

setDTthreads(0)
```

# Loading and cleaning

We obtained speed data from [Datos Abiertos Movilidad de Bogotá (Movility Open Data of Bogotá)](https://datos.movilidadbogota.gov.co/search?groupIds=71ef3e63c60749cbb89028c76558a304)

```{r}
#| label: read-speed-files
#| eval: false

lst_files <- list.files("../00_data/bogota/speed_data/",pattern = "csv$",full.names = T)

all_data <- rbindlist(lapply(lst_files,fread),fill = T)[,
         c("OBJECTID",
           "FIN",
           "Shape__Length",
           "LLAVE",
           "AÑO",
           "CUARTO_HORA"):=NULL
         ]
```

## Preliminary cleaning

```{r}
#| label: tidy-year
#| eval: false

all_data[,
           INICIO := mdy_hms(INICIO,
                             tz = "America/Bogota")
         ][,
           `:=`(YEAR = year(INICIO))
           ]
```

Not all links have data for all months. So the first step is to identify the
links with enough data per year. 

```{r}
#| label: summary-links-per-year
#| eval: false

links_summary <- unique(
  all_data[,
           c("TID","YEAR","MES")])[,
                                   .(count = .N),
                                   by = c("TID","YEAR")]
```

```{r}
#| label: plot-summary-links-per-year
#| eval: false

links_summary |> 
  ggplot(aes(count))+
  geom_histogram(binwidth = 1)+
  scale_y_log10()+
  scale_x_continuous(breaks = 0:12)+
  facet_grid(YEAR~.)
```

Links with data in 5 or more months per year will be used for this analysis.

```{r}
#| label: final-links
#| eval: false

links_data = links_summary[,count := 1*(count>=5)
                           ][,.(tot_count = sum(count)),
                             by = "TID"][
                               tot_count == 2,
                               ] |> pull(TID)
```

We extract the data only for these links with the following code and
clean the memory:
```{r}
#| label: cleaning-data
#| eval: false

clean_data <- all_data[TID %in% links_data]
rm(all_data,links_data,links_summary,lst_files,a)
gc()
```

```{r}
#| eval: false
#| label: saving-clean-data
#| include: false

save(clean_data,file = "raw_data/selected_speeds.RData")
```


```{r}
#| label: reading-clean-data
#| include: false

load(file = "raw_data/selected_speeds.RData")
gc()
```


## Classifiying the data

First, we will identify the days that were bank holidays in Colombia

```{r}
#| label: read_bankholidays

bank_holidays <- read.csv("../00_data/bogota/bank_holidays.csv") |>
  mutate(bank_holiday = dmy(bank_holiday)) |>
  pull(bank_holiday)
```

```{r}
#| label: update-dtype

clean_data[date(INICIO) %in% bank_holidays,
           DIA_SEMANA := "Festivo"
           ]

clean_data[,DAY_TYPE := fcase(
             DIA_SEMANA == "Domingo","weekend",
             DIA_SEMANA == "Sabado","weekend",
             DIA_SEMANA == "Festivo","weekend",
             DIA_SEMANA == "Viernes","friday",
             default = "weekday")
             ]
```

The following code extracts the 94th percentile of the speeds in each road link.

```{r}
max_speeds <- clean_data[,
                             .(p94_speed = quantile(VEL_PROMEDIO,
                                                       0.94,
                                                       na.rm = T)),
                             by =  c("TID","YEAR")]
```

Now, we calculate the median hourly speed for each road link by day type and road link.

```{r}
summary_speeds <- clean_data[,
                             .(median_speed = median(VEL_PROMEDIO,
                                                     na.rm = T)),
                             by =  c("TID","HORA","DAY_TYPE","YEAR")]
```


Now we normalise the values by dividing by the 95th-percentile speed

```{r}
norm_summary_spd <- merge(summary_speeds,
                          max_speeds,
                          by = c("TID","YEAR"))[,
                                                norm_speed := median_speed/p94_speed
                                                ]
```

## Some visualisations of speed distribution:

### Hourly distributions

#### Normalised speed
```{r}
norm_summary_spd |> 
  ggplot(aes(x = HORA,y = norm_speed))+
  geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+
  geom_jitter(alpha = 0.3, size = 0.3,col = "gray60")+
  stat_summary(geom = "line",
               fun = "mean",
               aes(col = DAY_TYPE),
               linewidth = 1,
               alpha = 0.6,
               show.legend = F)+
  facet_grid(DAY_TYPE~.)+
  theme_minimal()+
  labs(x = "Hour", y = "Norm speed (observed/94th percentile)")+
  scale_x_continuous(breaks = 0:23,
                     labels = sprintf("%02d:00",0:23))+
  theme(axis.text.x = element_text(angle = 90))+
  scale_colour_manual(values = paletteer_d("ggsci::lanonc_lancet",n = 3))
  
```

#### Absolute speed
```{r}
norm_summary_spd |> 
  ggplot(aes(x = HORA,y = median_speed))+
  geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+
  geom_jitter(alpha = 0.3, size = 0.3,col = "gray60")+
  stat_summary(geom = "line",
               fun = "mean",
               aes(col = DAY_TYPE),
               linewidth = 1,
               alpha = 0.6,
               show.legend = F)+
  facet_grid(DAY_TYPE~YEAR)+
  theme_minimal()+
  labs(x = "Hour", y = "Observed speed")+
  scale_x_continuous(breaks = 0:23,
                     labels = sprintf("%02d:00",0:23))+
  theme(axis.text.x = element_text(angle = 90))+
  scale_colour_manual(values = paletteer_d("ggsci::lanonc_lancet",n = 3))
  
```

### Daily profile by Link

#### Normalised speed

```{r}
norm_summary_spd |> 
  ggplot(aes(x = HORA,y = norm_speed))+
  # geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+
  # geom_jitter(alpha = 0.3, size = 0.3,col = "gray60")+
  stat_summary(geom = "line",
               fun = "mean",
               aes(col = DAY_TYPE,
                   group = TID),
               linewidth = 0.01,
               alpha = 0.04,
               # show.legend = F
               )+
  facet_grid(DAY_TYPE~YEAR)+
  theme_minimal()+
  labs(x = "Hour", y = "Speed ratio (observed/94th percentile)")+
  scale_x_continuous(breaks = 0:23,
                     labels = sprintf("%02d:00",0:23))+
  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+
  scale_colour_manual(values = paletteer_d("ggsci::default_nejm",n = 3))+
  theme(axis.text.x = element_text(angle = 90),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```

#### Absolute speed

```{r}
norm_summary_spd |> 
  ggplot(aes(x = HORA,y = median_speed))+
  # geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+
  # geom_jitter(alpha = 0.3, size = 0.3,col = "gray60")+
  stat_summary(geom = "line",
               fun = "mean",
               aes(col = DAY_TYPE,
                   group = TID),
               linewidth = 0.01,
               alpha = 0.04,
               # show.legend = F
               )+
  facet_grid(DAY_TYPE~YEAR)+
  geom_hline(yintercept = 60,col = "#EE4C97",linetype = "dashed",alpha = 0.6,linewidth = 1)+
  annotate(geom = "text",x = 23,y = 63,label = "Speed Limit ",vjust = 0,hjust = 1,face = "italic")+
  theme_minimal()+
  labs(x = "Hour", y = "Observed speed")+
  scale_x_continuous(breaks = 0:23,
                     labels = sprintf("%02d:00",0:23))+
  scale_y_continuous(limits = c(0,100),breaks = seq(0,100,20))+
  scale_colour_manual(values = paletteer_d("ggsci::default_nejm",n = 3))+
  theme(axis.text.x = element_text(angle = 90),
        panel.grid.minor = element_blank(),
        legend.position = "none")
```

Median overall speed profile 

```{r}
norm_summary_spd |> 
  ggplot(aes(x = HORA,y = norm_speed))+
  # geom_boxplot(aes(group = HORA), fill = NA,alpha = 0.3,outlier.shape = NA)+
  # geom_jitter(alpha = 0.3, size = 0.3,col = "gray60")+
  stat_summary(geom = "line",
               fun = "mean",
               aes(col = DAY_TYPE),
               linewidth = 1.5,
               alpha = 0.6,
               # show.legend = F
               )+
  facet_grid(.~YEAR)+
  theme_minimal()+
  labs(x = "Hour", y = "Speed ratio (observed/94th percentile)")+
  scale_x_continuous(breaks = 0:23,
                     labels = sprintf("%02d:00",0:23))+
  scale_y_continuous(limits = c(0,1.25),breaks = seq(0,1.25,0.25))+
  scale_colour_manual(values = paletteer_d("ggsci::default_nejm",n = 3))+
  theme(axis.text.x = element_text(angle = 90),
        panel.grid.minor = element_blank(),
        legend.position = "top")
```



## Spatial data

Up to this point, all the data processing has not involved the spatial component.
On the open data platform, it is possible to download the `gpkg` files for each
month. With the following code, we will identify which file(s) is(are) needed to have
all data.

Ideally, we need a file that contains all `r unique(clean_data$TID) |> length()` `TID`. 


```{r}
#| label: id-monthyear-max-tid

unique(
  clean_data[,
           c("TID","YEAR","MES")])[,
                                   .(count = .N),
                                   by = c("YEAR","MES")][count == max(count)]
```

As the file of February 2019 does not have all the links, we identify alternative files.

```{r}
#| label: id-spatial-files

tid_feb2019 <- unique(clean_data[YEAR == 2019 & MES == "February","TID"])[,1]
tid_missing <- unique(clean_data[!(TID %in% tid_feb2019$TID),"TID"])

unique(
  clean_data[TID %in% tid_missing$TID,
           c("TID","YEAR","MES")])[,
                                   .(count = .N),
                                   by = c("YEAR","MES")][count == max(count)]
```

The files for February 2019 and October 2020 have been downloaded manually from
the same source. As we are only interested in the geometries, we filter out all
the other data.



```{r}
#| label: read-spatial-speeds

sf_feb2019<- st_read(
   "../00_data/bogota/speed_data/Velocidades_Bitcarrier_Febrero_2019_1361955177739874723.gpkg",
   query="select TID,SHAPE from 'Velocidades_Bitcarrier_Febrero_2019'"
   ) |> filter(TID %in% tid_feb2019$TID) |> select(TID) |> slice_head(n = 1,by = TID)

sf_oct2020 <- st_read(
  "../00_data/bogota/speed_data/Velocidades_Bitcarrier_Octubre_2020_-1518838627102027019.gpkg",
  query="select TID,SHAPE from 'Velocidades_Bitcarrier_Octubre_2020'") |> filter(TID %in% tid_missing$TID) |> slice_head(n = 1,by = TID)
```


```{r}
#| label: bind-sf-speed

sf_speed <- bind_rows(sf_feb2019,sf_oct2020)
rm(sf_feb2019,sf_oct2020)
```


```{r}
#| label: load-urbanper

urban_perimeter <- st_read("raw_data/perimetrourbano.gpkg")
```

A quick visualisation of the maximum speeds:

```{r}
#| label: map-max-speeds

sf_speed[urban_perimeter,] |> 
  left_join(max_speeds,by = "TID") |> 
  ggplot()+
  geom_sf(aes(col = p94_speed),linewidth = 0.3,alpha = 0.7)+
  scale_color_gradientn(colours = paletteer_c("grDevices::Plasma", 30))+
  theme_void()

```




